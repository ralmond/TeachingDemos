[
  {
    "objectID": "RIntro/MatrixesAndDataFrames.html",
    "href": "RIntro/MatrixesAndDataFrames.html",
    "title": "Arrays, Matrixes and Data Frames",
    "section": "",
    "text": "Vector – ordered collection of objects of the same storage mode ([ extract)\n\nNamed Vector – adds a names attribute (Can use names in subscripts)\nMatrix, Array – adds a dim and dimnames attribute\n\nList – ordered collection of objects of any type or mode ([[ extract)\n\nNamed List – add names attribute (Can use $ to extract elements)\nS3 Class – adds a class attribute\ndata.frame – a list of columns in a spreadsheet. Uses ([ extract).\n\nS4 Class – formal class mechanism. Uses @ instead of $.\n\n\n\nThe mode funciton in R refers to storage modes, not the mode of a distribution.\n\nmode(123)\n\n[1] \"numeric\"\n\nmode(123L)\n\n[1] \"numeric\"\n\nmode(TRUE)\n\n[1] \"logical\"\n\nmode(\"True\")\n\n[1] \"character\"\n\nmode(3.14)\n\n[1] \"numeric\"\n\nmode(t)\n\n[1] \"function\"\n\n?mode\n\nThe as.XXX and is.XXX functions can be used to convert between different types.\n\nis.integer(3)\n\n[1] FALSE\n\nis.integer(3L)\n\n[1] TRUE\n\nas.integer(3)\n\n[1] 3\n\nis.integer(as.integer(3))\n\n[1] TRUE\n\nas.integer(\"three\")\n\nWarning: NAs introduced by coercion\n\n\n[1] NA\n\nas.character(3)\n\n[1] \"3\"\n\nas.logical(3)\n\n[1] TRUE\n\n\n\n\n\nAll R objects are vectors: scalars in R are vectors of length 1.\n\ncat(\"The output will start with '[1]' to show that this is a vector.\\n\")\n\nThe output will start with '[1]' to show that this is a vector.\n\n3.14159  \n\n[1] 3.14159\n\n\nR implicitly loops over all the elements of a vector. Such implicit loops are faster than explicit for loops.\n\n1:11\n\n [1]  1  2  3  4  5  6  7  8  9 10 11\n\nmean(1:11)\n\n[1] 6\n\ny &lt;- (1:11 - mean(1:11))/sd(1:11)\nmean (y)\n\n[1] 0\n\nsd(y)\n\n[1] 1\n\n\n\n\nThe : operator produces sequences (of integers) between first and second argument.\n\n1:3\n\n[1] 1 2 3\n\n3:1\n\n[1] 3 2 1\n\n-1:1\n\n[1] -1  0  1\n\n-3:-1\n\n[1] -3 -2 -1\n\n\nThe c() function can be used to glue vectors together. (c stands for combine.)\n\nc(1:3, 10:12)\n\n[1]  1  2  3 10 11 12\n\nc(\"Haenzel\", \"Greatel\", \"Tedd\",\"Alice\")\n\n[1] \"Haenzel\" \"Greatel\" \"Tedd\"    \"Alice\"  \n\n\n\n\n\nR has a number of built in random number generators to generate random numbers. The most commonly used are runif, rnorm and sample. There are also many others, with names that look like rXXX (try substituting chisq, t, beta, gamma, &c for XXX).\n\nrunif(5)\n\n[1] 0.4112052 0.2396971 0.9720560 0.3525871 0.5387468\n\nrnorm(10)\n\n [1] -0.09763423  0.52836939 -0.49125730 -0.06416362 -0.66827133  0.53975888\n [7] -1.59174455 -0.69786015  1.44320744  0.01815682\n\nsample.int(5,5,replace=TRUE)\n\n[1] 5 3 3 4 4\n\n\n\n\n\n\nGenerate 100 random numbers with mean 50 and standard deviation 25.\n\n1a. Use the result of the previous question to generate a random sample of size 101 with one outlier of 200.\n\nGenerate random integers between 0 and 100\n\n\nThe variable state.area contains the areas of the 50 US states (in alphabetical area). Create a random sample of size 10 of the state areas.\n\n\n\n\n\nThe [] operator is used to subscript vectors. There are three different things you can put inside of the brackets: numbers, logical expressions and names (character values).\n\n\nNumbers are the most straightforward way to do indexing. R starts the indexes at 1 and it goes up to the length of the vector. The function `length() is useful in writing indexes. Giving multiple indexes with return a sub-vector (remember, there are not scalars in R, just vectors of length 1).\n\nint10 &lt;- 1:10\nint10[3]\n\n[1] 3\n\nint10[c(5:7,9)]\n\n[1] 5 6 7 9\n\nstate.area[c(1,length(state.area))]\n\n[1] 51609 97914\n\n\nAnother useful trick is to use negative indexes. These leave the numbered varaibles out.\n\nint10[-2]\n\n[1]  1  3  4  5  6  7  8  9 10\n\nint10[-(3:8)]\n\n[1]  1  2  9 10\n\n\nIndexing expressions can also be used on the LHS of assignment operators, to allow to assignment to just certain values.\n\nint10[3] &lt;- -3\nint10\n\n [1]  1  2 -3  4  5  6  7  8  9 10\n\n\n\n\n\nThe second option for indexing is to use a logical vector the same length as the vector you are indexing.\n\nint10&lt;0\n\n [1] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\nint10[int10&lt;0]\n\n[1] -3\n\nint10[int10&lt;0] &lt;- abs (int10[int10&lt;0])\nint10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nBe careful with NAs.\n\nint55 &lt;- -5:5\nsqrt(int55) &lt; 1.2\n\nWarning in sqrt(int55): NaNs produced\n\n\n [1]    NA    NA    NA    NA    NA  TRUE  TRUE FALSE FALSE FALSE FALSE\n\nint55[sqrt(int55) &lt; 1.2]\n\nWarning in sqrt(int55): NaNs produced\n\n\n[1] NA NA NA NA NA  0  1\n\n\nThe real power of logical indexes comes when we have two vectors of the same length. For example, state.abb gives the two letter postal codes of the states. Suppose we wanted to see all of the states that are bigger than average:\n\nstate.abb[state.area&gt;median(state.area)]\n\n [1] \"AK\" \"AZ\" \"CA\" \"CO\" \"FL\" \"GA\" \"ID\" \"IL\" \"IA\" \"KS\" \"MI\" \"MN\" \"MO\" \"MT\" \"NE\"\n[16] \"NV\" \"NM\" \"ND\" \"OK\" \"OR\" \"SD\" \"TX\" \"UT\" \"WA\" \"WY\"\n\n\n\n\n\nThe built in langauge primitive if is not vectorized. It is expecting a single value. The code below will not do what you think it will.\n\ntry({\nif (int55 &lt; 0) {\n  cat(\"Negative.\\n\")\n} else { \n  cat(\"Non-negative.\\n\")\n}\n})\n\nError in if (int55 &lt; 0) { : the condition has length &gt; 1\n\n\nThe functions any(), all() and isTRUE() are often useful here.\n\nif (all(int55 &gt;0)) {\n  cat(\"Positive.\\n\")\n} else { \n  cat(\"Not all positive.\\n\")\n}\n\nNot all positive.\n\n\nThe function ifelse() can be used to loop over if-else expressions. There are two differences. First the condition is a logical vector. Second, both the if-true and if-false argument are always evaluated, so they better not generate an error!\n\nifelse(int55&lt;0, \"-\",\"+\")\n\n [1] \"-\" \"-\" \"-\" \"-\" \"-\" \"+\" \"+\" \"+\" \"+\" \"+\" \"+\"\n\n\n\n\n\nIt would be really convenient if we could access the state data by name. Florida is the 9 state alphabetically, but I can’t remember that.\nWhat we can do is add names to a vector. Then we can select by name.\n\nnames(state.area) &lt;- state.abb\nhead(state.area)\n\n    AL     AK     AZ     AR     CA     CO \n 51609 589757 113909  53104 158693 104247 \n\nhead(names(state.area))\n\n[1] \"AL\" \"AK\" \"AZ\" \"AR\" \"CA\" \"CO\"\n\nstate.area[\"FL\"]\n\n   FL \n58560 \n\nstate.area[c(\"NY\",\"CA\")]\n\n    NY     CA \n 49576 158693 \n\n\nSometimes we need to make up names. The paste() command is handy for that. It is vectorized, so you can put a bunch of numbers in.\n\npaste(\"Student\",1:5,sep=\"_\")\n\n[1] \"Student_1\" \"Student_2\" \"Student_3\" \"Student_4\" \"Student_5\"\n\n\n\n\n\n\nWrite an expression that removes the outlier from the data you generated for 1b.\n\n\nSuppose the data you generated for problem 1 was suppose to have a minimum score of 0 and a maximum score of 100. Fix, the data set so that all of the values are between 0 and 100.\n\n\nFix my positive/negative test, so that it has a 0 as well\n\n\nFind all of the states that are bigger than Florida.\n\n\nGenerate a bunch of random integers between -10 and 10. Then turn all negative integers into NA.",
    "crumbs": [
      "Arrays, Matrixes and Data Frames"
    ]
  },
  {
    "objectID": "RIntro/MatrixesAndDataFrames.html#implicit-looping-in-vectors",
    "href": "RIntro/MatrixesAndDataFrames.html#implicit-looping-in-vectors",
    "title": "Arrays, Matrixes and Data Frames",
    "section": "",
    "text": "All R objects are vectors: scalars in R are vectors of length 1.\n\ncat(\"The output will start with '[1]' to show that this is a vector.\\n\")\n\nThe output will start with '[1]' to show that this is a vector.\n\n3.14159  \n\n[1] 3.14159\n\n\nR implicitly loops over all the elements of a vector. Such implicit loops are faster than explicit for loops.\n\n1:11\n\n [1]  1  2  3  4  5  6  7  8  9 10 11\n\nmean(1:11)\n\n[1] 6\n\ny &lt;- (1:11 - mean(1:11))/sd(1:11)\nmean (y)\n\n[1] 0\n\nsd(y)\n\n[1] 1\n\n\n\n\nThe : operator produces sequences (of integers) between first and second argument.\n\n1:3\n\n[1] 1 2 3\n\n3:1\n\n[1] 3 2 1\n\n-1:1\n\n[1] -1  0  1\n\n-3:-1\n\n[1] -3 -2 -1\n\n\nThe c() function can be used to glue vectors together. (c stands for combine.)\n\nc(1:3, 10:12)\n\n[1]  1  2  3 10 11 12\n\nc(\"Haenzel\", \"Greatel\", \"Tedd\",\"Alice\")\n\n[1] \"Haenzel\" \"Greatel\" \"Tedd\"    \"Alice\"  \n\n\n\n\n\nR has a number of built in random number generators to generate random numbers. The most commonly used are runif, rnorm and sample. There are also many others, with names that look like rXXX (try substituting chisq, t, beta, gamma, &c for XXX).\n\nrunif(5)\n\n[1] 0.4112052 0.2396971 0.9720560 0.3525871 0.5387468\n\nrnorm(10)\n\n [1] -0.09763423  0.52836939 -0.49125730 -0.06416362 -0.66827133  0.53975888\n [7] -1.59174455 -0.69786015  1.44320744  0.01815682\n\nsample.int(5,5,replace=TRUE)\n\n[1] 5 3 3 4 4\n\n\n\n\n\n\nGenerate 100 random numbers with mean 50 and standard deviation 25.\n\n1a. Use the result of the previous question to generate a random sample of size 101 with one outlier of 200.\n\nGenerate random integers between 0 and 100\n\n\nThe variable state.area contains the areas of the 50 US states (in alphabetical area). Create a random sample of size 10 of the state areas.",
    "crumbs": [
      "Arrays, Matrixes and Data Frames"
    ]
  },
  {
    "objectID": "RIntro/MatrixesAndDataFrames.html#three-ways-of-subscripting-an-array",
    "href": "RIntro/MatrixesAndDataFrames.html#three-ways-of-subscripting-an-array",
    "title": "Arrays, Matrixes and Data Frames",
    "section": "",
    "text": "The [] operator is used to subscript vectors. There are three different things you can put inside of the brackets: numbers, logical expressions and names (character values).\n\n\nNumbers are the most straightforward way to do indexing. R starts the indexes at 1 and it goes up to the length of the vector. The function `length() is useful in writing indexes. Giving multiple indexes with return a sub-vector (remember, there are not scalars in R, just vectors of length 1).\n\nint10 &lt;- 1:10\nint10[3]\n\n[1] 3\n\nint10[c(5:7,9)]\n\n[1] 5 6 7 9\n\nstate.area[c(1,length(state.area))]\n\n[1] 51609 97914\n\n\nAnother useful trick is to use negative indexes. These leave the numbered varaibles out.\n\nint10[-2]\n\n[1]  1  3  4  5  6  7  8  9 10\n\nint10[-(3:8)]\n\n[1]  1  2  9 10\n\n\nIndexing expressions can also be used on the LHS of assignment operators, to allow to assignment to just certain values.\n\nint10[3] &lt;- -3\nint10\n\n [1]  1  2 -3  4  5  6  7  8  9 10\n\n\n\n\n\nThe second option for indexing is to use a logical vector the same length as the vector you are indexing.\n\nint10&lt;0\n\n [1] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\nint10[int10&lt;0]\n\n[1] -3\n\nint10[int10&lt;0] &lt;- abs (int10[int10&lt;0])\nint10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nBe careful with NAs.\n\nint55 &lt;- -5:5\nsqrt(int55) &lt; 1.2\n\nWarning in sqrt(int55): NaNs produced\n\n\n [1]    NA    NA    NA    NA    NA  TRUE  TRUE FALSE FALSE FALSE FALSE\n\nint55[sqrt(int55) &lt; 1.2]\n\nWarning in sqrt(int55): NaNs produced\n\n\n[1] NA NA NA NA NA  0  1\n\n\nThe real power of logical indexes comes when we have two vectors of the same length. For example, state.abb gives the two letter postal codes of the states. Suppose we wanted to see all of the states that are bigger than average:\n\nstate.abb[state.area&gt;median(state.area)]\n\n [1] \"AK\" \"AZ\" \"CA\" \"CO\" \"FL\" \"GA\" \"ID\" \"IL\" \"IA\" \"KS\" \"MI\" \"MN\" \"MO\" \"MT\" \"NE\"\n[16] \"NV\" \"NM\" \"ND\" \"OK\" \"OR\" \"SD\" \"TX\" \"UT\" \"WA\" \"WY\"\n\n\n\n\n\nThe built in langauge primitive if is not vectorized. It is expecting a single value. The code below will not do what you think it will.\n\ntry({\nif (int55 &lt; 0) {\n  cat(\"Negative.\\n\")\n} else { \n  cat(\"Non-negative.\\n\")\n}\n})\n\nError in if (int55 &lt; 0) { : the condition has length &gt; 1\n\n\nThe functions any(), all() and isTRUE() are often useful here.\n\nif (all(int55 &gt;0)) {\n  cat(\"Positive.\\n\")\n} else { \n  cat(\"Not all positive.\\n\")\n}\n\nNot all positive.\n\n\nThe function ifelse() can be used to loop over if-else expressions. There are two differences. First the condition is a logical vector. Second, both the if-true and if-false argument are always evaluated, so they better not generate an error!\n\nifelse(int55&lt;0, \"-\",\"+\")\n\n [1] \"-\" \"-\" \"-\" \"-\" \"-\" \"+\" \"+\" \"+\" \"+\" \"+\" \"+\"\n\n\n\n\n\nIt would be really convenient if we could access the state data by name. Florida is the 9 state alphabetically, but I can’t remember that.\nWhat we can do is add names to a vector. Then we can select by name.\n\nnames(state.area) &lt;- state.abb\nhead(state.area)\n\n    AL     AK     AZ     AR     CA     CO \n 51609 589757 113909  53104 158693 104247 \n\nhead(names(state.area))\n\n[1] \"AL\" \"AK\" \"AZ\" \"AR\" \"CA\" \"CO\"\n\nstate.area[\"FL\"]\n\n   FL \n58560 \n\nstate.area[c(\"NY\",\"CA\")]\n\n    NY     CA \n 49576 158693 \n\n\nSometimes we need to make up names. The paste() command is handy for that. It is vectorized, so you can put a bunch of numbers in.\n\npaste(\"Student\",1:5,sep=\"_\")\n\n[1] \"Student_1\" \"Student_2\" \"Student_3\" \"Student_4\" \"Student_5\"\n\n\n\n\n\n\nWrite an expression that removes the outlier from the data you generated for 1b.\n\n\nSuppose the data you generated for problem 1 was suppose to have a minimum score of 0 and a maximum score of 100. Fix, the data set so that all of the values are between 0 and 100.\n\n\nFix my positive/negative test, so that it has a 0 as well\n\n\nFind all of the states that are bigger than Florida.\n\n\nGenerate a bunch of random integers between -10 and 10. Then turn all negative integers into NA.",
    "crumbs": [
      "Arrays, Matrixes and Data Frames"
    ]
  },
  {
    "objectID": "RIntro/MatrixesAndDataFrames.html#matrixes-and-arrays-are-vectors-with-a-dim-attribute",
    "href": "RIntro/MatrixesAndDataFrames.html#matrixes-and-arrays-are-vectors-with-a-dim-attribute",
    "title": "Arrays, Matrixes and Data Frames",
    "section": "Matrixes and Arrays are vectors with a dim attribute",
    "text": "Matrixes and Arrays are vectors with a dim attribute\n\nA matrix is an object with rows and columns.\nAn array can have any number of dimensions.\nBut they all the entries need to be the same type (mode).\nThere is a dim() attribute which shows the dimensions of the matrix.\n\n\ndim(state.x77)\n\n[1] 50  8\n\nhead(state.x77)\n\n           Population Income Illiteracy Life Exp Murder HS Grad Frost   Area\nAlabama          3615   3624        2.1    69.05   15.1    41.3    20  50708\nAlaska            365   6315        1.5    69.31   11.3    66.7   152 566432\nArizona          2212   4530        1.8    70.55    7.8    58.1    15 113417\nArkansas         2110   3378        1.9    70.66   10.1    39.9    65  51945\nCalifornia      21198   5114        1.1    71.71   10.3    62.6    20 156361\nColorado         2541   4884        0.7    72.06    6.8    63.9   166 103766\n\n\n\ngetting and setting dims\nThe dim() function is used to access the number of rows and columns. dim()[1] gets the number of rows and dim()[2] the number of columns.\nFor maxtrixes, the functions nrow() and ncol() are easier to use.\nSetting dim() will reset a vector into a matrix or array.\n\nnrow(state.x77)\n\n[1] 50\n\nncol(state.x77)\n\n[1] 8\n\nint12 &lt;- 1:12\ndim(int12) &lt;- c(3,4)\nint12\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\n\n\n\nmatrix() and array() functions\n\nSetting the dim() attribute directly is not recommended (makes for hard to read code).\nInstead use matrix() or array()\nR stores matrixes in row major order (like FORTRAN, not like c).\n\nUse byrow=TRUE to reverse this in matrix or array\n\n\n\nmatrix(1:12,3,4)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\nmatrix(1:12,3,4,byrow=TRUE)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n\narray(1:24,c(2,3,4))\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]    7    9   11\n[2,]    8   10   12\n\n, , 3\n\n     [,1] [,2] [,3]\n[1,]   13   15   17\n[2,]   14   16   18\n\n, , 4\n\n     [,1] [,2] [,3]\n[1,]   19   21   23\n[2,]   20   22   24",
    "crumbs": [
      "Arrays, Matrixes and Data Frames"
    ]
  },
  {
    "objectID": "RIntro/MatrixesAndDataFrames.html#numeric-and-logical-indexes",
    "href": "RIntro/MatrixesAndDataFrames.html#numeric-and-logical-indexes",
    "title": "Arrays, Matrixes and Data Frames",
    "section": "numeric and logical indexes",
    "text": "numeric and logical indexes\nFor matrixes and arrays, the [] operator does something a little bit different. In particular, x[i,j] picks out row \\(i\\) and column \\(j\\). Either the row or column selector could be\n\nA number or vector of numbers (pick those rows or columns)\nA negative number of vector of negative numbers (excluded those rows or columns)\nA logical vector of size nrow(x) or ncol(x) (select the rows/columns corresponding to true).\nA character vector (select rows or columns by name, see below).\nLeft blank, in which case all rows/columns are selected.\n\nIf a single row or column is selected, then it turns into a vector.\n\nstate.x77[1:5,1:5]\n\n           Population Income Illiteracy Life Exp Murder\nAlabama          3615   3624        2.1    69.05   15.1\nAlaska            365   6315        1.5    69.31   11.3\nArizona          2212   4530        1.8    70.55    7.8\nArkansas         2110   3378        1.9    70.66   10.1\nCalifornia      21198   5114        1.1    71.71   10.3\n\nstate.x77[1:5,]\n\n           Population Income Illiteracy Life Exp Murder HS Grad Frost   Area\nAlabama          3615   3624        2.1    69.05   15.1    41.3    20  50708\nAlaska            365   6315        1.5    69.31   11.3    66.7   152 566432\nArizona          2212   4530        1.8    70.55    7.8    58.1    15 113417\nArkansas         2110   3378        1.9    70.66   10.1    39.9    65  51945\nCalifornia      21198   5114        1.1    71.71   10.3    62.6    20 156361\n\nstate.x77[9,]\n\nPopulation     Income Illiteracy   Life Exp     Murder    HS Grad      Frost \n   8277.00    4815.00       1.30      70.66      10.70      52.60      11.00 \n      Area \n  54090.00 \n\ndim(state.x77[9,])\n\nNULL\n\nhead(state.x77[,3])\n\n   Alabama     Alaska    Arizona   Arkansas California   Colorado \n       2.1        1.5        1.8        1.9        1.1        0.7 \n\nstate.x77[9,,drop=FALSE]\n\n        Population Income Illiteracy Life Exp Murder HS Grad Frost  Area\nFlorida       8277   4815        1.3    70.66   10.7    52.6    11 54090\n\ndim(state.x77[9,,drop=FALSE])\n\n[1] 1 8",
    "crumbs": [
      "Arrays, Matrixes and Data Frames"
    ]
  },
  {
    "objectID": "RIntro/MatrixesAndDataFrames.html#dimnames-and-character-indexes",
    "href": "RIntro/MatrixesAndDataFrames.html#dimnames-and-character-indexes",
    "title": "Arrays, Matrixes and Data Frames",
    "section": "dimnames and character indexes",
    "text": "dimnames and character indexes\nTo use character indexes with matrixes, we need to set the rownames() and colnames() of the matrix. We can also use the dimnames() (although this will produce a list).\n\nrownames(state.x77)\n\n [1] \"Alabama\"        \"Alaska\"         \"Arizona\"        \"Arkansas\"      \n [5] \"California\"     \"Colorado\"       \"Connecticut\"    \"Delaware\"      \n [9] \"Florida\"        \"Georgia\"        \"Hawaii\"         \"Idaho\"         \n[13] \"Illinois\"       \"Indiana\"        \"Iowa\"           \"Kansas\"        \n[17] \"Kentucky\"       \"Louisiana\"      \"Maine\"          \"Maryland\"      \n[21] \"Massachusetts\"  \"Michigan\"       \"Minnesota\"      \"Mississippi\"   \n[25] \"Missouri\"       \"Montana\"        \"Nebraska\"       \"Nevada\"        \n[29] \"New Hampshire\"  \"New Jersey\"     \"New Mexico\"     \"New York\"      \n[33] \"North Carolina\" \"North Dakota\"   \"Ohio\"           \"Oklahoma\"      \n[37] \"Oregon\"         \"Pennsylvania\"   \"Rhode Island\"   \"South Carolina\"\n[41] \"South Dakota\"   \"Tennessee\"      \"Texas\"          \"Utah\"          \n[45] \"Vermont\"        \"Virginia\"       \"Washington\"     \"West Virginia\" \n[49] \"Wisconsin\"      \"Wyoming\"       \n\ncolnames(state.x77)\n\n[1] \"Population\" \"Income\"     \"Illiteracy\" \"Life Exp\"   \"Murder\"    \n[6] \"HS Grad\"    \"Frost\"      \"Area\"      \n\ndimnames(state.x77)\n\n[[1]]\n [1] \"Alabama\"        \"Alaska\"         \"Arizona\"        \"Arkansas\"      \n [5] \"California\"     \"Colorado\"       \"Connecticut\"    \"Delaware\"      \n [9] \"Florida\"        \"Georgia\"        \"Hawaii\"         \"Idaho\"         \n[13] \"Illinois\"       \"Indiana\"        \"Iowa\"           \"Kansas\"        \n[17] \"Kentucky\"       \"Louisiana\"      \"Maine\"          \"Maryland\"      \n[21] \"Massachusetts\"  \"Michigan\"       \"Minnesota\"      \"Mississippi\"   \n[25] \"Missouri\"       \"Montana\"        \"Nebraska\"       \"Nevada\"        \n[29] \"New Hampshire\"  \"New Jersey\"     \"New Mexico\"     \"New York\"      \n[33] \"North Carolina\" \"North Dakota\"   \"Ohio\"           \"Oklahoma\"      \n[37] \"Oregon\"         \"Pennsylvania\"   \"Rhode Island\"   \"South Carolina\"\n[41] \"South Dakota\"   \"Tennessee\"      \"Texas\"          \"Utah\"          \n[45] \"Vermont\"        \"Virginia\"       \"Washington\"     \"West Virginia\" \n[49] \"Wisconsin\"      \"Wyoming\"       \n\n[[2]]\n[1] \"Population\" \"Income\"     \"Illiteracy\" \"Life Exp\"   \"Murder\"    \n[6] \"HS Grad\"    \"Frost\"      \"Area\"      \n\nrownames(state.x77) &lt;- state.abb\nhead(state.x77)\n\n   Population Income Illiteracy Life Exp Murder HS Grad Frost   Area\nAL       3615   3624        2.1    69.05   15.1    41.3    20  50708\nAK        365   6315        1.5    69.31   11.3    66.7   152 566432\nAZ       2212   4530        1.8    70.55    7.8    58.1    15 113417\nAR       2110   3378        1.9    70.66   10.1    39.9    65  51945\nCA      21198   5114        1.1    71.71   10.3    62.6    20 156361\nCO       2541   4884        0.7    72.06    6.8    63.9   166 103766",
    "crumbs": [
      "Arrays, Matrixes and Data Frames"
    ]
  },
  {
    "objectID": "RIntro/MatrixesAndDataFrames.html#row-and-column-sums-and-averages",
    "href": "RIntro/MatrixesAndDataFrames.html#row-and-column-sums-and-averages",
    "title": "Arrays, Matrixes and Data Frames",
    "section": "Row and column sums and averages",
    "text": "Row and column sums and averages\nRemember that a matrix is just a vector with a dim attribute. Consequently, mean and other summary functions don’t do what we want:\n\nmean(state.x77)\n\n[1] 9956.887\n\nsd(state.x77)\n\n[1] 37801.78\n\nvar(state.x77)\n\n              Population        Income   Illiteracy      Life Exp       Murder\nPopulation 19931683.7588   571229.7796  292.8679592 -4.078425e+02  5663.523714\nIncome       571229.7796   377573.3061 -163.7020408  2.806632e+02  -521.894286\nIlliteracy      292.8680     -163.7020    0.3715306 -4.815122e-01     1.581776\nLife Exp       -407.8425      280.6632   -0.4815122  1.802020e+00    -3.869480\nMurder         5663.5237     -521.8943    1.5817755 -3.869480e+00    13.627465\nHS Grad       -3551.5096     3076.7690   -3.2354694  6.312685e+00   -14.549616\nFrost        -77081.9727     7227.6041  -21.2900000  1.828678e+01  -103.406000\nArea        8587916.9494 19049013.7510 4018.3371429 -1.229410e+04 71940.429959\n                 HS Grad        Frost          Area\nPopulation  -3551.509551 -77081.97265  8.587917e+06\nIncome       3076.768980   7227.60408  1.904901e+07\nIlliteracy     -3.235469    -21.29000  4.018337e+03\nLife Exp        6.312685     18.28678 -1.229410e+04\nMurder        -14.549616   -103.40600  7.194043e+04\nHS Grad        65.237894    153.99216  2.298732e+05\nFrost         153.992163   2702.00857  2.627039e+05\nArea       229873.192816 262703.89306  7.280748e+09\n\ncor(state.x77)\n\n            Population     Income  Illiteracy    Life Exp     Murder\nPopulation  1.00000000  0.2082276  0.10762237 -0.06805195  0.3436428\nIncome      0.20822756  1.0000000 -0.43707519  0.34025534 -0.2300776\nIlliteracy  0.10762237 -0.4370752  1.00000000 -0.58847793  0.7029752\nLife Exp   -0.06805195  0.3402553 -0.58847793  1.00000000 -0.7808458\nMurder      0.34364275 -0.2300776  0.70297520 -0.78084575  1.0000000\nHS Grad    -0.09848975  0.6199323 -0.65718861  0.58221620 -0.4879710\nFrost      -0.33215245  0.2262822 -0.67194697  0.26206801 -0.5388834\nArea        0.02254384  0.3633154  0.07726113 -0.10733194  0.2283902\n               HS Grad      Frost        Area\nPopulation -0.09848975 -0.3321525  0.02254384\nIncome      0.61993232  0.2262822  0.36331544\nIlliteracy -0.65718861 -0.6719470  0.07726113\nLife Exp    0.58221620  0.2620680 -0.10733194\nMurder     -0.48797102 -0.5388834  0.22839021\nHS Grad     1.00000000  0.3667797  0.33354187\nFrost       0.36677970  1.0000000  0.05922910\nArea        0.33354187  0.0592291  1.00000000\n\n\nTaking row and column sums are such a frequent operation, that there is a shortcut for them: rowSums(), colSums(), rowMeans(), colMeans()\n\ncolMeans(state.x77)\n\nPopulation     Income Illiteracy   Life Exp     Murder    HS Grad      Frost \n 4246.4200  4435.8000     1.1700    70.8786     7.3780    53.1080   104.4600 \n      Area \n70735.8800",
    "crumbs": [
      "Arrays, Matrixes and Data Frames"
    ]
  },
  {
    "objectID": "RIntro/MatrixesAndDataFrames.html#apply-and-sweep",
    "href": "RIntro/MatrixesAndDataFrames.html#apply-and-sweep",
    "title": "Arrays, Matrixes and Data Frames",
    "section": "Apply and Sweep",
    "text": "Apply and Sweep\nThe apply() operator can turn any summary function into a row or column function.\n\nhelp(apply)\n\nThe MARGIN argument to apply should be 1 for rows, 2 for columns and so forth for generaly arrays.\n\nint12\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\napply(int12,1,max)\n\n[1] 10 11 12\n\napply(int12,2,max)\n\n[1]  3  6  9 12\n\n\nThe sweep operator “subtracts” a vector from all of the rows or columns of the matrix.\n“Subtracts” is in quotes because actually any operator can be used here. Subtracts “-” and divides “/” are the most common.\n\nhelp(sweep)\nrow.min &lt;- apply(int12,1,min)\nsweep(int12,1,row.min,\"/\")\n\n     [,1] [,2] [,3] [,4]\n[1,]    1  4.0    7 10.0\n[2,]    1  2.5    4  5.5\n[3,]    1  2.0    3  4.0\n\ncol.min &lt;- apply(int12,2,min)\nsweep(int12,2,col.min,\"-\")\n\n     [,1] [,2] [,3] [,4]\n[1,]    0    0    0    0\n[2,]    1    1    1    1\n[3,]    2    2    2    2",
    "crumbs": [
      "Arrays, Matrixes and Data Frames"
    ]
  },
  {
    "objectID": "RIntro/MatrixesAndDataFrames.html#exercises-2",
    "href": "RIntro/MatrixesAndDataFrames.html#exercises-2",
    "title": "Arrays, Matrixes and Data Frames",
    "section": "Exercises:",
    "text": "Exercises:\n\nFind the population for all states whose area is bigger than Florida’s.\n\n\nCalculate the population density (population per area) for each state\n\n\nTurn the state.x77 data into z-scores by subtracting the column means and dividing by the column standard deviations.\n\n\nScale the state.x77 data from 0 (minimum in the column) to 1 (maximum in the column).",
    "crumbs": [
      "Arrays, Matrixes and Data Frames"
    ]
  },
  {
    "objectID": "RIntro/MatrixesAndDataFrames.html#single-and-double-extraction",
    "href": "RIntro/MatrixesAndDataFrames.html#single-and-double-extraction",
    "title": "Arrays, Matrixes and Data Frames",
    "section": "Single [ and double [[ extraction",
    "text": "Single [ and double [[ extraction",
    "crumbs": [
      "Arrays, Matrixes and Data Frames"
    ]
  },
  {
    "objectID": "RIntro/MatrixesAndDataFrames.html#named-lists-and-extraction",
    "href": "RIntro/MatrixesAndDataFrames.html#named-lists-and-extraction",
    "title": "Arrays, Matrixes and Data Frames",
    "section": "Named Lists and $ extraction",
    "text": "Named Lists and $ extraction",
    "crumbs": [
      "Arrays, Matrixes and Data Frames"
    ]
  },
  {
    "objectID": "RIntro/MatrixesAndDataFrames.html#lapply-and-sapply-for-looping-through-lists",
    "href": "RIntro/MatrixesAndDataFrames.html#lapply-and-sapply-for-looping-through-lists",
    "title": "Arrays, Matrixes and Data Frames",
    "section": "lapply and sapply for looping through lists",
    "text": "lapply and sapply for looping through lists",
    "crumbs": [
      "Arrays, Matrixes and Data Frames"
    ]
  },
  {
    "objectID": "RIntro/MatrixesAndDataFrames.html#classes-as-list-with-special-behavior",
    "href": "RIntro/MatrixesAndDataFrames.html#classes-as-list-with-special-behavior",
    "title": "Arrays, Matrixes and Data Frames",
    "section": "Classes as list with special behavior",
    "text": "Classes as list with special behavior\n\nGeneric functions and methods\n\n\n‘factor’ and ‘ordered’ classes\n\nS4 classes vs S3 classes",
    "crumbs": [
      "Arrays, Matrixes and Data Frames"
    ]
  },
  {
    "objectID": "RIntro/MatrixesAndDataFrames.html#data.frame-and-read.table",
    "href": "RIntro/MatrixesAndDataFrames.html#data.frame-and-read.table",
    "title": "Arrays, Matrixes and Data Frames",
    "section": "data.frame() and read.table()",
    "text": "data.frame() and read.table()",
    "crumbs": [
      "Arrays, Matrixes and Data Frames"
    ]
  },
  {
    "objectID": "RIntro/MatrixesAndDataFrames.html#matrix-like-behaior-using-subscripts",
    "href": "RIntro/MatrixesAndDataFrames.html#matrix-like-behaior-using-subscripts",
    "title": "Arrays, Matrixes and Data Frames",
    "section": "Matrix-like behaior – Using [ subscripts",
    "text": "Matrix-like behaior – Using [ subscripts\n\napply, rownames, colnames and colsum and rowsum",
    "crumbs": [
      "Arrays, Matrixes and Data Frames"
    ]
  },
  {
    "objectID": "RIntro/MatrixesAndDataFrames.html#list-like-behavior-using-and-subscripts",
    "href": "RIntro/MatrixesAndDataFrames.html#list-like-behavior-using-and-subscripts",
    "title": "Arrays, Matrixes and Data Frames",
    "section": "List-like behavior – Using [[ and $ subscripts",
    "text": "List-like behavior – Using [[ and $ subscripts",
    "crumbs": [
      "Arrays, Matrixes and Data Frames"
    ]
  },
  {
    "objectID": "RIntro/MatrixesAndDataFrames.html#names-lapply-and-sapply",
    "href": "RIntro/MatrixesAndDataFrames.html#names-lapply-and-sapply",
    "title": "Arrays, Matrixes and Data Frames",
    "section": "names, lapply and sapply",
    "text": "names, lapply and sapply",
    "crumbs": [
      "Arrays, Matrixes and Data Frames"
    ]
  },
  {
    "objectID": "RIntro/MatrixesAndDataFrames.html#as.matrix-and-as.data.frame",
    "href": "RIntro/MatrixesAndDataFrames.html#as.matrix-and-as.data.frame",
    "title": "Arrays, Matrixes and Data Frames",
    "section": "as.matrix and as.data.frame",
    "text": "as.matrix and as.data.frame",
    "crumbs": [
      "Arrays, Matrixes and Data Frames"
    ]
  },
  {
    "objectID": "RIntro/EDAwithGGPlot.html",
    "href": "RIntro/EDAwithGGPlot.html",
    "title": "Exploratory Data Analysis with GGplot",
    "section": "",
    "text": "For this example, we are going to use GGplot, which is part of the tidyverse. Tidyverse is an extra layer on top of R which makes it easy to manipulate data as a kind of a workflow. Note that tidyverse is actually a meta-package: it downloads a number of generally useful packages, including GGplot (GG stands for Grammar of Graphics, a book about how to build up complex plots from smaller pieces.)\nThe command install.packages() installs packages, that is, it downloads them from the CRAN library to your local computer. The command library() tells R that you want to use that package in this session. You need to run library() every time, but you only need to run install.packages() once.\n\nif (!(\"tidyverse\" %in% row.names(installed.packages()))) {\n  install.packages(\"tidyverse\",repos=\"https://cloud.r-project.org\",dependencies=TRUE)\n}\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.2.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "Exploratory Data Analysis with GGplot"
    ]
  },
  {
    "objectID": "RIntro/EDAwithGGPlot.html#tidyverse-software",
    "href": "RIntro/EDAwithGGPlot.html#tidyverse-software",
    "title": "Exploratory Data Analysis with GGplot",
    "section": "",
    "text": "For this example, we are going to use GGplot, which is part of the tidyverse. Tidyverse is an extra layer on top of R which makes it easy to manipulate data as a kind of a workflow. Note that tidyverse is actually a meta-package: it downloads a number of generally useful packages, including GGplot (GG stands for Grammar of Graphics, a book about how to build up complex plots from smaller pieces.)\nThe command install.packages() installs packages, that is, it downloads them from the CRAN library to your local computer. The command library() tells R that you want to use that package in this session. You need to run library() every time, but you only need to run install.packages() once.\n\nif (!(\"tidyverse\" %in% row.names(installed.packages()))) {\n  install.packages(\"tidyverse\",repos=\"https://cloud.r-project.org\",dependencies=TRUE)\n}\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.2.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "Exploratory Data Analysis with GGplot"
    ]
  },
  {
    "objectID": "RIntro/EDAwithGGPlot.html#tibbles",
    "href": "RIntro/EDAwithGGPlot.html#tibbles",
    "title": "Exploratory Data Analysis with GGplot",
    "section": "Tibbles",
    "text": "Tibbles\nFor this exercise we will use the data set state.x77 which comes with R. You can find more information about this data set by doing:\n\nhelp(state.x77)\n\nA tibble is a data structure with rows corresponding to cases and columns to variables. It is a tidy version of a data frame.\n\nas_tibble(state.x77) %&gt;% add_column(region=state.region,name=state.name,code=state.abb,center_x=state.center$x,center_y=state.center$y) -&gt; state77\nstate77\n\n# A tibble: 50 × 13\n   Population Income Illiteracy `Life Exp` Murder `HS Grad` Frost   Area region \n        &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;  \n 1       3615   3624        2.1       69.0   15.1      41.3    20  50708 South  \n 2        365   6315        1.5       69.3   11.3      66.7   152 566432 West   \n 3       2212   4530        1.8       70.6    7.8      58.1    15 113417 West   \n 4       2110   3378        1.9       70.7   10.1      39.9    65  51945 South  \n 5      21198   5114        1.1       71.7   10.3      62.6    20 156361 West   \n 6       2541   4884        0.7       72.1    6.8      63.9   166 103766 West   \n 7       3100   5348        1.1       72.5    3.1      56     139   4862 Northe…\n 8        579   4809        0.9       70.1    6.2      54.6   103   1982 South  \n 9       8277   4815        1.3       70.7   10.7      52.6    11  54090 South  \n10       4931   4091        2         68.5   13.9      40.6    60  58073 South  \n# ℹ 40 more rows\n# ℹ 4 more variables: name &lt;chr&gt;, code &lt;chr&gt;, center_x &lt;dbl&gt;, center_y &lt;dbl&gt;\n\n\nThe View() command opens the data frame/matrix/tibble in another window.\n\nView(state77)\n\n\nTry state77 in the console. The tibble is slightly different from the data frame in the way it prints.\nTibble and data frames are pretty much interchangeable. (Where they aren’t use as.data.frame() or as_tibble() to convert.\n\n\nNote the type of the variables are shown in the display of the tibble. The name and postal code are left as strings, but region is a factor (with four levels). In a data frame, the string variables are automatically converted to factors, which is not always what you want.\n\n\nUse read_csv() instead of read.csv() to load a CSV file as a tibble instead of a data frame.",
    "crumbs": [
      "Exploratory Data Analysis with GGplot"
    ]
  },
  {
    "objectID": "RIntro/EDAwithGGPlot.html#the-pipe",
    "href": "RIntro/EDAwithGGPlot.html#the-pipe",
    "title": "Exploratory Data Analysis with GGplot",
    "section": "The Pipe",
    "text": "The Pipe\nThe special operator %&gt;% (or |&gt;) can be used to chain operations together.\nThe expression above gives an example. The output of as_tibble() is passed to the add_column() which is then passed to the assignment operator -&gt;.\nNote the backward arrow -&gt;. This is like the usual assignment operator &lt;- except now the name of the variable is on the right instead of the left.\nA typical chain looks like:\ndata %&gt;% select(variables) %&gt;% filter(cases) %&gt;% analysis() -&gt; result\nOr maybe the analysis is replaced with a call to ggplot to make a plot.",
    "crumbs": [
      "Exploratory Data Analysis with GGplot"
    ]
  },
  {
    "objectID": "RIntro/EDAwithGGPlot.html#selecting-variables",
    "href": "RIntro/EDAwithGGPlot.html#selecting-variables",
    "title": "Exploratory Data Analysis with GGplot",
    "section": "Selecting Variables",
    "text": "Selecting Variables\nThe select() command can be used to select a subset of variables.\n\nstate77 %&gt;% select(code,Population,Income)\n\n# A tibble: 50 × 3\n   code  Population Income\n   &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n 1 AL          3615   3624\n 2 AK           365   6315\n 3 AZ          2212   4530\n 4 AR          2110   3378\n 5 CA         21198   5114\n 6 CO          2541   4884\n 7 CT          3100   5348\n 8 DE           579   4809\n 9 FL          8277   4815\n10 GA          4931   4091\n# ℹ 40 more rows\n\nstate77 %&gt;% select(code,region:code)\n\n# A tibble: 50 × 3\n   code  region    name       \n   &lt;chr&gt; &lt;fct&gt;     &lt;chr&gt;      \n 1 AL    South     Alabama    \n 2 AK    West      Alaska     \n 3 AZ    West      Arizona    \n 4 AR    South     Arkansas   \n 5 CA    West      California \n 6 CO    West      Colorado   \n 7 CT    Northeast Connecticut\n 8 DE    South     Delaware   \n 9 FL    South     Florida    \n10 GA    South     Georgia    \n# ℹ 40 more rows\n\nstate77 %&gt;% select(-name)\n\n# A tibble: 50 × 12\n   Population Income Illiteracy `Life Exp` Murder `HS Grad` Frost   Area region \n        &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;  \n 1       3615   3624        2.1       69.0   15.1      41.3    20  50708 South  \n 2        365   6315        1.5       69.3   11.3      66.7   152 566432 West   \n 3       2212   4530        1.8       70.6    7.8      58.1    15 113417 West   \n 4       2110   3378        1.9       70.7   10.1      39.9    65  51945 South  \n 5      21198   5114        1.1       71.7   10.3      62.6    20 156361 West   \n 6       2541   4884        0.7       72.1    6.8      63.9   166 103766 West   \n 7       3100   5348        1.1       72.5    3.1      56     139   4862 Northe…\n 8        579   4809        0.9       70.1    6.2      54.6   103   1982 South  \n 9       8277   4815        1.3       70.7   10.7      52.6    11  54090 South  \n10       4931   4091        2         68.5   13.9      40.6    60  58073 South  \n# ℹ 40 more rows\n# ℹ 3 more variables: code &lt;chr&gt;, center_x &lt;dbl&gt;, center_y &lt;dbl&gt;\n\nstate77 %&gt;% select(code,starts_with(\"center\"))\n\n# A tibble: 50 × 3\n   code  center_x center_y\n   &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 AL       -86.8     32.6\n 2 AK      -127.      49.2\n 3 AZ      -112.      34.2\n 4 AR       -92.3     34.7\n 5 CA      -120.      36.5\n 6 CO      -106.      38.7\n 7 CT       -72.4     41.6\n 8 DE       -75.0     38.7\n 9 FL       -81.7     27.9\n10 GA       -83.4     32.3\n# ℹ 40 more rows\n\n\nUsually having more columns than you need is harmless.\nFor example, using lm() to fit a regression of ggplot() to make a plot will just use the variables referenced in the model or plot description.\nHowever, sometimes is it easier to work with a smaller subset of the data with just the stuff you need.",
    "crumbs": [
      "Exploratory Data Analysis with GGplot"
    ]
  },
  {
    "objectID": "RIntro/EDAwithGGPlot.html#making-new-variables",
    "href": "RIntro/EDAwithGGPlot.html#making-new-variables",
    "title": "Exploratory Data Analysis with GGplot",
    "section": "Making New Variables",
    "text": "Making New Variables\nWe already saw the add_column() function for adding columns.\nThe mutate() function adds new columns as a function of the old ones:\n\nstate77 %&gt;% mutate(Pop_Density=Population/Area) -&gt; state77a\nstate77a\n\n# A tibble: 50 × 14\n   Population Income Illiteracy `Life Exp` Murder `HS Grad` Frost   Area region \n        &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;  \n 1       3615   3624        2.1       69.0   15.1      41.3    20  50708 South  \n 2        365   6315        1.5       69.3   11.3      66.7   152 566432 West   \n 3       2212   4530        1.8       70.6    7.8      58.1    15 113417 West   \n 4       2110   3378        1.9       70.7   10.1      39.9    65  51945 South  \n 5      21198   5114        1.1       71.7   10.3      62.6    20 156361 West   \n 6       2541   4884        0.7       72.1    6.8      63.9   166 103766 West   \n 7       3100   5348        1.1       72.5    3.1      56     139   4862 Northe…\n 8        579   4809        0.9       70.1    6.2      54.6   103   1982 South  \n 9       8277   4815        1.3       70.7   10.7      52.6    11  54090 South  \n10       4931   4091        2         68.5   13.9      40.6    60  58073 South  \n# ℹ 40 more rows\n# ℹ 5 more variables: name &lt;chr&gt;, code &lt;chr&gt;, center_x &lt;dbl&gt;, center_y &lt;dbl&gt;,\n#   Pop_Density &lt;dbl&gt;",
    "crumbs": [
      "Exploratory Data Analysis with GGplot"
    ]
  },
  {
    "objectID": "RIntro/EDAwithGGPlot.html#recoding-variables",
    "href": "RIntro/EDAwithGGPlot.html#recoding-variables",
    "title": "Exploratory Data Analysis with GGplot",
    "section": "Recoding Variables",
    "text": "Recoding Variables\nRecoding is important because sometimes the way the variable is stored in the data file is not the same as the way we want to analyze it.\n\nFactor variables can represent categories with integer values or string labels.\n\nOften there is a code book which maps integer category labels to string values. For example:\n\n\n\nFemale\nMale\n\nThe factor() function creates factor variables.\n\nfactor(c(1,1,1,2,2,2),levels=1:2,labels=c(\"Female\",\"Male\"))\n\n[1] Female Female Female Male   Male   Male  \nLevels: Female Male\n\nfactor(c(\"Male\",\"Male\",\"Male\",\"Female\",\"Female\",\"Female\"),levels=c(\"Male\",\"Female\"))\n\n[1] Male   Male   Male   Female Female Female\nLevels: Male Female\n\nordered(c(\"H\",\"H\",\"M\",\"M\",\"L\",\"L\"), levels=c(\"L\",\"M\",\"H\"))\n\n[1] H H M M L L\nLevels: L &lt; M &lt; H\n\n\n\nThe levels argument tells R how the data are coded (in the case of integer coding).\nThe labels argument gives the names for the levels (if omitted it is the same as levels).\n\n\nThe ordered() function produces an ordered variable as opposed to factor() which produces a nominal one. This only makes a difference in a few places. Probably the most important one is how they are used in an Analysis of Variance (ANOVA). That is covered in EDF 5402.\n\n\n\n\n\n\n\nWarning\n\n\n\nNote Bene! The read_csv() function which is part of the tidyverse will read factor variables as either character or integer variables, depending on how they are coded. So you will need to use mutate(x=factor(x)) to convert x into a factor.\n\n\nThe function parse_factor() is almost the same, but gives a warning if some of the levels aren’t recognized.\n\nfactor(c(\"Male\",\"Female\",\"Non-binary\"),levels=c(\"Male\",\"Female\"))\n\n[1] Male   Female &lt;NA&gt;  \nLevels: Male Female\n\nparse_factor(c(\"Male\",\"Female\",\"Non-binary\"),levels=c(\"Male\",\"Female\"))\n\nWarning: 1 parsing failure.\nrow col           expected     actual\n  3  -- value in level set Non-binary\n\n\n[1] Male   Female &lt;NA&gt;  \nattr(,\"problems\")\n# A tibble: 1 × 4\n    row   col expected           actual    \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;              &lt;chr&gt;     \n1     3    NA value in level set Non-binary\nLevels: Male Female\n\n\nAnother way to do the coding is to use * recode() (makes a character or numeric value) * recode_factor() (makes a factor variable)\nThe first argument is the vector to be recorded, the remaining arguments are the values to be replaced.\n\nrecode_factor(c(1,1,1,2,2,2),`1`=\"Male\",`2`=\"Female\")\n\n[1] Male   Male   Male   Female Female Female\nLevels: Male Female\n\nrecode_factor(c(1,1,1,2,2,2),\"Male\",\"Female\")\n\n[1] Male   Male   Male   Female Female Female\nLevels: Male Female\n\nrecode_factor(c(\"M\",\"M\",\"F\",\"F\"),M=\"Male\",F=\"Female\")\n\n[1] Male   Male   Female Female\nLevels: Male Female\n\nrecode_factor(c(\"White\",\"Black\",\"Latinx\",\"Other\"),White=\"White\",.default=\"Non-White\")\n\n[1] White     Non-White Non-White Non-White\nLevels: White Non-White\n\n\nNote how we used the last version to collapse several categories into one. This is often useful, particularly when the number of subjects in one category is small.",
    "crumbs": [
      "Exploratory Data Analysis with GGplot"
    ]
  },
  {
    "objectID": "RIntro/EDAwithGGPlot.html#recoding-nas",
    "href": "RIntro/EDAwithGGPlot.html#recoding-nas",
    "title": "Exploratory Data Analysis with GGplot",
    "section": "Recoding NAs",
    "text": "Recoding NAs\nA special case of recoding comes about with missing values.\nIn R, these are called NA (for Not Applicable).\n\nNAs are contagious: NA + anything is still NA.\n\n\nNA+5\n\n[1] NA\n\nmean(c(1,2,NA))\n\n[1] NA\n\nmean(c(1,2,NA),na.rm=TRUE)\n\n[1] 1.5\n\n\n\nNaN (not a number) is similar but it comes from nonsense arthimatic (taking log of negative number).\nNAs can be coded in many different ways in a data set:\n\nLeave the value blank.\nSpecial character, e.g., . or *\nSpecial String, e.g., NA\nNonsense numeric value, e.g., -9\n\n\nWhen using nonsense numeric values, it is important to pick a value that is not plausible, e.g., a large negative value. That way, if you accidently forget to convert, you can know that something is wrong.\nThe function na_if() can be used to replace a value with NAs.\n\nna_if(c(1:5,-9),-9)\n\n[1]  1  2  3  4  5 NA\n\nstarwars %&gt;% select(name,eye_color) %&gt;%\n  mutate(eye_color=na_if(eye_color,\"unknown\"))\n\n# A tibble: 87 × 2\n   name               eye_color\n   &lt;chr&gt;              &lt;chr&gt;    \n 1 Luke Skywalker     blue     \n 2 C-3PO              yellow   \n 3 R2-D2              red      \n 4 Darth Vader        yellow   \n 5 Leia Organa        brown    \n 6 Owen Lars          blue     \n 7 Beru Whitesun Lars blue     \n 8 R5-D4              red      \n 9 Biggs Darklighter  brown    \n10 Obi-Wan Kenobi     blue-gray\n# ℹ 77 more rows\n\n\nThe function replace_na() goes in the opposite direction.\nFor example, we might want to treat missing values as score of 0 on a test.\n\nreplace_na(c(1,1,0,0,NA),0)\n\n[1] 1 1 0 0 0",
    "crumbs": [
      "Exploratory Data Analysis with GGplot"
    ]
  },
  {
    "objectID": "RIntro/EDAwithGGPlot.html#logical-tests",
    "href": "RIntro/EDAwithGGPlot.html#logical-tests",
    "title": "Exploratory Data Analysis with GGplot",
    "section": "Logical Tests",
    "text": "Logical Tests\nThe function if_else() is also useful for splitting data sets up into groups.\nWe can see the form in:\n\nargs(if_else)\n\nfunction (condition, true, false, missing = NULL, ..., ptype = NULL, \n    size = NULL) \nNULL\n\n\nNote that condition is a logical expression which should yeild a true or false value for every row of the tibble. The variable true is the value to use if true, false the value to use if false, and missing the value to use if missing.\n\nint5 &lt;- -5:5\nif_else(int5&lt;0,\"-\",\"+\")\n\n [1] \"-\" \"-\" \"-\" \"-\" \"-\" \"+\" \"+\" \"+\" \"+\" \"+\" \"+\"\n\nif_else(int5&lt;0,-int5,int5) #Absolute value\n\n [1] 5 4 3 2 1 0 1 2 3 4 5\n\nna_if(int5,0)\n\n [1] -5 -4 -3 -2 -1 NA  1  2  3  4  5\n\nif_else(na_if(int5,0)&lt;0 ,\"-\",\"+\",\"0\")\n\n [1] \"-\" \"-\" \"-\" \"-\" \"-\" \"0\" \"+\" \"+\" \"+\" \"+\" \"+\"\n\n\nHere are the common logical tests:\n\n== – equals (don’t confuse this with = assignment.)\n!= – not equals\n&lt;, &lt;=, =&gt;, &gt; – less than, &c.\n! – Not (true if the rest of the expression is false)\nis.na() – True if the value is NA, false otherwise. (Also, !is.na())\n& – logical and (true when LHS and RHS are true)\n| – logical or (true if either LHS or RHS is true)\n%in% – True if value is in list.\n\n\ndrupes &lt;- c(\"Almond\",\"Cashew\",\"Walnut\")\nc(\"Peanut\",\"Almond\",\"Hazelnut\",\"Macademia\",\"Cashew\") %in% drupes\n\n[1] FALSE  TRUE FALSE FALSE  TRUE",
    "crumbs": [
      "Exploratory Data Analysis with GGplot"
    ]
  },
  {
    "objectID": "RIntro/EDAwithGGPlot.html#selecting-cases",
    "href": "RIntro/EDAwithGGPlot.html#selecting-cases",
    "title": "Exploratory Data Analysis with GGplot",
    "section": "Selecting Cases",
    "text": "Selecting Cases\nVery often instead of setting the value to NA, we just want to exclude that row from the data set.\nThe command filter() does this.\n\nstate77 %&gt;% filter(!(code %in% c(\"AK\",\"HI\")))\n\n# A tibble: 48 × 13\n   Population Income Illiteracy `Life Exp` Murder `HS Grad` Frost   Area region \n        &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;  \n 1       3615   3624        2.1       69.0   15.1      41.3    20  50708 South  \n 2       2212   4530        1.8       70.6    7.8      58.1    15 113417 West   \n 3       2110   3378        1.9       70.7   10.1      39.9    65  51945 South  \n 4      21198   5114        1.1       71.7   10.3      62.6    20 156361 West   \n 5       2541   4884        0.7       72.1    6.8      63.9   166 103766 West   \n 6       3100   5348        1.1       72.5    3.1      56     139   4862 Northe…\n 7        579   4809        0.9       70.1    6.2      54.6   103   1982 South  \n 8       8277   4815        1.3       70.7   10.7      52.6    11  54090 South  \n 9       4931   4091        2         68.5   13.9      40.6    60  58073 South  \n10        813   4119        0.6       71.9    5.3      59.5   126  82677 West   \n# ℹ 38 more rows\n# ℹ 4 more variables: name &lt;chr&gt;, code &lt;chr&gt;, center_x &lt;dbl&gt;, center_y &lt;dbl&gt;\n\n\nSometimes we want to temporarily remove the biggest values or the smallest values so we can see the details in a plot.\n\nstate77 %&gt;% select(name,Area) %&gt;% filter(Area &lt;200000)\n\n# A tibble: 48 × 2\n   name          Area\n   &lt;chr&gt;        &lt;dbl&gt;\n 1 Alabama      50708\n 2 Arizona     113417\n 3 Arkansas     51945\n 4 California  156361\n 5 Colorado    103766\n 6 Connecticut   4862\n 7 Delaware      1982\n 8 Florida      54090\n 9 Georgia      58073\n10 Hawaii        6425\n# ℹ 38 more rows\n\n\nSometimes we want to create subsets of the data that just have fewer cases.\nThe functions sample_frac() and sample_n() specify the size of the sample in fraction of the original data or absolute size.\nThe function slice() will select a contiguous range of cases, which is useful when looping through the data.",
    "crumbs": [
      "Exploratory Data Analysis with GGplot"
    ]
  },
  {
    "objectID": "RIntro/EDAwithGGPlot.html#calculating-summary-statistics",
    "href": "RIntro/EDAwithGGPlot.html#calculating-summary-statistics",
    "title": "Exploratory Data Analysis with GGplot",
    "section": "Calculating Summary Statistics",
    "text": "Calculating Summary Statistics\nPipe the output of the select and filter command into summarize():\n\nstate77 %&gt;% summarize(N=n(),Income=mean(Income),Population=mean(Population))\n\n# A tibble: 1 × 3\n      N Income Population\n  &lt;int&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1    50  4436.      4246.\n\n\nHere are some useful functions to use with summarize():\n\nn(), n_distinct(), sum(!is.na()) – Count, count of unique values, count of non-missing values.\nmean(), median() – Measures of center\nmin(), max(), quantile() – Position other than the center.\n\n\nstate77 %&gt;% select(Population) %&gt;% summarize(Min=min(Population),Q1=quantile(Population,.25),Q2=median(Population),Q3=quantile(Population,.75),Max=max(Population))\n\n# A tibble: 1 × 5\n    Min    Q1    Q2    Q3   Max\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   365 1080. 2838. 4968. 21198\n\n\n\nsd(), IQR(), mad() – measures of scale.\nsum(), prod() – Arithmetic\nsum(), any(), all() – Summarize logical expressions (count number true, true if all are true, true if any is true).\n\nAll of these functions have an optional argument na.rm. If there are NAs, you usually want to include na.rm=TRUE, as otherwise the value will be NA.",
    "crumbs": [
      "Exploratory Data Analysis with GGplot"
    ]
  },
  {
    "objectID": "RIntro/EDAwithGGPlot.html#summarizing-multiple-columns.",
    "href": "RIntro/EDAwithGGPlot.html#summarizing-multiple-columns.",
    "title": "Exploratory Data Analysis with GGplot",
    "section": "Summarizing Multiple columns.",
    "text": "Summarizing Multiple columns.\nOften, you want to do the same summary on several columns.\nThe function summarize_all() does that.\n\nstate77 %&gt;% select(Area,Population) %&gt;% summarize_all(mean,na.rm=TRUE)\n\n# A tibble: 1 × 2\n    Area Population\n   &lt;dbl&gt;      &lt;dbl&gt;\n1 70736.      4246.\n\n\nYou can use multiple statsitics by putting them in a list.\n\nstate77 %&gt;% select(Area,Population) %&gt;% summarize_all(list(mean=mean,sd=sd))\n\n# A tibble: 1 × 4\n  Area_mean Population_mean Area_sd Population_sd\n      &lt;dbl&gt;           &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;\n1    70736.           4246.  85327.         4464.\n\n\nThe function summarize_at() combines the select() and sumarize().\nThe function summarize_if() allows the selection of columns based on logical criteria.",
    "crumbs": [
      "Exploratory Data Analysis with GGplot"
    ]
  },
  {
    "objectID": "RIntro/EDAwithGGPlot.html#calculating-statistics-by-group",
    "href": "RIntro/EDAwithGGPlot.html#calculating-statistics-by-group",
    "title": "Exploratory Data Analysis with GGplot",
    "section": "Calculating Statistics by Group",
    "text": "Calculating Statistics by Group\nVery often we want to be to compare groups. We can use the function group_by() to split the data set by a factor variable.\n\nstate77 %&gt;% group_by(region) %&gt;% select(Area,Population) %&gt;% summarize_all(list(mean=mean,sd=sd))\n\nAdding missing grouping variables: `region`\n\n\n# A tibble: 4 × 5\n  region        Area_mean Population_mean Area_sd Population_sd\n  &lt;fct&gt;             &lt;dbl&gt;           &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;\n1 Northeast        18141            5495.  18076.         6080.\n2 South            54605.           4208.  57965.         2780.\n3 North Central    62652            4803   14967.         3703.\n4 West            134463            2915. 134982.         5579.\n\n\n\nstate77 %&gt;% group_by(region) %&gt;%\n  select(Area,Population) %&gt;%\n  summarise_all(list(Min=min,Q1=function(x){quantile(x,.25)},Q2=median,Q3=function(x){quantile(x,.75)},Max=max))\n\nAdding missing grouping variables: `region`\n\n\n# A tibble: 4 × 11\n  region     Area_Min Population_Min Area_Q1 Population_Q1 Area_Q2 Population_Q2\n  &lt;fct&gt;         &lt;dbl&gt;          &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;\n1 Northeast      1049            472   7521           931     9027         3100 \n2 South          1982            579  37294.         2622.   46113         3710.\n3 North Cen…    36097            637  55427          2096    62906         4255 \n4 West           6425            365  82677           746   103766         1144 \n# ℹ 4 more variables: Area_Q3 &lt;dbl&gt;, Population_Q3 &lt;dbl&gt;, Area_Max &lt;dbl&gt;,\n#   Population_Max &lt;dbl&gt;\n\n\n\nThe function(){} makes an anonymous function. This gets around the problem that quantile() needs two arguments, but summarize_all() expects a function of just one. In more recent versions of R, one can use \\(){} instead of function to create anonymous functions.\n\nThe cheat sheet.\nYou can find a handy list of dplyr and other tidyverse commands for manipulating data by selected “Help &gt; Cheat Sheets &gt; Data Mainpulation with dplyr” from the RStudio menu.\n\n\nGraphics\n\nMaking Histograms\n\nggplot(state77,aes(Population)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\n\nggplot(state77,aes(Population)) + geom_histogram(binwidth=500)\n\n\n\n\n\n\n\n\n\nggplot(state77,aes(Population)) + geom_histogram(bins=10)\n\n\n\n\n\n\n\n\n\nggplot(state77,aes(Population)) + geom_dotplot()\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n\n\n\nggplot(state77,aes(Population)) +geom_dotplot(binwidth=1000) +geom_density(aes(y=..scaled..))\n\nWarning: The dot-dot notation (`..scaled..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(scaled)` instead.\n\n\n\n\n\n\n\n\n\n\nggplot(state77,aes(Population)) +geom_histogram(binwidth=1000) +geom_density(aes(y=1000*..count..))\n\n\n\n\n\n\n\n\n\nggplot(state77,aes(Population)) +geom_histogram(binwidth=1000) +stat_function(fun= function(x) dnorm(x,mean=mean(state77$Population), sd=sd(state77$Population))*nrow(state77)*1000)\n\n\n\n\n\n\n\n\n\nbw &lt;- 1000\nggplot(state77,aes(Population)) + geom_histogram(aes(y=..density..),binwidth=bw) + \nstat_function(fun=dnorm, args=c(mean=mean(state77$Population), sd=sd(state77$Population))) +\nscale_y_continuous(\"Density\",sec.axis=sec_axis(trans = ~ . * bw * nrow(state77), name = \"Counts\"))\n\nWarning: The `trans` argument of `sec_axis()` is deprecated as of ggplot2 3.5.0.\nℹ Please use the `transform` argument instead.\n\n\n\n\n\n\n\n\n\n\n\nPanel Histograms by a Group\n\nggplot(state77,aes(Population)) + facet_grid(rows=vars(region)) + geom_dotplot()\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n\n\n\nggplot(state77,aes(Population)) + facet_grid(rows=vars(region)) + geom_dotplot(binwidth=750)+geom_density(aes(y=750*..count..))\n\n\n\n\n\n\n\n\n\n\nMaking Boxplots\n\nggplot(state77,aes(x=region,y=Population)) + geom_boxplot()\n\n\n\n\n\n\n\n\n\nggplot(state77,aes(x=region,y=Population)) + geom_violin()\n\n\n\n\n\n\n\n\n\nggplot(state77,aes(region,Population)) + geom_dotplot(binaxis=\"y\",stackdir=\"center\")\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\nSaving Your Work\n\nSaving Your Plots\n\nggsave(\"foo.png\")\n\nSaving 7 x 5 in image\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\nJust saved file.\n\n\n\n\nSaving Your Tables\n\nlibrary(xtable)\nprint(xtable(state77 %&gt;% group_by(region)%&gt;% select(Population,Area) %&gt;% summarize_all(list(mean=mean,sd=sd))),digits=3,type=\"html\",file=\"foo.html\")\n\nAdding missing grouping variables: `region`\n\n\nresult\n\n\nWorking in R Markdown",
    "crumbs": [
      "Exploratory Data Analysis with GGplot"
    ]
  },
  {
    "objectID": "RIntro/EDAwithGGPlot.html#the-cheat-sheet.",
    "href": "RIntro/EDAwithGGPlot.html#the-cheat-sheet.",
    "title": "Exploratory Data Analysis with GGplot",
    "section": "The cheat sheet.",
    "text": "The cheat sheet.\nYou can find a handy list of dplyr and other tidyverse commands for manipulating data by selected “Help &gt; Cheat Sheets &gt; Data Mainpulation with dplyr” from the RStudio menu.",
    "crumbs": [
      "Exploratory Data Analysis with GGplot"
    ]
  },
  {
    "objectID": "RIntro/EDAwithGGPlot.html#making-histograms",
    "href": "RIntro/EDAwithGGPlot.html#making-histograms",
    "title": "Exploratory Data Analysis with GGplot",
    "section": "Making Histograms",
    "text": "Making Histograms\n\nggplot(state77,aes(Population)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\n\nggplot(state77,aes(Population)) + geom_histogram(binwidth=500)\n\n\n\n\n\n\n\n\n\nggplot(state77,aes(Population)) + geom_histogram(bins=10)\n\n\n\n\n\n\n\n\n\nggplot(state77,aes(Population)) + geom_dotplot()\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n\n\n\nggplot(state77,aes(Population)) +geom_dotplot(binwidth=1000) +geom_density(aes(y=..scaled..))\n\nWarning: The dot-dot notation (`..scaled..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(scaled)` instead.\n\n\n\n\n\n\n\n\n\n\nggplot(state77,aes(Population)) +geom_histogram(binwidth=1000) +geom_density(aes(y=1000*..count..))\n\n\n\n\n\n\n\n\n\nggplot(state77,aes(Population)) +geom_histogram(binwidth=1000) +stat_function(fun= function(x) dnorm(x,mean=mean(state77$Population), sd=sd(state77$Population))*nrow(state77)*1000)\n\n\n\n\n\n\n\n\n\nbw &lt;- 1000\nggplot(state77,aes(Population)) + geom_histogram(aes(y=..density..),binwidth=bw) + \nstat_function(fun=dnorm, args=c(mean=mean(state77$Population), sd=sd(state77$Population))) +\nscale_y_continuous(\"Density\",sec.axis=sec_axis(trans = ~ . * bw * nrow(state77), name = \"Counts\"))\n\nWarning: The `trans` argument of `sec_axis()` is deprecated as of ggplot2 3.5.0.\nℹ Please use the `transform` argument instead.",
    "crumbs": [
      "Exploratory Data Analysis with GGplot"
    ]
  },
  {
    "objectID": "RIntro/EDAwithGGPlot.html#panel-histograms-by-a-group",
    "href": "RIntro/EDAwithGGPlot.html#panel-histograms-by-a-group",
    "title": "Exploratory Data Analysis with GGplot",
    "section": "Panel Histograms by a Group",
    "text": "Panel Histograms by a Group\n\nggplot(state77,aes(Population)) + facet_grid(rows=vars(region)) + geom_dotplot()\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n\n\n\nggplot(state77,aes(Population)) + facet_grid(rows=vars(region)) + geom_dotplot(binwidth=750)+geom_density(aes(y=750*..count..))",
    "crumbs": [
      "Exploratory Data Analysis with GGplot"
    ]
  },
  {
    "objectID": "RIntro/EDAwithGGPlot.html#making-boxplots",
    "href": "RIntro/EDAwithGGPlot.html#making-boxplots",
    "title": "Exploratory Data Analysis with GGplot",
    "section": "Making Boxplots",
    "text": "Making Boxplots\n\nggplot(state77,aes(x=region,y=Population)) + geom_boxplot()\n\n\n\n\n\n\n\n\n\nggplot(state77,aes(x=region,y=Population)) + geom_violin()\n\n\n\n\n\n\n\n\n\nggplot(state77,aes(region,Population)) + geom_dotplot(binaxis=\"y\",stackdir=\"center\")\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.",
    "crumbs": [
      "Exploratory Data Analysis with GGplot"
    ]
  },
  {
    "objectID": "RIntro/EDAwithGGPlot.html#saving-your-plots",
    "href": "RIntro/EDAwithGGPlot.html#saving-your-plots",
    "title": "Exploratory Data Analysis with GGplot",
    "section": "Saving Your Plots",
    "text": "Saving Your Plots\n\nggsave(\"foo.png\")\n\nSaving 7 x 5 in image\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\nJust saved file.",
    "crumbs": [
      "Exploratory Data Analysis with GGplot"
    ]
  },
  {
    "objectID": "RIntro/EDAwithGGPlot.html#saving-your-tables",
    "href": "RIntro/EDAwithGGPlot.html#saving-your-tables",
    "title": "Exploratory Data Analysis with GGplot",
    "section": "Saving Your Tables",
    "text": "Saving Your Tables\n\nlibrary(xtable)\nprint(xtable(state77 %&gt;% group_by(region)%&gt;% select(Population,Area) %&gt;% summarize_all(list(mean=mean,sd=sd))),digits=3,type=\"html\",file=\"foo.html\")\n\nAdding missing grouping variables: `region`\n\n\nresult",
    "crumbs": [
      "Exploratory Data Analysis with GGplot"
    ]
  },
  {
    "objectID": "RIntro/EDAwithGGPlot.html#working-in-r-markdown",
    "href": "RIntro/EDAwithGGPlot.html#working-in-r-markdown",
    "title": "Exploratory Data Analysis with GGplot",
    "section": "Working in R Markdown",
    "text": "Working in R Markdown",
    "crumbs": [
      "Exploratory Data Analysis with GGplot"
    ]
  },
  {
    "objectID": "RIntro/TidyStrings.html",
    "href": "RIntro/TidyStrings.html",
    "title": "String Hacks",
    "section": "",
    "text": "Tidyverse contains the stringr package.\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.2.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "String Hacks"
    ]
  },
  {
    "objectID": "RIntro/TidyStrings.html#making-file-names.",
    "href": "RIntro/TidyStrings.html#making-file-names.",
    "title": "String Hacks",
    "section": "Making file names.",
    "text": "Making file names.\n\nstr_c(\"data\",1:3,\".csv\")\n\n[1] \"data1.csv\" \"data2.csv\" \"data3.csv\"\n\nstr_c(\"dir\",\"subdir\",\"file.ext\",sep=\"/\")\n\n[1] \"dir/subdir/file.ext\"\n\nfile.path(\"dir\",\"subdir\",\"file.ext\")\n\n[1] \"dir/subdir/file.ext\"",
    "crumbs": [
      "String Hacks"
    ]
  },
  {
    "objectID": "RIntro/TidyStrings.html#regular-expressions.",
    "href": "RIntro/TidyStrings.html#regular-expressions.",
    "title": "String Hacks",
    "section": "Regular expressions.",
    "text": "Regular expressions.\n\nx &lt;- c(\"apple\", \"banana\", \"pear\")\nstr_view(x, \"an\")\n\n[2] │ b&lt;an&gt;&lt;an&gt;a\n\nstr_view(x, \"an\") -&gt; foo\nfoo\n\n[2] │ b&lt;an&gt;&lt;an&gt;a",
    "crumbs": [
      "String Hacks"
    ]
  },
  {
    "objectID": "RIntro/TidyStrings.html#match-strings-exactly-3-characters-long.",
    "href": "RIntro/TidyStrings.html#match-strings-exactly-3-characters-long.",
    "title": "String Hacks",
    "section": "Match strings exactly 3 characters long.",
    "text": "Match strings exactly 3 characters long.\n\nexp &lt;- c(\"a\",\"aa\",\"aaa\",\"aaaa\",\"aaaaa\")\nstr_view(exp,\"^...$\")\n\n[3] │ &lt;aaa&gt;",
    "crumbs": [
      "String Hacks"
    ]
  },
  {
    "objectID": "RIntro/TidyStrings.html#challenge-problem",
    "href": "RIntro/TidyStrings.html#challenge-problem",
    "title": "String Hacks",
    "section": "Challenge problem",
    "text": "Challenge problem\nThe answer to a stat question is \\(t = -.876\\). Suppose that the answer is to be counted correct:\n* No matter how many digits the person has. * No matter whether or not the person put a leading zero. * No matter whether or not the person included the minus sign. * Does not count rounding errors.\nWrite a regular expression that matches a character string (consisting of numbers) which “scores” this example.\n\nposex &lt;- c(\"-.876\",\"-.88\", \"-.9\",\"-0.9\",\"-0.876\",\n             \".876\",\".88\", \".9\",\"0.9\",\"0.876\",\n             \".87\",\".8\",\"- 0.9\")\nnegex &lt;- c(\"-.54\",\".33\",\".888\",\"-1.876\")\nstr_detect(posex,\".876?\")\n\n [1]  TRUE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE FALSE\n[13] FALSE\n\nstr_detect(negex,\".876?\")\n\n[1] FALSE FALSE FALSE  TRUE",
    "crumbs": [
      "String Hacks"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html",
    "href": "RIntro/RDataStructures.html",
    "title": "Basic R Data Structures",
    "section": "",
    "text": "S1 (qpe) – original version internal to Bell Labs\n\nFortran interface\nCopy on modify (preserve original data).\nData stored on disk.\n\nS2 (Becker, Chambers & Wilks, 1988; Blue Book).\n\nvector\nmatrix, array\nfunction\nplot\n\nS3 (Chambers & Hastie, 1992; White Book)\n\nlist\ndata.frame\nformula & lm\nclass() – Informal (S3) classes\nUseMethod – Informal Generic Functions\n\nSplus is commercial version of S\nR open source version of S\n\nInput from XLISP-STAT\nWorkspace, not disk storage.\n\nS4 (Chambers, 1998; Green Book)\n\nFormal Classes & Generic Function\nReally first implemented in R\n\nR6 (Chambers, 2016)\n\nReference classes (like c++, java, &c)",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#a-brief-history-of-s-splus-and-r",
    "href": "RIntro/RDataStructures.html#a-brief-history-of-s-splus-and-r",
    "title": "Basic R Data Structures",
    "section": "",
    "text": "S1 (qpe) – original version internal to Bell Labs\n\nFortran interface\nCopy on modify (preserve original data).\nData stored on disk.\n\nS2 (Becker, Chambers & Wilks, 1988; Blue Book).\n\nvector\nmatrix, array\nfunction\nplot\n\nS3 (Chambers & Hastie, 1992; White Book)\n\nlist\ndata.frame\nformula & lm\nclass() – Informal (S3) classes\nUseMethod – Informal Generic Functions\n\nSplus is commercial version of S\nR open source version of S\n\nInput from XLISP-STAT\nWorkspace, not disk storage.\n\nS4 (Chambers, 1998; Green Book)\n\nFormal Classes & Generic Function\nReally first implemented in R\n\nR6 (Chambers, 2016)\n\nReference classes (like c++, java, &c)",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#storage-modes",
    "href": "RIntro/RDataStructures.html#storage-modes",
    "title": "Basic R Data Structures",
    "section": "Storage modes",
    "text": "Storage modes",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#the-c-function",
    "href": "RIntro/RDataStructures.html#the-c-function",
    "title": "Basic R Data Structures",
    "section": "The c() function",
    "text": "The c() function",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#vectorized-operations",
    "href": "RIntro/RDataStructures.html#vectorized-operations",
    "title": "Basic R Data Structures",
    "section": "Vectorized operations",
    "text": "Vectorized operations",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#summary-operators",
    "href": "RIntro/RDataStructures.html#summary-operators",
    "title": "Basic R Data Structures",
    "section": "Summary operators",
    "text": "Summary operators",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#numerical-indexes",
    "href": "RIntro/RDataStructures.html#numerical-indexes",
    "title": "Basic R Data Structures",
    "section": "Numerical Indexes",
    "text": "Numerical Indexes\n\nAssignment & Indexes",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#negative-indexes",
    "href": "RIntro/RDataStructures.html#negative-indexes",
    "title": "Basic R Data Structures",
    "section": "Negative indexes",
    "text": "Negative indexes",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#logical-indexes",
    "href": "RIntro/RDataStructures.html#logical-indexes",
    "title": "Basic R Data Structures",
    "section": "Logical Indexes",
    "text": "Logical Indexes",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#the-if-statement",
    "href": "RIntro/RDataStructures.html#the-if-statement",
    "title": "Basic R Data Structures",
    "section": "The if statement",
    "text": "The if statement\n\nisTRUE()\n\n\nall and any",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#the-ifelse-function",
    "href": "RIntro/RDataStructures.html#the-ifelse-function",
    "title": "Basic R Data Structures",
    "section": "The ifelse function",
    "text": "The ifelse function",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#named-vectors",
    "href": "RIntro/RDataStructures.html#named-vectors",
    "title": "Basic R Data Structures",
    "section": "Named vectors",
    "text": "Named vectors",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#character-indexes",
    "href": "RIntro/RDataStructures.html#character-indexes",
    "title": "Basic R Data Structures",
    "section": "Character indexes",
    "text": "Character indexes",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#vector-exercises",
    "href": "RIntro/RDataStructures.html#vector-exercises",
    "title": "Basic R Data Structures",
    "section": "Vector exercises",
    "text": "Vector exercises",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#column-major-order",
    "href": "RIntro/RDataStructures.html#column-major-order",
    "title": "Basic R Data Structures",
    "section": "Column major order",
    "text": "Column major order",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#indexes",
    "href": "RIntro/RDataStructures.html#indexes",
    "title": "Basic R Data Structures",
    "section": "indexes",
    "text": "indexes",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#row-and-column-sums-apply-function",
    "href": "RIntro/RDataStructures.html#row-and-column-sums-apply-function",
    "title": "Basic R Data Structures",
    "section": "row and column sums, apply function",
    "text": "row and column sums, apply function",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#sweep-operator",
    "href": "RIntro/RDataStructures.html#sweep-operator",
    "title": "Basic R Data Structures",
    "section": "sweep operator",
    "text": "sweep operator",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#arrays",
    "href": "RIntro/RDataStructures.html#arrays",
    "title": "Basic R Data Structures",
    "section": "arrays",
    "text": "arrays",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#aperm",
    "href": "RIntro/RDataStructures.html#aperm",
    "title": "Basic R Data Structures",
    "section": "aperm",
    "text": "aperm",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#writing-functions",
    "href": "RIntro/RDataStructures.html#writing-functions",
    "title": "Basic R Data Structures",
    "section": "writing functions",
    "text": "writing functions",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#functions-as-arguments",
    "href": "RIntro/RDataStructures.html#functions-as-arguments",
    "title": "Basic R Data Structures",
    "section": "functions as arguments",
    "text": "functions as arguments",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#anonymous-functions",
    "href": "RIntro/RDataStructures.html#anonymous-functions",
    "title": "Basic R Data Structures",
    "section": "anonymous functions",
    "text": "anonymous functions",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#local-and-global-variables",
    "href": "RIntro/RDataStructures.html#local-and-global-variables",
    "title": "Basic R Data Structures",
    "section": "Local and global variables",
    "text": "Local and global variables",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#variable-scope",
    "href": "RIntro/RDataStructures.html#variable-scope",
    "title": "Basic R Data Structures",
    "section": "Variable Scope",
    "text": "Variable Scope",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#search-lists",
    "href": "RIntro/RDataStructures.html#search-lists",
    "title": "Basic R Data Structures",
    "section": "Search Lists",
    "text": "Search Lists",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#function-exercises",
    "href": "RIntro/RDataStructures.html#function-exercises",
    "title": "Basic R Data Structures",
    "section": "Function exercises",
    "text": "Function exercises",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#the-two-extraction-operators",
    "href": "RIntro/RDataStructures.html#the-two-extraction-operators",
    "title": "Basic R Data Structures",
    "section": "The two extraction operators",
    "text": "The two extraction operators",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#named-lists-dictionaries",
    "href": "RIntro/RDataStructures.html#named-lists-dictionaries",
    "title": "Basic R Data Structures",
    "section": "Named lists (dictionaries)",
    "text": "Named lists (dictionaries)",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#the-extractor",
    "href": "RIntro/RDataStructures.html#the-extractor",
    "title": "Basic R Data Structures",
    "section": "The $ extractor",
    "text": "The $ extractor",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#the-for-loop",
    "href": "RIntro/RDataStructures.html#the-for-loop",
    "title": "Basic R Data Structures",
    "section": "The for loop",
    "text": "The for loop\n\nstate.x77\n\n               Population Income Illiteracy Life Exp Murder HS Grad Frost\nAlabama              3615   3624        2.1    69.05   15.1    41.3    20\nAlaska                365   6315        1.5    69.31   11.3    66.7   152\nArizona              2212   4530        1.8    70.55    7.8    58.1    15\nArkansas             2110   3378        1.9    70.66   10.1    39.9    65\nCalifornia          21198   5114        1.1    71.71   10.3    62.6    20\nColorado             2541   4884        0.7    72.06    6.8    63.9   166\nConnecticut          3100   5348        1.1    72.48    3.1    56.0   139\nDelaware              579   4809        0.9    70.06    6.2    54.6   103\nFlorida              8277   4815        1.3    70.66   10.7    52.6    11\nGeorgia              4931   4091        2.0    68.54   13.9    40.6    60\nHawaii                868   4963        1.9    73.60    6.2    61.9     0\nIdaho                 813   4119        0.6    71.87    5.3    59.5   126\nIllinois            11197   5107        0.9    70.14   10.3    52.6   127\nIndiana              5313   4458        0.7    70.88    7.1    52.9   122\nIowa                 2861   4628        0.5    72.56    2.3    59.0   140\nKansas               2280   4669        0.6    72.58    4.5    59.9   114\nKentucky             3387   3712        1.6    70.10   10.6    38.5    95\nLouisiana            3806   3545        2.8    68.76   13.2    42.2    12\nMaine                1058   3694        0.7    70.39    2.7    54.7   161\nMaryland             4122   5299        0.9    70.22    8.5    52.3   101\nMassachusetts        5814   4755        1.1    71.83    3.3    58.5   103\nMichigan             9111   4751        0.9    70.63   11.1    52.8   125\nMinnesota            3921   4675        0.6    72.96    2.3    57.6   160\nMississippi          2341   3098        2.4    68.09   12.5    41.0    50\nMissouri             4767   4254        0.8    70.69    9.3    48.8   108\nMontana               746   4347        0.6    70.56    5.0    59.2   155\nNebraska             1544   4508        0.6    72.60    2.9    59.3   139\nNevada                590   5149        0.5    69.03   11.5    65.2   188\nNew Hampshire         812   4281        0.7    71.23    3.3    57.6   174\nNew Jersey           7333   5237        1.1    70.93    5.2    52.5   115\nNew Mexico           1144   3601        2.2    70.32    9.7    55.2   120\nNew York            18076   4903        1.4    70.55   10.9    52.7    82\nNorth Carolina       5441   3875        1.8    69.21   11.1    38.5    80\nNorth Dakota          637   5087        0.8    72.78    1.4    50.3   186\nOhio                10735   4561        0.8    70.82    7.4    53.2   124\nOklahoma             2715   3983        1.1    71.42    6.4    51.6    82\nOregon               2284   4660        0.6    72.13    4.2    60.0    44\nPennsylvania        11860   4449        1.0    70.43    6.1    50.2   126\nRhode Island          931   4558        1.3    71.90    2.4    46.4   127\nSouth Carolina       2816   3635        2.3    67.96   11.6    37.8    65\nSouth Dakota          681   4167        0.5    72.08    1.7    53.3   172\nTennessee            4173   3821        1.7    70.11   11.0    41.8    70\nTexas               12237   4188        2.2    70.90   12.2    47.4    35\nUtah                 1203   4022        0.6    72.90    4.5    67.3   137\nVermont               472   3907        0.6    71.64    5.5    57.1   168\nVirginia             4981   4701        1.4    70.08    9.5    47.8    85\nWashington           3559   4864        0.6    71.72    4.3    63.5    32\nWest Virginia        1799   3617        1.4    69.48    6.7    41.6   100\nWisconsin            4589   4468        0.7    72.48    3.0    54.5   149\nWyoming               376   4566        0.6    70.29    6.9    62.9   173\n                 Area\nAlabama         50708\nAlaska         566432\nArizona        113417\nArkansas        51945\nCalifornia     156361\nColorado       103766\nConnecticut      4862\nDelaware         1982\nFlorida         54090\nGeorgia         58073\nHawaii           6425\nIdaho           82677\nIllinois        55748\nIndiana         36097\nIowa            55941\nKansas          81787\nKentucky        39650\nLouisiana       44930\nMaine           30920\nMaryland         9891\nMassachusetts    7826\nMichigan        56817\nMinnesota       79289\nMississippi     47296\nMissouri        68995\nMontana        145587\nNebraska        76483\nNevada         109889\nNew Hampshire    9027\nNew Jersey       7521\nNew Mexico     121412\nNew York        47831\nNorth Carolina  48798\nNorth Dakota    69273\nOhio            40975\nOklahoma        68782\nOregon          96184\nPennsylvania    44966\nRhode Island     1049\nSouth Carolina  30225\nSouth Dakota    75955\nTennessee       41328\nTexas          262134\nUtah            82096\nVermont          9267\nVirginia        39780\nWashington      66570\nWest Virginia   24070\nWisconsin       54464\nWyoming         97203",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/WorkingWithRData.html",
    "href": "RIntro/WorkingWithRData.html",
    "title": "Working With R Data",
    "section": "",
    "text": "At the end of this lesson you should be able to\n\nMake vectors in R\nAccess parts of the vector using the [] operator.\n\nNumeric Indexes\nNegative Indexes\nLogical Indexes\nCharacter Indexes\n\nCheck types of object using is and mode functions.\nConvert types of object using as functions.\nAccess names elements of lists using $.\nAccess elements, row and columns of matrixes using [,]\nConvert between data frames and matrixes\nRead and write data frames using read.csv and write.csv.\n\nThis lesson covers the traditional R way of doing things. The next lesson will show tidyverse alternatives.",
    "crumbs": [
      "Working With R Data"
    ]
  },
  {
    "objectID": "RIntro/WorkingWithRData.html#basic-r-container-objects",
    "href": "RIntro/WorkingWithRData.html#basic-r-container-objects",
    "title": "Working With R Data",
    "section": "Basic R Container objects",
    "text": "Basic R Container objects\n\nVector – ordered collection of objects of the same storage mode ([ extract)\n\nNamed Vector – adds a names attribute (Can use names in subscripts)\nMatrix, Array – adds a dim and dimnames attribute\n\nList – ordered collection of objects of any type or mode ([[ extract)\n\nNamed List – add names attribute (Can use $ to extract elements)\nS3 Class – adds a class attribute\ndata.frame – a list of columns in a spreadsheet. Uses ([ or $ to extract).\ntibble – The tidyverse extension of a data frame.\n\nS4 Class – formal class mechanism. Uses @ instead of $.",
    "crumbs": [
      "Working With R Data"
    ]
  },
  {
    "objectID": "RIntro/WorkingWithRData.html#storage-modes.",
    "href": "RIntro/WorkingWithRData.html#storage-modes.",
    "title": "Working With R Data",
    "section": "Storage modes.",
    "text": "Storage modes.\nThe mode function in R refers to storage modes, not the mode of a distribution.\n\nmode(123)\n\n[1] \"numeric\"\n\nmode(123L)\n\n[1] \"numeric\"\n\nmode(TRUE)\n\n[1] \"logical\"\n\nmode(\"True\")\n\n[1] \"character\"\n\nmode(3.14)\n\n[1] \"numeric\"\n\nmode(t)\n\n[1] \"function\"\n\n?mode\n\n\nThe is.XXX functions can be used to check the type (mode or class) of an object.\nThe as.XXX functions can be used to convert between different types.\n\n\nis.integer(3)\n\n[1] FALSE\n\nis.integer(3L)\n\n[1] TRUE\n\nas.integer(3)\n\n[1] 3\n\nis.integer(as.integer(3))\n\n[1] TRUE\n\nas.integer(\"three\")\n\nWarning: NAs introduced by coercion\n\n\n[1] NA\n\nas.character(3)\n\n[1] \"3\"\n\nas.logical(3)\n\n[1] TRUE\n\n\nThe most commonly seen modes are:\n\nNumeric\n\nReal or double (the default)\nInteger (Putting an L after a number tells R that this should be an integer.)\nLogical (TRUE/T or FALSE/F)\n\nCharacter – Each element of a character vector is a string.\nAny – A vector of anything is a list; thus, almost all R objects are in fact vectors.",
    "crumbs": [
      "Working With R Data"
    ]
  },
  {
    "objectID": "RIntro/WorkingWithRData.html#factors",
    "href": "RIntro/WorkingWithRData.html#factors",
    "title": "Working With R Data",
    "section": "Factors",
    "text": "Factors\n\nThe factor and ordered classes also behave a lot like storage modes.\nAtucally, they are R classes where the data values are integers and there is a special property which gives the names of the levels.\nThe built-in data value state.region is a factor.\n\n*( The function head() lists the first 6 data points instead of all of them.)\n\nhead(state.region)\n\n[1] South West  West  South West  West \nLevels: Northeast South North Central West\n\nlevels(state.region)\n\n[1] \"Northeast\"     \"South\"         \"North Central\" \"West\"         \n\nhead(as.integer(state.region))\n\n[1] 2 4 4 2 4 4\n\nhead(as.character(state.region))\n\n[1] \"South\" \"West\"  \"West\"  \"South\" \"West\"  \"West\" \n\nunclass(state.region)\n\n [1] 2 4 4 2 4 4 1 2 2 2 4 4 3 3 3 3 2 2 1 2 1 3 3 2 3 4 3 4 1 1 4 1 2 3 3 2 4 1\n[39] 1 2 3 2 2 4 1 2 4 2 3 4\nattr(,\"levels\")\n[1] \"Northeast\"     \"South\"         \"North Central\" \"West\"         \n\n\n\nThe values of a factor variable are just labels,\n\nNumeric labels\n\nas.integer()\n\nString labels\n\nas.character()\n\n\nThe function as.factor() will force a numeric or character vector into a factor.\n\nR will just pick and arbitrary order (usually alphabetical) for labels.\nAlphabetical ordering doesn’t always work with as.ordered().\n\nHigh,Low,Medium\nUse the function ordered() with more control over the levels.\n\n\n\n\nhelp(ordered)\nofact &lt;- ordered(c(\"H\",\"M\",\"H\",\"L\",\"M\",\"H\"),levels=c(\"L\",\"M\",\"H\"))\nofact\n\n[1] H M H L M H\nLevels: L &lt; M &lt; H",
    "crumbs": [
      "Working With R Data"
    ]
  },
  {
    "objectID": "RIntro/WorkingWithRData.html#making-vectors",
    "href": "RIntro/WorkingWithRData.html#making-vectors",
    "title": "Working With R Data",
    "section": "Making vectors",
    "text": "Making vectors\nThe : operator produces sequences (of integers) between first and second argument. (The function seq() allows step sizes of other than one.)\n\n1:3\n\n[1] 1 2 3\n\n3:1\n\n[1] 3 2 1\n\n-1:1\n\n[1] -1  0  1\n\n-3:-1\n\n[1] -3 -2 -1\n\n\nThe c() function can be used to glue vectors together. (c stands for combine)\n\nc(1:3, 10:12)\n\n[1]  1  2  3 10 11 12\n\nc(\"Hansel\", \"Gretel\", \"Tedd\",\"Alice\")\n\n[1] \"Hansel\" \"Gretel\" \"Tedd\"   \"Alice\" \n\n\n\nImplicit Looping\nR implicitly loops over all the elements of a vector. Such implicit loops are faster than explicit for loops.\n\n1:11\n\n [1]  1  2  3  4  5  6  7  8  9 10 11\n\n(1:11)/2\n\n [1] 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5\n\nmean(1:11)\n\n[1] 6\n\nz &lt;- ((1:11) - mean(1:11))/sd(1:11)\nz\n\n [1] -1.5075567 -1.2060454 -0.9045340 -0.6030227 -0.3015113  0.0000000\n [7]  0.3015113  0.6030227  0.9045340  1.2060454  1.5075567\n\nmean (z)\n\n[1] 0\n\nsd(z)\n\n[1] 1\n\n\n\n\nRandom vectors\n\nR has a number of built in random number generators to generate random numbers.\nThe most commonly used are runif, rnorm and sample.\n\nSample has a replace option to do sampling with or without replacement.\n\nThere are also many others, with names that look like rXXX (try substituting chisq, t, beta, gamma, &c for XXX).\n\n\nrunif(5)\n\n[1] 0.8993509 0.3629690 0.9190707 0.2406019 0.0136735\n\nrnorm(10)\n\n [1]  0.8012908  1.4091884 -0.2956745 -0.3597769  2.0241834 -0.8597461\n [7]  0.6713195  0.9224147  1.0649169  0.3856146\n\nsample.int(5,5,replace=TRUE)\n\n[1] 3 4 4 1 5\n\n\n\n\nExercises\n\nGenerate 100 random numbers with mean 50 and standard deviation 25.\n\n1a. Use the result of the previous question to generate a random sample of size 101 with one outlier of 200.\n\nGenerate random integers between 0 and 100\n\n\nThe variable state.area contains the areas of the 50 US states (in alphabetical area). Create a random sample of size 10 of the state areas.",
    "crumbs": [
      "Working With R Data"
    ]
  },
  {
    "objectID": "RIntro/WorkingWithRData.html#three-ways-of-subscripting-a-vector",
    "href": "RIntro/WorkingWithRData.html#three-ways-of-subscripting-a-vector",
    "title": "Working With R Data",
    "section": "Three ways of subscripting a vector",
    "text": "Three ways of subscripting a vector\n\nThe [] operator is used to subscript vectors.\nThere are three different things you can put inside of the brackets:\n\nnumbers,\n\nnegative numbers (exclude values)\n\nlogical expressions\nnames (character values).\n\n\n\nNumeric Indexes\n\nNumbers are the most straightforward way to do indexing.\nR starts the indexes at 1 and it goes up to the length of the vector.\nThe function length() is useful in writing indexes.\nGiving multiple indexes with return a sub-vector (remember, there are no scalars in R, just vectors of length 1).\n\n\nint10 &lt;- 1:10\nint10[3]\n\n[1] 3\n\nint10[c(5:7,9)]\n\n[1] 5 6 7 9\n\nstate.area[c(1,length(state.area))]\n\n[1] 51609 97914\n\n\nAnother useful trick is to use negative indexes. These leave the numbered variables out.\n\nint10[-2]\n\n[1]  1  3  4  5  6  7  8  9 10\n\nint10[-(3:8)]\n\n[1]  1  2  9 10\n\n\nIndexing expressions can also be used on the LHS of assignment operators, to allow to assignment to just certain values.\n\nint10[3] &lt;- -3\nint10\n\n [1]  1  2 -3  4  5  6  7  8  9 10\n\n\n\n\nLogical Indexes\nThe second option for indexing is to use a logical vector the same length as the vector you are indexing.\n\nint10&lt;0\n\n [1] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\nint10[int10&lt;0]\n\n[1] -3\n\nint10[int10&lt;0] &lt;- abs (int10[int10&lt;0])\nint10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nBe careful with NAs.\n\nint55 &lt;- -5:5\nsqrt(int55) &lt; 1.2\n\nWarning in sqrt(int55): NaNs produced\n\n\n [1]    NA    NA    NA    NA    NA  TRUE  TRUE FALSE FALSE FALSE FALSE\n\nint55[sqrt(int55) &lt; 1.2]\n\nWarning in sqrt(int55): NaNs produced\n\n\n[1] NA NA NA NA NA  0  1\n\n\nThe real power of logical indexes comes when we have two vectors of the same length.\nFor example, state.abb gives the two letter postal codes of the states. Suppose we wanted to see all of the states that are bigger than average:\n\nstate.abb[state.area&gt;median(state.area)]\n\n [1] \"AK\" \"AZ\" \"CA\" \"CO\" \"FL\" \"GA\" \"ID\" \"IL\" \"IA\" \"KS\" \"MI\" \"MN\" \"MO\" \"MT\" \"NE\"\n[16] \"NV\" \"NM\" \"ND\" \"OK\" \"OR\" \"SD\" \"TX\" \"UT\" \"WA\" \"WY\"\n\n\n\n\nAside: ifelse and if\nThe built in language primitive if is not vectorized. It is expecting a single value. The code below will not do what you think it will.\n\ntry({\n  if (int55 &lt; 0) {\n    cat(\"Negative.\\n\")\n  } else { \n    cat(\"Non-negative.\\n\")\n  }\n})\n\nError in if (int55 &lt; 0) { : the condition has length &gt; 1\n\n\nThe functions any(), all() and isTRUE() are often useful here.\n\nif (all(int55 &gt;0)) {\n  cat(\"Positive.\\n\")\n} else { \n  cat(\"Not all positive.\\n\")\n}\n\nNot all positive.\n\n\nThe function ifelse() can be used to loop over if-else expressions.\n\nThere are two differences from if.\n\nFirst the condition is a logical vector.\nSecond, both the if-true and if-false argument are always evaluated, so they better not generate an error!\n\n\n\nifelse(int55&lt;0, \"-\",\"+\")\n\n [1] \"-\" \"-\" \"-\" \"-\" \"-\" \"+\" \"+\" \"+\" \"+\" \"+\" \"+\"\n\n\n\nintA &lt;- 1:10\nintA[3] &lt;- -3\nA&lt;- sqrt(intA)\n\nWarning in sqrt(intA): NaNs produced\n\nA\n\n [1] 1.000000 1.414214      NaN 2.000000 2.236068 2.449490 2.645751 2.828427\n [9] 3.000000 3.162278\n\nmean(A)\n\n[1] NaN\n\nmean(A,na.rm=TRUE)\n\n[1] 2.304025\n\n\n\n\nNames and character indexes\nIt would be really convenient if we could access the state data by name.\nFlorida is the 9 state alphabetically, but I can’t remember that.\nWhat we can do is add names to a vector. Then we can select by name.\n\nnames(state.area) &lt;- state.abb\nhead(state.area)\n\n    AL     AK     AZ     AR     CA     CO \n 51609 589757 113909  53104 158693 104247 \n\nhead(names(state.area))\n\n[1] \"AL\" \"AK\" \"AZ\" \"AR\" \"CA\" \"CO\"\n\nstate.area[\"FL\"]\n\n   FL \n58560 \n\nstate.area[c(\"NY\",\"CA\")]\n\n    NY     CA \n 49576 158693 \n\n\nSometimes we need to make up names.\nThe paste() command is handy for that.\nIt is vectorized, so you can put a bunch of numbers in.\n\npaste(\"Student\",1:5,sep=\"_\")\n\n[1] \"Student_1\" \"Student_2\" \"Student_3\" \"Student_4\" \"Student_5\"\n\n\n\n\nExercises\n\nWrite an expression that removes the outlier from the data you generated for 1b.\n\n\nSuppose the data you generated for problem 1 was suppose to have a minimum score of 0 and a maximum score of 100. Fix, the data set so that all of the values are between 0 and 100.\n\n\nFix my positive/negative test, so that it has a 0 as well\n\n\nFind all of the states that are bigger than Florida.\n\n\nGenerate a bunch of random integers between -10 and 10. Then turn all negative integers into NA.",
    "crumbs": [
      "Working With R Data"
    ]
  },
  {
    "objectID": "RIntro/WorkingWithRData.html#matrixes-and-arrays",
    "href": "RIntro/WorkingWithRData.html#matrixes-and-arrays",
    "title": "Working With R Data",
    "section": "Matrixes and Arrays",
    "text": "Matrixes and Arrays\n\nA matrix is an object with rows and columns.\nAn array can have any number of dimensions.\nBut they all the entries need to be the same type (mode).\nThere is a dim() attribute which shows the dimensions of the matrix.\n\n\ndim(state.x77)\n\n[1] 50  8\n\nhead(state.x77)\n\n           Population Income Illiteracy Life Exp Murder HS Grad Frost   Area\nAlabama          3615   3624        2.1    69.05   15.1    41.3    20  50708\nAlaska            365   6315        1.5    69.31   11.3    66.7   152 566432\nArizona          2212   4530        1.8    70.55    7.8    58.1    15 113417\nArkansas         2110   3378        1.9    70.66   10.1    39.9    65  51945\nCalifornia      21198   5114        1.1    71.71   10.3    62.6    20 156361\nColorado         2541   4884        0.7    72.06    6.8    63.9   166 103766\n\n\n\nGetting and setting dims\n\nThe dim() function is used to access the number of rows and columns.\ndim()[1] gets the number of rows\ndim()[2] gets the number of columns.\nFor matrixes, the functions nrow() and ncol() are easier to remember.\n\nSetting dim() will reshape a vector into a matrix or array.\n\nnrow(state.x77)\n\n[1] 50\n\nncol(state.x77)\n\n[1] 8\n\nint12 &lt;- 1:12\ndim(int12) &lt;- c(3,4)\nint12\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\n\n\n\nmatrix() and array() functions\n\nSetting the dim() attribute directly is not recommended (makes for hard to read code).\nInstead use matrix() or array()\nR stores matrixes in row major order (like FORTRAN, not like c).\n\nUse byrow=TRUE to reverse this in matrix or array\n\n\n\nmatrix(1:12,3,4)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\nmatrix(1:12,3,4,byrow=TRUE)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n\narray(1:24,c(2,3,4))\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]    7    9   11\n[2,]    8   10   12\n\n, , 3\n\n     [,1] [,2] [,3]\n[1,]   13   15   17\n[2,]   14   16   18\n\n, , 4\n\n     [,1] [,2] [,3]\n[1,]   19   21   23\n[2,]   20   22   24\n\n\n\n\nNumeric and logical indexes\nFor matrixes and arrays, the [] operator does something a little bit different. In particular, x[i,j] picks out row \\(i\\) and column \\(j\\).\nEither the row or column selector could be\n\nA number or vector of numbers (pick those rows or columns)\nA negative number of vector of negative numbers (excluded those rows or columns)\nA logical vector of size nrow(x) or ncol(x) (select the rows/columns corresponding to true).\nA character vector (select rows or columns by name, see below).\nLeft blank, in which case all rows/columns are selected.\n\nIf a single row or column is selected, then it turns into a vector.\n\nstate.x77[1:5,1:5]\n\n           Population Income Illiteracy Life Exp Murder\nAlabama          3615   3624        2.1    69.05   15.1\nAlaska            365   6315        1.5    69.31   11.3\nArizona          2212   4530        1.8    70.55    7.8\nArkansas         2110   3378        1.9    70.66   10.1\nCalifornia      21198   5114        1.1    71.71   10.3\n\nstate.x77[1:5,]\n\n           Population Income Illiteracy Life Exp Murder HS Grad Frost   Area\nAlabama          3615   3624        2.1    69.05   15.1    41.3    20  50708\nAlaska            365   6315        1.5    69.31   11.3    66.7   152 566432\nArizona          2212   4530        1.8    70.55    7.8    58.1    15 113417\nArkansas         2110   3378        1.9    70.66   10.1    39.9    65  51945\nCalifornia      21198   5114        1.1    71.71   10.3    62.6    20 156361\n\nstate.x77[9,]\n\nPopulation     Income Illiteracy   Life Exp     Murder    HS Grad      Frost \n   8277.00    4815.00       1.30      70.66      10.70      52.60      11.00 \n      Area \n  54090.00 \n\ndim(state.x77[9,])\n\nNULL\n\nhead(state.x77[,3])\n\n   Alabama     Alaska    Arizona   Arkansas California   Colorado \n       2.1        1.5        1.8        1.9        1.1        0.7 \n\nstate.x77[9,,drop=FALSE]\n\n        Population Income Illiteracy Life Exp Murder HS Grad Frost  Area\nFlorida       8277   4815        1.3    70.66   10.7    52.6    11 54090\n\ndim(state.x77[9,,drop=FALSE])\n\n[1] 1 8\n\n\n\n\ndimnames and character indexes\nTo use character indexes with matrixes, we need to set the rownames() and colnames() of the matrix.\nWe can also use the dimnames() (although this will produce a list).\n\nrownames(state.x77)\n\n [1] \"Alabama\"        \"Alaska\"         \"Arizona\"        \"Arkansas\"      \n [5] \"California\"     \"Colorado\"       \"Connecticut\"    \"Delaware\"      \n [9] \"Florida\"        \"Georgia\"        \"Hawaii\"         \"Idaho\"         \n[13] \"Illinois\"       \"Indiana\"        \"Iowa\"           \"Kansas\"        \n[17] \"Kentucky\"       \"Louisiana\"      \"Maine\"          \"Maryland\"      \n[21] \"Massachusetts\"  \"Michigan\"       \"Minnesota\"      \"Mississippi\"   \n[25] \"Missouri\"       \"Montana\"        \"Nebraska\"       \"Nevada\"        \n[29] \"New Hampshire\"  \"New Jersey\"     \"New Mexico\"     \"New York\"      \n[33] \"North Carolina\" \"North Dakota\"   \"Ohio\"           \"Oklahoma\"      \n[37] \"Oregon\"         \"Pennsylvania\"   \"Rhode Island\"   \"South Carolina\"\n[41] \"South Dakota\"   \"Tennessee\"      \"Texas\"          \"Utah\"          \n[45] \"Vermont\"        \"Virginia\"       \"Washington\"     \"West Virginia\" \n[49] \"Wisconsin\"      \"Wyoming\"       \n\ncolnames(state.x77)\n\n[1] \"Population\" \"Income\"     \"Illiteracy\" \"Life Exp\"   \"Murder\"    \n[6] \"HS Grad\"    \"Frost\"      \"Area\"      \n\ndimnames(state.x77)\n\n[[1]]\n [1] \"Alabama\"        \"Alaska\"         \"Arizona\"        \"Arkansas\"      \n [5] \"California\"     \"Colorado\"       \"Connecticut\"    \"Delaware\"      \n [9] \"Florida\"        \"Georgia\"        \"Hawaii\"         \"Idaho\"         \n[13] \"Illinois\"       \"Indiana\"        \"Iowa\"           \"Kansas\"        \n[17] \"Kentucky\"       \"Louisiana\"      \"Maine\"          \"Maryland\"      \n[21] \"Massachusetts\"  \"Michigan\"       \"Minnesota\"      \"Mississippi\"   \n[25] \"Missouri\"       \"Montana\"        \"Nebraska\"       \"Nevada\"        \n[29] \"New Hampshire\"  \"New Jersey\"     \"New Mexico\"     \"New York\"      \n[33] \"North Carolina\" \"North Dakota\"   \"Ohio\"           \"Oklahoma\"      \n[37] \"Oregon\"         \"Pennsylvania\"   \"Rhode Island\"   \"South Carolina\"\n[41] \"South Dakota\"   \"Tennessee\"      \"Texas\"          \"Utah\"          \n[45] \"Vermont\"        \"Virginia\"       \"Washington\"     \"West Virginia\" \n[49] \"Wisconsin\"      \"Wyoming\"       \n\n[[2]]\n[1] \"Population\" \"Income\"     \"Illiteracy\" \"Life Exp\"   \"Murder\"    \n[6] \"HS Grad\"    \"Frost\"      \"Area\"      \n\nrownames(state.x77) &lt;- state.abb\nhead(state.x77)\n\n   Population Income Illiteracy Life Exp Murder HS Grad Frost   Area\nAL       3615   3624        2.1    69.05   15.1    41.3    20  50708\nAK        365   6315        1.5    69.31   11.3    66.7   152 566432\nAZ       2212   4530        1.8    70.55    7.8    58.1    15 113417\nAR       2110   3378        1.9    70.66   10.1    39.9    65  51945\nCA      21198   5114        1.1    71.71   10.3    62.6    20 156361\nCO       2541   4884        0.7    72.06    6.8    63.9   166 103766\n\n\n\n\nRow and column sums and averages\nRemember that a matrix is just a vector with a dim attribute. Consequently, mean and other summary functions don’t do what we want:\n\nmean(state.x77)\n\n[1] 9956.887\n\nsd(state.x77)\n\n[1] 37801.78\n\nvar(state.x77)\n\n              Population        Income   Illiteracy      Life Exp       Murder\nPopulation 19931683.7588   571229.7796  292.8679592 -4.078425e+02  5663.523714\nIncome       571229.7796   377573.3061 -163.7020408  2.806632e+02  -521.894286\nIlliteracy      292.8680     -163.7020    0.3715306 -4.815122e-01     1.581776\nLife Exp       -407.8425      280.6632   -0.4815122  1.802020e+00    -3.869480\nMurder         5663.5237     -521.8943    1.5817755 -3.869480e+00    13.627465\nHS Grad       -3551.5096     3076.7690   -3.2354694  6.312685e+00   -14.549616\nFrost        -77081.9727     7227.6041  -21.2900000  1.828678e+01  -103.406000\nArea        8587916.9494 19049013.7510 4018.3371429 -1.229410e+04 71940.429959\n                 HS Grad        Frost          Area\nPopulation  -3551.509551 -77081.97265  8.587917e+06\nIncome       3076.768980   7227.60408  1.904901e+07\nIlliteracy     -3.235469    -21.29000  4.018337e+03\nLife Exp        6.312685     18.28678 -1.229410e+04\nMurder        -14.549616   -103.40600  7.194043e+04\nHS Grad        65.237894    153.99216  2.298732e+05\nFrost         153.992163   2702.00857  2.627039e+05\nArea       229873.192816 262703.89306  7.280748e+09\n\ncor(state.x77)\n\n            Population     Income  Illiteracy    Life Exp     Murder\nPopulation  1.00000000  0.2082276  0.10762237 -0.06805195  0.3436428\nIncome      0.20822756  1.0000000 -0.43707519  0.34025534 -0.2300776\nIlliteracy  0.10762237 -0.4370752  1.00000000 -0.58847793  0.7029752\nLife Exp   -0.06805195  0.3402553 -0.58847793  1.00000000 -0.7808458\nMurder      0.34364275 -0.2300776  0.70297520 -0.78084575  1.0000000\nHS Grad    -0.09848975  0.6199323 -0.65718861  0.58221620 -0.4879710\nFrost      -0.33215245  0.2262822 -0.67194697  0.26206801 -0.5388834\nArea        0.02254384  0.3633154  0.07726113 -0.10733194  0.2283902\n               HS Grad      Frost        Area\nPopulation -0.09848975 -0.3321525  0.02254384\nIncome      0.61993232  0.2262822  0.36331544\nIlliteracy -0.65718861 -0.6719470  0.07726113\nLife Exp    0.58221620  0.2620680 -0.10733194\nMurder     -0.48797102 -0.5388834  0.22839021\nHS Grad     1.00000000  0.3667797  0.33354187\nFrost       0.36677970  1.0000000  0.05922910\nArea        0.33354187  0.0592291  1.00000000\n\n\nTaking row and column sums are such a frequent operation, that there is a shortcut for them: rowSums(), colSums(), rowMeans(), colMeans()\n\ncolMeans(state.x77)\n\nPopulation     Income Illiteracy   Life Exp     Murder    HS Grad      Frost \n 4246.4200  4435.8000     1.1700    70.8786     7.3780    53.1080   104.4600 \n      Area \n70735.8800 \n\n\nThe summary function in the tidyverse package is another way to do this.\n\n\nExercises:\n\nFind the population for all states whose area is bigger than Florida’s.\n\n\nCalculate the population density (population per area) for each state\n\n\nTurn the state.x77 data into z-scores by subtracting the column means and dividing by the column standard deviations.\n\n\nScale the state.x77 data from 0 (minimum in the column) to 1 (maximum in the column).",
    "crumbs": [
      "Working With R Data"
    ]
  },
  {
    "objectID": "RIntro/WorkingWithRData.html#lists",
    "href": "RIntro/WorkingWithRData.html#lists",
    "title": "Working With R Data",
    "section": "Lists",
    "text": "Lists\n\nA list in R is a special vector whose elements can be anything, even other lists.\nIt is possible to build up quite complex objects from lists (Old S3 class system.)\n\nUse the `list() constructor to make lists\n\nlist(1,2:3,\"four\",quote(2+3))\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2 3\n\n[[3]]\n[1] \"four\"\n\n[[4]]\n2 + 3\n\n\nNotice that the second element is a vector and the last element is an R expression (this is what quote does). R lists are quite flexible.\n\nNotice that the list is show with a double square bracket [[ instead of a single one [. This is because with lists the extraction operators behave a little bit differently. The single bracket refers to a sublist, and the double bracket to the element. Fortunately, this doesn’t come up a lot at the beginning because, most people use the $ extractors instead.",
    "crumbs": [
      "Working With R Data"
    ]
  },
  {
    "objectID": "RIntro/WorkingWithRData.html#named-lists-and-extraction",
    "href": "RIntro/WorkingWithRData.html#named-lists-and-extraction",
    "title": "Working With R Data",
    "section": "Named Lists and $ extraction",
    "text": "Named Lists and $ extraction\nNamed lists have a special role in R. They are similar to environments in that they allow the analyst to associate names and values. If x is a list then x[[name]] or x$name will retrieve (or set if used with &lt;-) that element.\n\nalist &lt;- list(one=1, two=2:3, three=\"three\", four=quote(2+2))\nalist\n\n$one\n[1] 1\n\n$two\n[1] 2 3\n\n$three\n[1] \"three\"\n\n$four\n2 + 2\n\nalist$two\n\n[1] 2 3\n\nalist$two &lt;- 2\nalist\n\n$one\n[1] 1\n\n$two\n[1] 2\n\n$three\n[1] \"three\"\n\n$four\n2 + 2",
    "crumbs": [
      "Working With R Data"
    ]
  },
  {
    "objectID": "RIntro/WorkingWithRData.html#lists-and-classes",
    "href": "RIntro/WorkingWithRData.html#lists-and-classes",
    "title": "Working With R Data",
    "section": "Lists and Classes",
    "text": "Lists and Classes\nThis ability to associate names and values is very hand. The older S3 (informal) class system just uses lists with appropriate values as classes. To get components, just use the $ operator.\nFor example, the function lm() does a regression and returns an object of class lm. The $ operator can be used to access its components.\n\nfit1 &lt;- lm(dist~speed,data=cars)\nfit1$coefficients\n\n(Intercept)       speed \n -17.579095    3.932409",
    "crumbs": [
      "Working With R Data"
    ]
  },
  {
    "objectID": "RIntro/WorkingWithRData.html#data-frame",
    "href": "RIntro/WorkingWithRData.html#data-frame",
    "title": "Working With R Data",
    "section": "Data frame",
    "text": "Data frame\n\nA data frame is a list that behaves like a matrix.\n\nA data frame is a list of columns with a class of data.frame.\n\nDifferent columns can have different classes or storage modes.\n\nMatrixes and arrays all must be the same kind of value.\n\nUsing the single square bracket [i,j] can reference row i and column j, like a matrix.\nUsing the $ operator can reference columns.\n\n\n?mtcars\nnames(mtcars)  # Get the variable names\n\n [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n[11] \"carb\"\n\nrownames(mtcars) # Get the car names\n\n [1] \"Mazda RX4\"           \"Mazda RX4 Wag\"       \"Datsun 710\"         \n [4] \"Hornet 4 Drive\"      \"Hornet Sportabout\"   \"Valiant\"            \n [7] \"Duster 360\"          \"Merc 240D\"           \"Merc 230\"           \n[10] \"Merc 280\"            \"Merc 280C\"           \"Merc 450SE\"         \n[13] \"Merc 450SL\"          \"Merc 450SLC\"         \"Cadillac Fleetwood\" \n[16] \"Lincoln Continental\" \"Chrysler Imperial\"   \"Fiat 128\"           \n[19] \"Honda Civic\"         \"Toyota Corolla\"      \"Toyota Corona\"      \n[22] \"Dodge Challenger\"    \"AMC Javelin\"         \"Camaro Z28\"         \n[25] \"Pontiac Firebird\"    \"Fiat X1-9\"           \"Porsche 914-2\"      \n[28] \"Lotus Europa\"        \"Ford Pantera L\"      \"Ferrari Dino\"       \n[31] \"Maserati Bora\"       \"Volvo 142E\"         \n\nmtcars[1:5,] # First five rows\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n\nmtcars[\"Honda Civic\", ]  # Just one car\n\n             mpg cyl disp hp drat    wt  qsec vs am gear carb\nHonda Civic 30.4   4 75.7 52 4.93 1.615 18.52  1  1    4    2\n\nmtcars[,\"mpg\"] # Just MPG variable\n\n [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4\n[16] 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7\n[31] 15.0 21.4\n\nmtcars$disp  # Just DISP variable\n\n [1] 160.0 160.0 108.0 258.0 360.0 225.0 360.0 146.7 140.8 167.6 167.6 275.8\n[13] 275.8 275.8 472.0 460.0 440.0  78.7  75.7  71.1 120.1 318.0 304.0 350.0\n[25] 400.0  79.0 120.3  95.1 351.0 145.0 301.0 121.0",
    "crumbs": [
      "Working With R Data"
    ]
  },
  {
    "objectID": "RIntro/WorkingWithRData.html#data.frame-as.matrix-and-as.data.frame",
    "href": "RIntro/WorkingWithRData.html#data.frame-as.matrix-and-as.data.frame",
    "title": "Working With R Data",
    "section": "data.frame(), as.matrix and as.data.frame",
    "text": "data.frame(), as.matrix and as.data.frame\nThe function data.frame() will put a data frame together column by column. (If one of the arguments is a matrix each column in the matrix will become a column in the data frame.)\n\nstateX77 &lt;- data.frame(state.x77,region=state.region,row.names=state.abb)\nstateX77\n\n   Population Income Illiteracy Life.Exp Murder HS.Grad Frost   Area\nAL       3615   3624        2.1    69.05   15.1    41.3    20  50708\nAK        365   6315        1.5    69.31   11.3    66.7   152 566432\nAZ       2212   4530        1.8    70.55    7.8    58.1    15 113417\nAR       2110   3378        1.9    70.66   10.1    39.9    65  51945\nCA      21198   5114        1.1    71.71   10.3    62.6    20 156361\nCO       2541   4884        0.7    72.06    6.8    63.9   166 103766\nCT       3100   5348        1.1    72.48    3.1    56.0   139   4862\nDE        579   4809        0.9    70.06    6.2    54.6   103   1982\nFL       8277   4815        1.3    70.66   10.7    52.6    11  54090\nGA       4931   4091        2.0    68.54   13.9    40.6    60  58073\nHI        868   4963        1.9    73.60    6.2    61.9     0   6425\nID        813   4119        0.6    71.87    5.3    59.5   126  82677\nIL      11197   5107        0.9    70.14   10.3    52.6   127  55748\nIN       5313   4458        0.7    70.88    7.1    52.9   122  36097\nIA       2861   4628        0.5    72.56    2.3    59.0   140  55941\nKS       2280   4669        0.6    72.58    4.5    59.9   114  81787\nKY       3387   3712        1.6    70.10   10.6    38.5    95  39650\nLA       3806   3545        2.8    68.76   13.2    42.2    12  44930\nME       1058   3694        0.7    70.39    2.7    54.7   161  30920\nMD       4122   5299        0.9    70.22    8.5    52.3   101   9891\nMA       5814   4755        1.1    71.83    3.3    58.5   103   7826\nMI       9111   4751        0.9    70.63   11.1    52.8   125  56817\nMN       3921   4675        0.6    72.96    2.3    57.6   160  79289\nMS       2341   3098        2.4    68.09   12.5    41.0    50  47296\nMO       4767   4254        0.8    70.69    9.3    48.8   108  68995\nMT        746   4347        0.6    70.56    5.0    59.2   155 145587\nNE       1544   4508        0.6    72.60    2.9    59.3   139  76483\nNV        590   5149        0.5    69.03   11.5    65.2   188 109889\nNH        812   4281        0.7    71.23    3.3    57.6   174   9027\nNJ       7333   5237        1.1    70.93    5.2    52.5   115   7521\nNM       1144   3601        2.2    70.32    9.7    55.2   120 121412\nNY      18076   4903        1.4    70.55   10.9    52.7    82  47831\nNC       5441   3875        1.8    69.21   11.1    38.5    80  48798\nND        637   5087        0.8    72.78    1.4    50.3   186  69273\nOH      10735   4561        0.8    70.82    7.4    53.2   124  40975\nOK       2715   3983        1.1    71.42    6.4    51.6    82  68782\nOR       2284   4660        0.6    72.13    4.2    60.0    44  96184\nPA      11860   4449        1.0    70.43    6.1    50.2   126  44966\nRI        931   4558        1.3    71.90    2.4    46.4   127   1049\nSC       2816   3635        2.3    67.96   11.6    37.8    65  30225\nSD        681   4167        0.5    72.08    1.7    53.3   172  75955\nTN       4173   3821        1.7    70.11   11.0    41.8    70  41328\nTX      12237   4188        2.2    70.90   12.2    47.4    35 262134\nUT       1203   4022        0.6    72.90    4.5    67.3   137  82096\nVT        472   3907        0.6    71.64    5.5    57.1   168   9267\nVA       4981   4701        1.4    70.08    9.5    47.8    85  39780\nWA       3559   4864        0.6    71.72    4.3    63.5    32  66570\nWV       1799   3617        1.4    69.48    6.7    41.6   100  24070\nWI       4589   4468        0.7    72.48    3.0    54.5   149  54464\nWY        376   4566        0.6    70.29    6.9    62.9   173  97203\n          region\nAL         South\nAK          West\nAZ          West\nAR         South\nCA          West\nCO          West\nCT     Northeast\nDE         South\nFL         South\nGA         South\nHI          West\nID          West\nIL North Central\nIN North Central\nIA North Central\nKS North Central\nKY         South\nLA         South\nME     Northeast\nMD         South\nMA     Northeast\nMI North Central\nMN North Central\nMS         South\nMO North Central\nMT          West\nNE North Central\nNV          West\nNH     Northeast\nNJ     Northeast\nNM          West\nNY     Northeast\nNC         South\nND North Central\nOH North Central\nOK         South\nOR          West\nPA     Northeast\nRI     Northeast\nSC         South\nSD North Central\nTN         South\nTX         South\nUT          West\nVT     Northeast\nVA         South\nWA          West\nWV         South\nWI North Central\nWY          West\n\nstateX77$Income\n\n [1] 3624 6315 4530 3378 5114 4884 5348 4809 4815 4091 4963 4119 5107 4458 4628\n[16] 4669 3712 3545 3694 5299 4755 4751 4675 3098 4254 4347 4508 5149 4281 5237\n[31] 3601 4903 3875 5087 4561 3983 4660 4449 4558 3635 4167 3821 4188 4022 3907\n[46] 4701 4864 3617 4468 4566\n\n\nThe functions as.data.frame() and as.matrix() can be used to go back and forth between the two different representations.\n\nAll matrixes can be converted to data frames, but data frames can only be converted to matrixes if all of the variables are the same type.\nThere are certain mathematical operators (like taking the inverse) which only work on matrixes.\n\nFor most of what I do in R, the data frame is the most convenient representation for data.\nThe tidyverse package uses the tibble instead of the data.frame. A tibble is a new class for data frames which has slightly more intelligence printing and more consistent subseting behavior.",
    "crumbs": [
      "Working With R Data"
    ]
  },
  {
    "objectID": "RIntro/WorkingWithRData.html#read.table-and-read.csv",
    "href": "RIntro/WorkingWithRData.html#read.table-and-read.csv",
    "title": "Working With R Data",
    "section": "read.table and read.csv",
    "text": "read.table and read.csv\nMost common format for storing data is tab separated value (.dat) and comma separated value (.csv).\n\nCases are rows\nVariables are separated by tab or comma\nOften a header row giving variable names\nSometimes there are row names.\nSometimes quotes are used for strings\n\nThe functions read.table() and read.csv() read these data files and produce data frames. * Really the same function with different options. * Many options, look at the help!!\n\nhelp(read.table)\n\nThese functions automatically convert strings to factors. The as.is optional argument suppresses that. Often factors, dates and missing values need to be cleaned up after reading in the data. (More about this in the next lesson).\nWindows Only. Usually both .dat and .csv files are mapped to open in Excel when you double click on them. If the file is open in Excel, then Windows will lock the file and not let another program read it. You may need to close the file in Excel before you can read it into R.\nThe functions write.table() and write.csv() go in the opposite directions.\nThe tidyverse alternative is read_csv(). It might be somewhat easier to use, but it produces tibbles instead of data frames. More about this in the next session.",
    "crumbs": [
      "Working With R Data"
    ]
  },
  {
    "objectID": "RIntro/WorkingWithRData.html#foreign-interfaces",
    "href": "RIntro/WorkingWithRData.html#foreign-interfaces",
    "title": "Working With R Data",
    "section": "Foreign interfaces",
    "text": "Foreign interfaces\nR can read data from an other packages, but you need to load the foreign package first.\n\nlibrary(foreign) (Part of the base R distribution)\n\nread.spss (SPSS)\nread.dta (Stata)\nread.ssd (SAS)\nread.systat (Systat)\n\n\nExcel workbooks are another common format. The easiest way to work with Excel data is to save it in .csv format from Excel. You could also try the xlsx package (need to install it first).\n\nlibrary(xlsx) (Need to install from CRAN)\n\nread.xlsx (Excel)\n\n\nThe book R for Data Science (Grolemund and Wickham, 2017) recommends the haven and readxl packages. Also, the DBI package allows importing data directly from databases (an advanced R trick).\n\nExercises\nUse the function write.csv() to write out the stateX77 data we made. Read it into Excel (or another spreadsheet) make some changes. Now read the modified version back into R.",
    "crumbs": [
      "Working With R Data"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "These are a series of small shiny apps I developed for teaching. Please feel free to use them in your courses or for your personal learning.\nAs learning from my R code is part of the learning, the source for these pages can be found on my github account: github:ralmond/TeachingDemos.\nIf you see a problem feel free to raise an issue on github (or better yet, send me a pull request with a fix for the issue.)\nAll pages are Copyright 2026 Russell G. Almond. Permission to reuse and remix granted under the basis of the CC-BY 4.0 license.",
    "crumbs": [
      "Home",
      "About"
    ]
  },
  {
    "objectID": "IntroStats/SlopeStandardErrors.html",
    "href": "IntroStats/SlopeStandardErrors.html",
    "title": "Slope Variation",
    "section": "",
    "text": "n &lt;- 1000\nr &lt;- .6\nx &lt;- rnorm(n)\ny &lt;- r*x + sqrt(1-r^2)*rnorm(n)\ndat &lt;- data.frame(x,y)\n\nfit1 &lt;- lm(y~x,data=dat)\n\nplot(x,y,type=\"n\")\ncoef.sim &lt;- coef(sim(fit1))\nfor (i in 1:nrow(coef.sim)) {\n  abline(a=coef.sim[i,1],b=coef.sim[i,2],col=\"gray50\")\n}\npoints(x,y)\nabline(fit1)\n\n\n\n\n\n\n\nplot(coef.sim)\n\n\n\n\n\n\n\ncor(coef.sim)\n\n             (Intercept)            x\n(Intercept)  1.000000000 -0.009521679\nx           -0.009521679  1.000000000"
  },
  {
    "objectID": "IntroStats/GammaParams.html",
    "href": "IntroStats/GammaParams.html",
    "title": "Normal Parameters",
    "section": "",
    "text": "The exponential distribution is a distribution often used for waiting times. Suppose the expected time to the next arrival is \\(\\theta\\). Then the probability that person will come at exactly time \\(x\\) is \\(f(x|\\theta) = \\frac{1}{\\theta}e^{-x/\\theta}\\). The exponential distribution has some interesting properties. In particular, if you have already waited for time period \\(z\\), then the conditional expectation is \\(z+\\theta\\).\nSuppose instead of waiting for one event, we wait for \\(k\\) events. Then we get the gamma distribution with shape parameter \\(k\\) and scale parameter \\(\\theta\\). Its probability density function is: \\[ f(x|k,\\theta) = \\frac{1}{\\Gamma(k)\\theta^k}x^{k-1}e^{-x/\\theta}\\]\nThe expected value is \\(k\\theta\\) and the standard deviation is \\(k\\theta^2\\).\n\n\n\n\n\n\nShape parameter\n\n\n\n\n\nScale parameter\n\n\n\n\n\n\n\n\n\n\nBe somewhat careful when using the gamma distribution in R. The gamma distribution is often parameterized using the rate parameter \\(\\beta=1/theta\\). If you are using the scale parameter, you need to name it explicitly and not rely on the position.\nIf the shape parameter is 1, then the gamma distribution is just the exponential distribution. It is extremely positively skewed. As the shape parameter increases, the gamma distribution becomes more and more symmetric, eventually converging to the normal distribution.\nThe chi-squared distribution is also a special case of the gamma distribution, with parameters \\(k=\\nu/2\\) and \\(\\theta=\\nu\\) (where \\(\\nu\\) is the degrees of freedom). Therefore, the gamma distribution is often used to model variances."
  },
  {
    "objectID": "IntroStats/CorrelationOutliers.html",
    "href": "IntroStats/CorrelationOutliers.html",
    "title": "Correlation Coefficient",
    "section": "",
    "text": "This demonstration will use some random data. Lets start by generating the random data. So give a random seed and pick a sample size for your sample.\nSample Size:\n\n10\n25\n50\n100\n250\n500\n\n\n\n\n\n\nRandom number Seed (integer)"
  },
  {
    "objectID": "IntroStats/CorrelationOutliers.html#the-effect-of-outliers.",
    "href": "IntroStats/CorrelationOutliers.html#the-effect-of-outliers.",
    "title": "Correlation Coefficient",
    "section": "The effect of outliers.",
    "text": "The effect of outliers.\nThe data set below has \\(N\\) data points. The sliders are hooked up to the first one (which is plotted in red). The rest are generated from a normal distribution with mean 0 and standard deviation 1. They are uncorrelated, but there often will be a small correlation because of sampling variability.\n\n\n\n\n\n\nX-coordinate of point 1:\n\n\n\n\n\nY-coordinate of point 1:\n\n\n\n\n\n\n\n\n\n\n\nOutliers in Y\nSet the \\(X\\) value for the red point to zero. Now move the \\(Y\\) value up and down. How sensitive is the correlation to changes in the \\(Y\\) value with \\(X=\\bar X\\)?\n\n\nLeverage Points: Outliers in X\nNow set \\(X\\) to a high value (away from the mean at 0). Again move \\(Y\\) up and down, what happens to the line? Set \\(X\\) to a low value and try again.\nValues which are outliers in the \\(X\\) variable (or in the case of multiple regression, one or more of the \\(X\\) variables) are known as leverage points or influential points.\n\n\nSample Size\nNow try changing the sample size in the dialogues at the top of the page. Note that you will need to tweak one of the sliders for the graph to redraw at the new sample size. Is the correlation more or less sensitive at low sample sizes? At high sample sizes?\nAt the low sample size, set the first data point somewhere close to \\((0,0)\\). It should have little effect on the correlation. Now change the seed (and tweak the point). What happens to the correlation with a new sample? Try that again! Now try it with higher sample sizes.\n\n\nWhat to do about outliers\nThere are four reasons that there might be an outlier in a data set:\n\nSomething went wrong in the data entry. Somebody left our a decimal point, or hit an extra key on the keyboard. Or maybe a number got put in the wrong column, so the subject’s shoe size was entered in place of the subject’s IQ.\nSomething went wrong in the data collection. I had a friend who used to hook research subjects up to the Vitalog monitoring pack. It included a probe for body temperature. Sometimes the probe would read 72 degrees F (room temperature) instead of 98 degrees F (body temperature). They called this “probe slippage.”\nThere is a person in the sample who really doesn’t belong there. For example, the teacher took the test along with the students, and somehow the teacher’s answers were mixed in with th student answers.\nThere is a member of population that is just a little bit different. Maybe they belong to a rare subpopulation.\n\nIf the experimenters took good records, problems of Type 1 can be corrected (also, problems of Type 3 clearly identified). If not, it may still be possible to identify that the value is clearly out of range and needs to be eliminated (for example, an SAT score of 0, when the minimum SAT score is 200). The same thing is true for errors of Type 2. When the value is clearly out of range that is the best solution, although you need to be careful that the missingness is not related to what is being studied (for example, patients dropping out of a drug trial because of the side effects).\nIn the absence of good records Type 3 and Type 4 outliers are hard to distinguish. Often what we want to do is to take the data point out and run the analysis again. Then we can compare the two correlation coefficients. If the difference is small, no problem. If the difference is big, we can report that we have an influential point and let the reader come to his or her own conclusion."
  },
  {
    "objectID": "IntroStats/SkewnessPractice.html",
    "href": "IntroStats/SkewnessPractice.html",
    "title": "Skewness Practice",
    "section": "",
    "text": "In this exercise, the computer will generate 3 datasets: A, B and C. These will be randomly assigned to a positively skewed, negatively skewed, and symmetric distribution type. Each will be plotted with a normal curve on top for reference. Your job is to determine which is which.\nYou can redraw from the same distributions by changing the sample size.\n\n\n\n\n\n\nSample Size:\n\n50\n100\n500\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentify the skewness of each distribution.\n\n\n\n\n\n\nA\n\nUnknown\nNegatively Skewed\nPositively Skewed\nSymmetric\n\n\n\n\n\n\nB\n\nUnknown\nNegatively Skewed\nPositively Skewed\nSymmetric\n\n\n\n\n\n\nC\n\nUnknown\nNegatively Skewed\nPositively Skewed\nSymmetric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo try again with different distributions, reload the page. If you are having trouble, try increasing the sample size: sometimes a small sample won’t display the characteristics of the distribution strongly.\nHere are the other exercises in this series:\n\nSkewness Practice:\n\nHistograms\nBoxplots\nQ-Q Plots\n\nKurtosis Practice:\n\nHistograms\nBoxplots\nQ-Q Plots"
  },
  {
    "objectID": "IntroStats/SkewnessPractice.html#skewness-determination-exercise.",
    "href": "IntroStats/SkewnessPractice.html#skewness-determination-exercise.",
    "title": "Skewness Practice",
    "section": "",
    "text": "In this exercise, the computer will generate 3 datasets: A, B and C. These will be randomly assigned to a positively skewed, negatively skewed, and symmetric distribution type. Each will be plotted with a normal curve on top for reference. Your job is to determine which is which.\nYou can redraw from the same distributions by changing the sample size.\n\n\n\n\n\n\nSample Size:\n\n50\n100\n500\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentify the skewness of each distribution.\n\n\n\n\n\n\nA\n\nUnknown\nNegatively Skewed\nPositively Skewed\nSymmetric\n\n\n\n\n\n\nB\n\nUnknown\nNegatively Skewed\nPositively Skewed\nSymmetric\n\n\n\n\n\n\nC\n\nUnknown\nNegatively Skewed\nPositively Skewed\nSymmetric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo try again with different distributions, reload the page. If you are having trouble, try increasing the sample size: sometimes a small sample won’t display the characteristics of the distribution strongly.\nHere are the other exercises in this series:\n\nSkewness Practice:\n\nHistograms\nBoxplots\nQ-Q Plots\n\nKurtosis Practice:\n\nHistograms\nBoxplots\nQ-Q Plots"
  },
  {
    "objectID": "IntroStats/Z-scores.html",
    "href": "IntroStats/Z-scores.html",
    "title": "Standardized Variables",
    "section": "",
    "text": "A (interval or ratio) variable on a raw score can be standardized to have mean 0 and standard deviation 1 by simply subtracting the mean and dividing by the standard deviation. This formula come in two flavors: one using the population mean and standard deviation (mu and sigma) and one using the sample statistics (x-bar and s). The subscripts are to remind you what variable you are using, as there is often both an X and Y wandering around.\n\\[ z = \\frac{x-\\mu_X}{\\sigma_X}; \\qquad Z = \\frac{X-\\bar X}{s_X} \\]\n\n\n\n\n\n\nMean of X:\n\n\n\n\n\nStandard Deviation of X:\n\n\n\n\n\nx:\n\n\n\n\n\n\n\n\n\n\nOften the next step is to look up the Z score on a normal calculator."
  },
  {
    "objectID": "IntroStats/Z-scores.html#standardizing-a-raw-score.",
    "href": "IntroStats/Z-scores.html#standardizing-a-raw-score.",
    "title": "Standardized Variables",
    "section": "",
    "text": "A (interval or ratio) variable on a raw score can be standardized to have mean 0 and standard deviation 1 by simply subtracting the mean and dividing by the standard deviation. This formula come in two flavors: one using the population mean and standard deviation (mu and sigma) and one using the sample statistics (x-bar and s). The subscripts are to remind you what variable you are using, as there is often both an X and Y wandering around.\n\\[ z = \\frac{x-\\mu_X}{\\sigma_X}; \\qquad Z = \\frac{X-\\bar X}{s_X} \\]\n\n\n\n\n\n\nMean of X:\n\n\n\n\n\nStandard Deviation of X:\n\n\n\n\n\nx:\n\n\n\n\n\n\n\n\n\n\nOften the next step is to look up the Z score on a normal calculator."
  },
  {
    "objectID": "IntroStats/Z-scores.html#going-from-a-standard-z-score-to-a-raw-score.",
    "href": "IntroStats/Z-scores.html#going-from-a-standard-z-score-to-a-raw-score.",
    "title": "Standardized Variables",
    "section": "Going from a standard (z) score to a raw score.",
    "text": "Going from a standard (z) score to a raw score.\nSolving the above equations for X allows the z-score to be translated back into a raw score. Often, a new variable is needed, so lets change the variables from X to Y. Once again, there are two variants based on whether sample or population means and standard deviations are used:\n\\[ y = \\sigma_Y z + \\mu_Y\\, ; \\qquad Y = s_Y Z + \\bar{Y}\\ .\\]\n\n\n\n\n\n\nMean of Y:\n\n\n\n\n\nStandard Deviation of Y:\n\n\n\n\n\nz:\n\n\n\n\n\n\n\n\n\n\n\n\nNote that these formulae are well worth memorizing, as they will come up over and over again."
  },
  {
    "objectID": "IntroStats/TestCI.html",
    "href": "IntroStats/TestCI.html",
    "title": "Confidence Intervals and Tests",
    "section": "",
    "text": "Suppose we are trying to find out the mean of a certain population, \\(\\mu\\). For example, suppose we are interested in the game eRebuild (https://mileresearch.coe.fsu.edu/erebuild) which aims to teach middle school students mathematics. Here \\(\\mu\\) would be how much a math a middle school student learns by playing the game. That is our target of inference.\nWe will make three simplifying assumptions:\n\nWe can measure “math”"
  },
  {
    "objectID": "IntroStats/BinomialParms.html",
    "href": "IntroStats/BinomialParms.html",
    "title": "Binomial Parameters",
    "section": "",
    "text": "The binomial distribution can be thought of as a number of draws, \\(n\\), from an urn with a proportion \\(p\\), of black balls.\nThe probability of drawing exactly \\(x\\) balls from an this urn is: \\[ p(X|n,p) = \\binom{n}{X} p^X (1-p)^{n-X}\\]\nThe expected value is \\(np\\), and the standard deviation is \\(\\sqrt{np(1-p)}\\).\nSometimes we write this in terms of the proportion of black balls in the sample. That is \\(p\\), with a standard deviation of \\(\\sqrt{p(1-p)/n}\\).\n\n\n\n\n\n\nNumber of draws:\n\n\n\n\n\nProbability of success:\n\n\n\n\n\n\n\n\n\n\nNote that this distribution is positively skewed if \\(p &lt; 0.5\\) and negatively skewed if \\(p &gt; 0.5\\).\nNote how when \\(n\\) gets large, the binomial distribution looks a lot like the normal. This is one of the first central limit theorems that was discovered. (The closer that \\(p\\) is to 0 or 1, the longer convergence to the normal takes.)"
  },
  {
    "objectID": "IntroStats/Studenttcalculator.html",
    "href": "IntroStats/Studenttcalculator.html",
    "title": "Student’s Calculator",
    "section": "",
    "text": "In this tool, you input a \\(t\\) score and the degrees of freedom, and get a corresponding \\(p\\)-value.\n\n\n\n\n\n\nWhich tails\n\nUpper tail: Pr(t &lt; T)\nLower tail: Pr(T &lt; t)\nBoth tails: Pr(T &lt;-t or t&lt; T)\nMiddle: Pr(-t &lt; T &lt; t)\n\n\n\n\n\n\nt-value:\n\n\n\n\n\nDegrees of Freedom (n-1 or n1 + n2 -2)"
  },
  {
    "objectID": "IntroStats/Studenttcalculator.html#students-t-probabilities.",
    "href": "IntroStats/Studenttcalculator.html#students-t-probabilities.",
    "title": "Student’s Calculator",
    "section": "",
    "text": "In this tool, you input a \\(t\\) score and the degrees of freedom, and get a corresponding \\(p\\)-value.\n\n\n\n\n\n\nWhich tails\n\nUpper tail: Pr(t &lt; T)\nLower tail: Pr(T &lt; t)\nBoth tails: Pr(T &lt;-t or t&lt; T)\nMiddle: Pr(-t &lt; T &lt; t)\n\n\n\n\n\n\nt-value:\n\n\n\n\n\nDegrees of Freedom (n-1 or n1 + n2 -2)"
  },
  {
    "objectID": "IntroStats/Studenttcalculator.html#students-t-quantiles-critical-values.",
    "href": "IntroStats/Studenttcalculator.html#students-t-quantiles-critical-values.",
    "title": "Student’s Calculator",
    "section": "Student’s t Quantiles (Critical values).",
    "text": "Student’s t Quantiles (Critical values).\nIn this tool, you input a probability and degrees of freedom, and get a corresponding t score.\n\n\n\n\n\n\nWhich tails\n\nUpper tail: Pr(t &lt; T)\nLower tail: Pr(T &lt; t)\nBoth tails: Pr(T &lt;-t or t&lt; T)\nMiddle: Pr(-t &lt; T &lt; t)\n\n\n\n\n\n\nProbability of shaded region:\n\n\n\n\n\nDegrees of Freedom (n-1 or n1 + n2 -2)"
  },
  {
    "objectID": "IntroStats/StandardDeviations.html",
    "href": "IntroStats/StandardDeviations.html",
    "title": "Standard Deviations",
    "section": "",
    "text": "This R Markdown document is made interactive using Shiny. Unlike the more traditional workflow of creating static reports, you can now create documents that allow your readers to change the assumptions underlying your analysis and see the results immediately.\nTo learn more, see Interactive Documents."
  },
  {
    "objectID": "IntroStats/StandardDeviations.html#inputs-and-outputs",
    "href": "IntroStats/StandardDeviations.html#inputs-and-outputs",
    "title": "Standard Deviations",
    "section": "Inputs and Outputs",
    "text": "Inputs and Outputs\nYou can embed Shiny inputs and outputs in your document. Outputs are automatically updated whenever inputs change. This demonstrates how a standard R plot can be made interactive by wrapping it in the Shiny renderPlot function. The selectInput and sliderInput functions create the input widgets used to drive the plot.\n\ndata &lt;- rnorm(15,mean=0,sd=2)\nplot(data,1:length(data))\nabline(v=0)\nsegments(mean(data),1:length(data),data,1:length(data))\n\n\n\n\n\n\n\nsd(data)\n\n[1] 2.41615\n\nhist(data)\n\n\n\n\n\n\n\n\n\\[ \\sqrt(\\sum (X_i - \\mu)^2/N) \\]\n\ndata1 &lt;- c(11,13,14,15,17,18,19,20)\ndata2 &lt;- c(12,15,15,15,15,15,15,15)\ndata3 &lt;- c(5,13,19,24,33,38,51,70)\ndata4 &lt;- c(11,12,13,16,18,20,22,23)\npar(mfrow=c(2,2))\nplot(data1,1:length(data1),main=\"Data 1\",xlim=c(0,50))\nabline(v=mean(data1))\nsegments(mean(data1),1:length(data1),data1,1:length(data1))\nsd(data1)\n\n[1] 3.136764\n\nplot(data2,1:length(data2),main=\"Data 2\",xlim=c(0,50))\nabline(v=mean(data2))\nsegments(mean(data2),1:length(data2),data2,1:length(data2))\nsd(data2)\n\n[1] 1.06066\n\nplot(data3,1:length(data3),main=\"Data 3\",xlim=c(0,50))\nabline(v=mean(data3))\nsegments(mean(data3),1:length(data3),data3,1:length(data3))\nsd(data3)\n\n[1] 21.25987\n\nplot(data4,1:length(data4),main=\"Data 4\",xlim=c(0,50))\nabline(v=mean(data4))\nsegments(mean(data4),1:length(data4),data4,1:length(data4))\n\n\n\n\n\n\n\nsd(data4)\n\n[1] 4.611709\n\n\n\n\n\n\n\n\nNumber of bins:\n\n10\n20\n35\n50\n\n\n\n\n\n\nBandwidth adjustment:"
  },
  {
    "objectID": "IntroStats/StandardDeviations.html#embedded-application",
    "href": "IntroStats/StandardDeviations.html#embedded-application",
    "title": "Standard Deviations",
    "section": "Embedded Application",
    "text": "Embedded Application\nIt’s also possible to embed an entire Shiny application within an R Markdown document using the shinyAppDir function. This example embeds a Shiny application located in another directory:\n\n\n\n\n\nNote the use of the height parameter to determine how much vertical space the embedded application should occupy.\nYou can also use the shinyApp function to define an application inline rather then in an external directory.\nIn all of R code chunks above the echo = FALSE attribute is used. This is to prevent the R code within the chunk from rendering in the document alongside the Shiny components."
  },
  {
    "objectID": "IntroStats/CentralLimitTheroem.html",
    "href": "IntroStats/CentralLimitTheroem.html",
    "title": "Central Limit Theorem",
    "section": "",
    "text": "Pick a distribution: * Uniform – platykurtic * Binomial – symmetric and mesokurtic * Exponential – highly skewed * Gamma (shape = 3) – skewed * T (df =3) – high kurtosis\nSlide the sample size up and down, notice how the empirical distribution function and histogram coverge to the normal distribution function and density.\nDistribution Type\n\nUniform\nBinomial\nExponential\nGamma\nT\n\n\n\n\n\n\nNumber of Samples:\n\n\n\n\n\nSize of each sample:\nThe left column shows the original distribution. (I call that the black hat in my CLT demo.)\nThe right column shows the distribution of means of size \\(M\\) (adjusted with the second slider). (This is the white hat distribuiton, and \\(M\\) is the number of cards averaged to get the white hat value.)\nThe top row shows histograms with a normal curve on top.\nThe bottom row shows a QQ-plot. This shows how much the sample is different from a normal distribution. A normal distribution should be right on top of the diagonal line.\nAs \\(M\\) (the number of cards averages to get to the white hat) gets bigger, the distribution should get closer and closer to the normal distribution."
  },
  {
    "objectID": "IntroStats/CentralLimitTheroem.html#take-home",
    "href": "IntroStats/CentralLimitTheroem.html#take-home",
    "title": "Central Limit Theorem",
    "section": "Take home",
    "text": "Take home\n\nEven if the underlying data aren’t normal, the distribuiton of the means of various groups should be close to normal.\nClose depends on the sample size.\nA bigger sample is needed if the data are highly skewed (expontential and gamma) or leptokurtic (exponential and Student t)."
  },
  {
    "objectID": "IntroStats/KurtosisPractice.html",
    "href": "IntroStats/KurtosisPractice.html",
    "title": "Kurtosis Practice",
    "section": "",
    "text": "In this exercise, the computer will generate 3 datasets: X, Y and Z. These will be randomly assigned to high (leptokurtic), medium (mesokurtic) and low (platykurtic) distributions. A normal curve is drawn over the top for reference.\nYou can redraw from the same distributions by changing the sample size.\n\n\n\n\n\n\nSample Size:\n\n50\n100\n500\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentify the kurtosis of each distribution.\n\n\n\n\n\n\nX\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\nY\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\nZ\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo try again with different distributions, reload the page. If you are having trouble, try increasing the sample size: sometimes a small sample won’t display the characteristics of the distribution strongly.\nHere are the other exercises in this series:\n\nSkewness Practice:\n\nHistograms\nBoxplots\nQ-Q Plots\n\nKurtosis Practice:\n\nHistograms\nBoxplots\nQ-Q Plots"
  },
  {
    "objectID": "IntroStats/KurtosisPractice.html#kurtosis-determination-exercise.",
    "href": "IntroStats/KurtosisPractice.html#kurtosis-determination-exercise.",
    "title": "Kurtosis Practice",
    "section": "",
    "text": "In this exercise, the computer will generate 3 datasets: X, Y and Z. These will be randomly assigned to high (leptokurtic), medium (mesokurtic) and low (platykurtic) distributions. A normal curve is drawn over the top for reference.\nYou can redraw from the same distributions by changing the sample size.\n\n\n\n\n\n\nSample Size:\n\n50\n100\n500\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentify the kurtosis of each distribution.\n\n\n\n\n\n\nX\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\nY\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\nZ\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo try again with different distributions, reload the page. If you are having trouble, try increasing the sample size: sometimes a small sample won’t display the characteristics of the distribution strongly.\nHere are the other exercises in this series:\n\nSkewness Practice:\n\nHistograms\nBoxplots\nQ-Q Plots\n\nKurtosis Practice:\n\nHistograms\nBoxplots\nQ-Q Plots"
  },
  {
    "objectID": "IntroStats/Independence.html",
    "href": "IntroStats/Independence.html",
    "title": "Independence",
    "section": "",
    "text": "Imagine a population which is split into two groups: \\(A\\) and \\(B\\). We select 100 people at random and ask them a question, which has two answers yes and no. Define the following quantities:\nDefine the following values (row and column totals):\nDividing any of those numbers by \\(N_{xx}\\) produces a corresponding proportion \\(P_{xx}\\) (which can be interpreted as a probability or proportion.\nSuppose group membership and the answer to the question are statistically indepedent. In the diagram below, adjust \\(P_{A+}\\) and \\(P_{+y}\\) to make a two-by-two table:\nP(Member of Group A)\n\n\n\n\n\nP(Answered `yes`)\nThere are two things you should notice about the independent data.\nWe could say that the row and column proportions are always the same.\nAnother way to think about this is to say: * If we learned which group a person belongs to, that would not change the probability of their answer. * If we learned how a person answered, that would not change the probablity of their group."
  },
  {
    "objectID": "IntroStats/Independence.html#dependent",
    "href": "IntroStats/Independence.html#dependent",
    "title": "Independence",
    "section": "Dependent",
    "text": "Dependent\nTo make the table dependence, we need to add another parameter to the model to specify the degree of dependence.\nFor a two-by-two table, the odds ratio is as fairly easy to understand choice: \\[ OR = \\frac{P_{Ay}/P_{An}}{P_{By}/P_{Bn}}\\] When group and answer are indpendent the cross product ratio should be 1.\nIf Group \\(A\\) is more likely to answer yes, then the ratio should be bigger than 1.\nIf Group \\(B\\) is more likely to answer yes, then the ratio should be less than one.\n\n\n\n\n\n\nP(Member of Group A)\n\n\n\n\n\nP(Answered `yes`)\n\n\n\n\n\nOdds Ratio\n\n1/4\n1/3\n1/2\n2/3\n1\n3/2\n2\n3\n4"
  },
  {
    "objectID": "IntroStats/CorrelationExercise.html",
    "href": "IntroStats/CorrelationExercise.html",
    "title": "Scatterplot examples",
    "section": "",
    "text": "This demonstration will use some random data. Lets start by generating the random data. So give a [random seed][seed] and pick a sample size for your sample.\n\n\n\n\n\n\nSample Size:\n\n25\n50\n100\n250\n500\n1000\n\n\n\n\n\n\nRandom number Seed (integer)"
  },
  {
    "objectID": "IntroStats/RareDisease.html",
    "href": "IntroStats/RareDisease.html",
    "title": "Rare Disease – COVID-19",
    "section": "",
    "text": "The rare disease problem is one of those “paradoxes” of statistics. The results are surprising because their are two sources of information: the prevalence of the disease in the population and the accuracy of the test. Often the former is stronger evidence than the latter, so people find it surprising.\nThe question of interest is: “What is the probability that a patient has the disease, given that the patient tests positive?” (This is sometimes called the True Positive probability; one minus would be called the False Positive probability.) A related question of interest is “What is the probability that a patient does not have the disease given that the patient tests negative?” (This is the False Negative probability.)\nStart by defining some variables. Let \\(D \\in \\{Y,N\\}\\) be whether or not a given individual has the disease, and let \\(T\\in\\{+,-\\}\\) be whether that individual gets a positive or negative result on the test. The joint probability of \\(D, T\\) is characterized by three numbers:\nLets try an example. A company called BioResponse just (March 19, 2020) launched the CoronaCheck test kit (Press Release). This article reports: “Our manufacturers report a sensitivity of 97.2% and specificity of 92%.”\nNow the hard part: estimating the base rate. This is hard because (a) people can have very mild symptoms for days and not know they have the disease, and (b) there has been a general shortage of test kits. So relying on official numbers is likely to give a big underestimate. As I’m not planning on updating this web site in real time, the numbers I’m putting in here will be out of date by the time you read this.\nAs I live in Florida, I’ll use the official Florida numbers: https://floridahealthcovid19.gov/#latest-stats . As of 2020-03-22 18:000, there were 1007 known cases in Florida, which has a population of 21,992,985. That gives a base rate of 4.5787327^{-5}. For the US, the number is 33,276 known cases, and a population of 330,464,151 for a base rate of 1.0069473^{-4}.\nUpdate: As of 2020-09-02, the state of Florida is reporting 624,116 cases, for a base rate of 0.028378. Note that the number of known cases is smaller than the total number of cases (especially, as we have learned that some people get very mild symptoms and may not know they are sick to seek testing).\n\\(Update^2\\): I have found a web site which gives background rates for SARS-COV-2 by state and county, so you can get local information. Hopefully, they are updating with the latest numbers. microCOVID Project.\nBase Rate (Pr(D=Y)):\n\n\n\n\n\nSensitivity (Pr(T=+|D=Y)):\n\n\n\n\n\nSpecificity (Pr(T=-|D=N)):"
  },
  {
    "objectID": "IntroStats/RareDisease.html#calculating-true-positive-and-false-positive.",
    "href": "IntroStats/RareDisease.html#calculating-true-positive-and-false-positive.",
    "title": "Rare Disease – COVID-19",
    "section": "Calculating true positive and false positive.",
    "text": "Calculating true positive and false positive.\nOne way to calculate this is to use Bayes’ theorem. However, from the table above, it is easy to calculate the true positive and false positive rates. We now just look at the columns of the table.\n\n\n\nFalse Positive Rate\nPr(D=N|T=+)\n\n\n\n\n\nFalse Negative Rate\nPr(D=Y|T=-)\n\n\n\n\nWhat is going on???\nThat false positive rate seems very high. What is really going on? The root cause is that as of this writing (Mar 22, 2020) COVID-19 is still pretty rare. So although getting a false positive is rare, actually having COVID-19 is much rarer. The following picture might help:\n\n\n\n\n\nThe thin bar on the left represents people who have COVID-19. There are still (fortunately) very few of them. The bar on top represents the false positives, fortunately, there are still a lot more of them than the true positives, so true positives are still rare. (May it always be so).\nOn the other hand, the false negative rate is very comforting. It means that if you test negative, you can be pretty sure that it is safe for you to be around other people (especially the old or sick)."
  },
  {
    "objectID": "IntroStats/RareDisease.html#sensitivity-analysis",
    "href": "IntroStats/RareDisease.html#sensitivity-analysis",
    "title": "Rare Disease – COVID-19",
    "section": "Sensitivity Analysis",
    "text": "Sensitivity Analysis\nDon’t forget that these base rates are underestimates. There is currently a shortage of tests, so these are only cases that have actually be able to be tested. Also, symptoms can take up to 3 days to appear, so some people who have it, probably don’t even know that they should ask to be tested. The actual infection rate could be 10 or more times as high as the known infection rate.\nAlso, there are various risk factors which should be added to the base rate. If the person being tested has traveled lately to an area with a higher rate, the base rate should go up. So too if the person has a fever or other symptoms of the virus.\nSo, play around with the base rate. Play with the sensitivity and specificity? How does this change? This will help you get a better feel for how the rare disease problem works.\nFinally, don’t forget that this thing grows exponentially fast (that is why it is a pandemic). This number could be go up very quickly. Here is an explanation.. ( As an aside, this is the kind of thing we would analyze on the log scale.)"
  },
  {
    "objectID": "IntroStats/RareDisease.html#how-would-this-test-be-used.",
    "href": "IntroStats/RareDisease.html#how-would-this-test-be-used.",
    "title": "Rare Disease – COVID-19",
    "section": "How would this test be used.",
    "text": "How would this test be used.\nActually, the most interesting thing about the CoronaCheck kit is that it only takes 15 minutes. This is great considering the older test takes 3 days. So assuming BioResponse can produce these quickly (or that other vendors come online with similar tests), these can be used for screening (say health care workers, or other first responders), as well as people presenting with other symptoms or having recently traveled.\nIf these people test positive on the quick screening test, they should be isolated and possibly a more sensitive (and probably time consuming) test be given. If they test negative, then they can be cleared to go about their normal activity. I’m sure this is how this test will be used.\nAnother factor is that doctors are simply not giving out tests unless there are other risk factors. I was in my doctors office for my daughter’s physical and talking to the nurse. She said that there was a woman who was tired (needs more sleep?) and congested (this is Tallahassee in March, the trees are raining pollen), but no fever. The nurse had to explain that she didn’t have enough test to give out unless there were more symptoms (particularly a fever). This will change as our testing capacity gets better (last I looked, Mar 20, the US was still doing only about 1/2 the number of test per capita as South Korea."
  },
  {
    "objectID": "IntroStats/RareDisease.html#dont-break-lockdownself-isolation",
    "href": "IntroStats/RareDisease.html#dont-break-lockdownself-isolation",
    "title": "Rare Disease – COVID-19",
    "section": "Don’t break lockdown/self-isolation",
    "text": "Don’t break lockdown/self-isolation\nDon’t panic, but do not be complacent either.\nSome of you reading this will be in official lockdown. Others will be under a self-distancing protocol. This is still extremely important as (1) the base rate will rise over time, probably quite quickly and (2) the disease takes up to 3 days to get started and the symptoms might appear like a common cold (Novel Coronavirus 19 is in fact an uncommon cold). You might have it and not know it yet. If you break the self-distancing protocol, you could be another Typhoid Mary spreading sickness and misery all around you.\nOh, and congrats to BioResponse on their breakthrough. I can’t judge the quality of the numbers from just a press release, but if they really can make that number of tests, that would be a big help. I hope lots of other biotech companies are working on this problem, too.\nStay healthy. Keep your distance. Wash your hands, and obey the local health authorities. Lets make sure we keep that base rate (i.e., the infection rate) low."
  },
  {
    "objectID": "IntroStats/LogNormalParams.html",
    "href": "IntroStats/LogNormalParams.html",
    "title": "Log-Normal Parameters",
    "section": "",
    "text": "A parameter is a value that can be changed in a statistical model. For example, the mean and standard deviation are the parameters of the normal distribution, which is a model for a population. Changing the value of a parameter, changes the model. We can see that in the illustration below. Try changing the values of the mean and standard deviation and see what happens to the shape of the curve."
  },
  {
    "objectID": "IntroStats/LogNormalParams.html#inputs-and-outputs",
    "href": "IntroStats/LogNormalParams.html#inputs-and-outputs",
    "title": "Log-Normal Parameters",
    "section": "Inputs and Outputs",
    "text": "Inputs and Outputs\n\n\n\n\n\n\nMean Log:\n\n\n\n\n\nStandard Deviation Log:"
  },
  {
    "objectID": "IntroStats/LogNormalParams.html#scale-and-location-parameters",
    "href": "IntroStats/LogNormalParams.html#scale-and-location-parameters",
    "title": "Log-Normal Parameters",
    "section": "Scale and Location Parameters",
    "text": "Scale and Location Parameters\nThe mean has a special role in the normal distribution; it determines where the center of the curve is. This makes it a location parameter.\nThe standard deviation has a special role in the normal distribution; it streches and shrinks the curve around the mean. This makes it a scale parameter.\nSometimes, the effects of scale and location parameters can be hard to see. This is because most statistical graphcis packages adjust the axis of the graph, so that the curve will always appear centered in the plotting window. In the normal curve above, I fixed the plotting window so that you can see the curve move. In the example below, I let the plotting window adjust with the curve. Notice how the curve stays the same, but the labels on the axis change.\n\n\n\n\n\n\nMean:\n\n\n\n\n\nStandard Deviation:"
  },
  {
    "objectID": "IntroStats/VaccineCI.html",
    "href": "IntroStats/VaccineCI.html",
    "title": "Confidence Interval: COVID Vaccine tests",
    "section": "",
    "text": "Data from Moderna Vaccine Study Control Group\nX &lt;- 95\nN &lt;- 1500\nIn a sample of 1500 volunteers receiving the placebo, there were 95 positive cases; so our estimate for the rate of COVID-19 in this population (and this time period) is 0.063.\nRecall that the formula for the \\(\\alpha\\) confidence interval is\nC.I. \\[ \\bar X \\pm (z_{1-\\alpha/2})\\ \\sigma_{\\bar X}\\] Here \\(z_{1-\\alpha/2}\\) is the \\(1-\\alpha/2\\) quantile of the normal distribution. We can look that up on a Normal Table. For \\(\\alpha=.95\\), \\(z_{1-\\alpha/2}=1.96\\approx 2\\).\nFor binomial distribution \\[ \\bar X = X/N=\\hat p\\] This is the value 0.063 we calculated earlier. The hat over the \\(p\\) is a sign that it is a (maximum likelihood) estimate.\nThe usual formula for the standard error of a mean (from a simple random sample) is \\[ \\sigma_{\\bar X} = \\frac{\\sigma}{\\sqrt{N}} \\] For the binomial distribution, the standard deviation is \\[ \\sigma = \\sqrt{p(1-p)}; \\qquad s = \\sqrt{\\hat p(1-\\hat p)}\\] Plug that into the formula for the standard error and we get:\n\\[ \\sigma_{\\bar X} = \\sqrt{p(1-p)/N} \\] Lets go ahead and calculate those\np.hat &lt;- X/N\nse &lt;- sqrt(p.hat*(1-p.hat)/N)\nThe probability estimate is 0.063 and the standard error is 0.0063.\nI’ll now use an R trick. qnorm() is the R function to calculate the quantiles of the normal distribution. If I give it two probabilities, it will give me both the postive and negative values. So I will pass it \\((\\alpha/2,1-\\alpha/2)\\), this gives the values \\(r round(qnorm(c(.025,.975)),3)\\).\nBecause R does calculations on vectors, it will calculate both sides of the confidence interval with one formula.\nci &lt;- p.hat + qnorm(c(.025,.975))*se\nPrevlance of covid at the time and in the locations the study was run was between (5.1%,7.6%).\nNote that a lot of things have changed between now and then. In particular, the rise of the much more transmissable delta variant. But also changes in how seriously people take masking and other percautions. In particular, there is probably considerable regional variation in the prevalence of COVID-19.\nThe web site https://www.microcovid.org/ tracks this on a county-by-county basis."
  },
  {
    "objectID": "IntroStats/VaccineCI.html#severe-covid",
    "href": "IntroStats/VaccineCI.html#severe-covid",
    "title": "Confidence Interval: COVID Vaccine tests",
    "section": "Severe Covid",
    "text": "Severe Covid\nSame thing with the severe (hospitalizations or death) COVID numbers.\n\nX1 &lt;- 11\np1 &lt;- X1/N\nse1 &lt;- sqrt(p1*(1-p1)/N)\nci1 &lt;- p1 + qnorm(c(.025,.975))*se1\n\nPrevlance of severe covid at the time and in the locations the study was run was between (0.3%,1.2%)."
  },
  {
    "objectID": "IntroStats/RegressionPredictionA.html",
    "href": "IntroStats/RegressionPredictionA.html",
    "title": "Regression Prediction Error",
    "section": "",
    "text": "We will work with an example data set from Ezekiel (1930) which provides information about the speed of a number of cars and the stopping distance in feet.\n\nhelp(cars)\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00  \n\n\nLets take a quick look at these data.\n\nplot(dist~speed,data=cars,xlab=\"Speed (mph)\", ylab=\"Stopping Distance (ft)\")\nabline(lm(dist~speed,data=cars))\nlines(lowess(cars),col=2,lty=2)\n\n\n\n\n\n\n\n\nThe solid black line is the least squares regression line, or basic model.\nThe dashed red line is a lowess curve fit to the same date.\n\nThere may be a little bit of a curve here, but it is hard to see.\n\nWe will go ahead and fit a regression using least squares. (This the the lm or linear model function in R.)\n\ncars.fit &lt;- lm (dist~speed,data=cars)\nprint(cars.fit)\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nCoefficients:\n(Intercept)        speed  \n    -17.579        3.932  \n\n\nThe method of least squares or maximum likelihood (which in the case of simple regression are the same) finds the single best fitting line.\n\nLeast Squares means the line has the smallest some of squared residuals.\nMaximum Likelihood means that these are the parameters (slope and intercept) that have the highest probability of generating the target data."
  },
  {
    "objectID": "IntroStats/RegressionPredictionA.html#the-cars-data-set",
    "href": "IntroStats/RegressionPredictionA.html#the-cars-data-set",
    "title": "Regression Prediction Error",
    "section": "",
    "text": "We will work with an example data set from Ezekiel (1930) which provides information about the speed of a number of cars and the stopping distance in feet.\n\nhelp(cars)\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00  \n\n\nLets take a quick look at these data.\n\nplot(dist~speed,data=cars,xlab=\"Speed (mph)\", ylab=\"Stopping Distance (ft)\")\nabline(lm(dist~speed,data=cars))\nlines(lowess(cars),col=2,lty=2)\n\n\n\n\n\n\n\n\nThe solid black line is the least squares regression line, or basic model.\nThe dashed red line is a lowess curve fit to the same date.\n\nThere may be a little bit of a curve here, but it is hard to see.\n\nWe will go ahead and fit a regression using least squares. (This the the lm or linear model function in R.)\n\ncars.fit &lt;- lm (dist~speed,data=cars)\nprint(cars.fit)\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nCoefficients:\n(Intercept)        speed  \n    -17.579        3.932  \n\n\nThe method of least squares or maximum likelihood (which in the case of simple regression are the same) finds the single best fitting line.\n\nLeast Squares means the line has the smallest some of squared residuals.\nMaximum Likelihood means that these are the parameters (slope and intercept) that have the highest probability of generating the target data."
  },
  {
    "objectID": "IntroStats/RegressionPredictionA.html#lots-of-different-regression-lines",
    "href": "IntroStats/RegressionPredictionA.html#lots-of-different-regression-lines",
    "title": "Regression Prediction Error",
    "section": "Lots of different regression lines",
    "text": "Lots of different regression lines\nI’ll try the regression using a different method (Markov Chain Monte Carlo, or MCMC). In this method we sample 4000 different plausible sets of parameters that could have given rise to the data. (These are sampled with a probability proportional to how likely they are to have generated the observed data).\nThe printed summary shows the median of the 4000 samples. It should be close to, but not exactly the same as, the least squares/maximum likelihood estimate.\n\n\n\n\n\n\nNote\n\n\n\nI can’t run stan on the server, so I’ll run it first on my local file and save the result.\n\nlibrary(rstanarm)  \n\nLoading required package: Rcpp\n\n\nThis is rstanarm version 2.32.2\n\n\n- See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!\n\n\n- Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.\n\n\n- For execution on a local, multicore CPU with excess RAM we recommend calling\n\n\n  options(mc.cores = parallel::detectCores())\n\ncars.mcmc &lt;- stan_glm(dist~speed,data=cars,refresh=0)\nsaveRDS(cars.mcmc,\"cars_mcmc.RDS\")\n\nThe next bit of code will read the results from a file instead of running stan.\n\n\n\ncars.mcmc &lt;- readRDS(\"cars_mcmc.RDS\")\ncars.coef &lt;- as.matrix(cars.mcmc$stanfit)\nprint(cars.mcmc)\n\nstan_glm\n family:       gaussian [identity]\n formula:      dist ~ speed\n observations: 50\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) -17.6    6.8 \nspeed         3.9    0.4 \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 15.5    1.5  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg"
  },
  {
    "objectID": "IntroStats/RegressionPredictionA.html#mean-confidence-interval",
    "href": "IntroStats/RegressionPredictionA.html#mean-confidence-interval",
    "title": "Regression Prediction Error",
    "section": "Mean Confidence Interval",
    "text": "Mean Confidence Interval\nThe MCMC approach is useful because it helps us remember that the estimates that are produced by the [least squares] regression are not the truth, but rather just the most likely set of parameters. There are other possibilities that are nearly as likely.\nThe next graph is designed to show this.\nThe first N (you can adjust using the slider) samples from the MCMC are plotted as gray lines.\nThe least squares line is plotted in black.\n\n\n\n\n\n\nConfidence Level:\n\n50\n68\n90\n95\n99\n\n\n\n\n\n\nNumber of plausible values to plot\n\n\n\n\n\n\n\n\n\n\nNote the dashed curves surrounding the regression line.\nThese are the confidence interval for the regression line.\nThe level of the confidence interval is how many of these plausible regression lines should fit between the dashed curves (expressed as a percentage).\nSPSS calls this the “mean” prediction interval. R calls it the “confidence” interval.\nYou can use the graph (or the R predict function, or the prediction option in SPSS) to get a prediction for the average (over a number of trials) stopping time at a given speed."
  },
  {
    "objectID": "IntroStats/RegressionPredictionA.html#individual-prediction-interval",
    "href": "IntroStats/RegressionPredictionA.html#individual-prediction-interval",
    "title": "Regression Prediction Error",
    "section": "Individual Prediction Interval",
    "text": "Individual Prediction Interval\nThe mean confidence interval above is for the average over many attempts at stopping the car.\nWe don’t expect a single attempt to fall exactly on the line.\n\n68% of the time we expect to be one standard error above or below the line.\n95% of the time we expect to be two standard errors above or below the line.\nTo get the total error, we need to add\n\nThe error in the regression line (see above)\nThe error around the regression line.\n\n\n(Actually, we add these on the squared variance scale).\nThe picture below shows the individual prediction interval. Once again, you can pick your confidence level.\nSPSS calls this the individual prediction interval. R calls it the prediction interval.\n\n\n\n\n\n\nConfidence Level:\n\n50\n68\n90\n95\n99\n\n\n\n\n\n\n\n\n\n\n\nLook at the area in the graph which is colored cyan.\nThese are predictions that the car will stop in negative distance. Impossible!\nThe model is wrong.\nThat shouldn’t worry us, models are always wrong. The just might be close enough to be right to be useful.\nWe might say that the linear model is useful, but only if the car is going 5 mph or more."
  },
  {
    "objectID": "IntroStats/RegressionPredictionA.html#model-checking",
    "href": "IntroStats/RegressionPredictionA.html#model-checking",
    "title": "Regression Prediction Error",
    "section": "Model Checking",
    "text": "Model Checking\nNote that there was a slight curve in the lowess line in the scatterplot at the top of this analysis.\nSometimes the curve is easier to see if we take the linear trend out.\nWe can do this by plotting the residuals versus the fitted values.\n In a simple regression, this is the same as plotting against \\(X\\), as the fitted values are just a linear transformation of \\(X\\) (and the graph will just be rescaled to fit). For multiple regression, the fitted values are a mix of all the \\(X\\) values, so this plot is a useful summary.\n\n\n\n\n\n\n\n\n\nLooking a little more closely, we can see the curve.\nIt would be easy to miss without the lowess line, but the lowess line points it out to us.\nThere is a little bit of curvature, curving up at the lower distances, keeping the stopping distances in positive territory.\nSo what to conclude?\n\nIn the range of 5 mph – 25 mph the linear model looks pretty good.\nFor low speeds, we need a better model.\nMaybe we need a better model for higher speeds as well."
  },
  {
    "objectID": "IntroStats/RegressionPredictionA.html#try-a-transformation",
    "href": "IntroStats/RegressionPredictionA.html#try-a-transformation",
    "title": "Regression Prediction Error",
    "section": "Try a transformation",
    "text": "Try a transformation\n\ncars.sqfit &lt;- lm (sqrt(dist)~speed,data=cars)\nsummary(cars.sqfit)\n\n\nCall:\nlm(formula = sqrt(dist) ~ speed, data = cars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0684 -0.6983 -0.1799  0.5909  3.1534 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.27705    0.48444   2.636   0.0113 *  \nspeed        0.32241    0.02978  10.825 1.77e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.102 on 48 degrees of freedom\nMultiple R-squared:  0.7094,    Adjusted R-squared:  0.7034 \nF-statistic: 117.2 on 1 and 48 DF,  p-value: 1.773e-14\n\nplot(cars.sqfit,ask=FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\nggplot(cars,aes(x=speed,y=dist)) + geom_point() + geom_smooth(method=\"lm\") +\n  scale_y_sqrt()\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "IntroStats/SkewnessBoxplot.html",
    "href": "IntroStats/SkewnessBoxplot.html",
    "title": "Skewness Boxplot Practice",
    "section": "",
    "text": "In this exercise, the computer will generate 3 datasets: A, B and C. These will be randomly assigned to a positively skewed, negatively skewed, and symmetric distribution type. Your job is to determine which is which.\nYou can redraw from the same distributions by changing the sample size.\n\nlibrary(shiny)\nui &lt;- fluidPage(\ninputPanel(\n  selectInput(\"nn\", label = \"Sample Size:\",\n              choices = c(50, 100, 500, 1000), selected = 100)),\nmainPanel(\n  plotOutput(\"boxplots\")),\n  h4(\"Which is which?\"),\n  p(\"Identify the skewness of each distribution.\"),\n  do.call(inputPanel,\n         lapply(names(key), function (k)\n                selectInput(k, label=k,\n                  choices=c(Unknown=\"unknown\", \n                           longnames),\n                  selected=\"unknown\"))),\n  h4(\"Answers:\\n\"),\n  tableOutput(\"answers\"))\n\nserver &lt;- function (input,output) {\n  output$boxplots &lt;- renderPlot({\n    ## Draw random data \n    kdat &lt;- lapply(names(key), function (k) {\n    x &lt;-do.call(distlist[[key[k]]][[kdist[k]]],\n                list(input$nn))\n      scale(x,(min(x)+max(x))/2,(max(x)-min(x)))*100+50\n  })\n  names(kdat) &lt;- names(key)\n  kdat &lt;- as.data.frame(kdat)\n  \n  boxplot(kdat,xlab=\"X\")\n\n})\n  output$answers &lt;- renderTable({\n  answer &lt;- sapply(names(key),\n   function (k) {\n      if (input[[k]]==\"unknown\") {\n        \"Make your selection.\\n\"\n      } else {\n        paste(ifelse(input[[k]]==key[k],\n                     \"Correct:\", \"Incorrect:\"),\n              \"Distribution was\",kdist[k],\n               \"(\",\n        names(longnames)[grep(key[k],longnames)],\n               \")\\n\")\n    }})\n  names(answer) &lt;- names(key)\n  as.data.frame(answer)\n}, colnames=FALSE,rownames=TRUE)\n}\nshinyApp(ui=ui,server=server)\n\n\n\n\nTo try again with different distributions, reload the page. If you are having trouble, try increasing the sample size: sometimes a small sample won’t display the characteristics of the distribution strongly."
  },
  {
    "objectID": "IntroStats/SkewnessBoxplot.html#skewness-determination-exercise.",
    "href": "IntroStats/SkewnessBoxplot.html#skewness-determination-exercise.",
    "title": "Skewness Boxplot Practice",
    "section": "",
    "text": "In this exercise, the computer will generate 3 datasets: A, B and C. These will be randomly assigned to a positively skewed, negatively skewed, and symmetric distribution type. Your job is to determine which is which.\nYou can redraw from the same distributions by changing the sample size.\n\nlibrary(shiny)\nui &lt;- fluidPage(\ninputPanel(\n  selectInput(\"nn\", label = \"Sample Size:\",\n              choices = c(50, 100, 500, 1000), selected = 100)),\nmainPanel(\n  plotOutput(\"boxplots\")),\n  h4(\"Which is which?\"),\n  p(\"Identify the skewness of each distribution.\"),\n  do.call(inputPanel,\n         lapply(names(key), function (k)\n                selectInput(k, label=k,\n                  choices=c(Unknown=\"unknown\", \n                           longnames),\n                  selected=\"unknown\"))),\n  h4(\"Answers:\\n\"),\n  tableOutput(\"answers\"))\n\nserver &lt;- function (input,output) {\n  output$boxplots &lt;- renderPlot({\n    ## Draw random data \n    kdat &lt;- lapply(names(key), function (k) {\n    x &lt;-do.call(distlist[[key[k]]][[kdist[k]]],\n                list(input$nn))\n      scale(x,(min(x)+max(x))/2,(max(x)-min(x)))*100+50\n  })\n  names(kdat) &lt;- names(key)\n  kdat &lt;- as.data.frame(kdat)\n  \n  boxplot(kdat,xlab=\"X\")\n\n})\n  output$answers &lt;- renderTable({\n  answer &lt;- sapply(names(key),\n   function (k) {\n      if (input[[k]]==\"unknown\") {\n        \"Make your selection.\\n\"\n      } else {\n        paste(ifelse(input[[k]]==key[k],\n                     \"Correct:\", \"Incorrect:\"),\n              \"Distribution was\",kdist[k],\n               \"(\",\n        names(longnames)[grep(key[k],longnames)],\n               \")\\n\")\n    }})\n  names(answer) &lt;- names(key)\n  as.data.frame(answer)\n}, colnames=FALSE,rownames=TRUE)\n}\nshinyApp(ui=ui,server=server)\n\n\n\n\nTo try again with different distributions, reload the page. If you are having trouble, try increasing the sample size: sometimes a small sample won’t display the characteristics of the distribution strongly."
  },
  {
    "objectID": "IntroStats/SkewnessBoxplot.html#what-to-look-for",
    "href": "IntroStats/SkewnessBoxplot.html#what-to-look-for",
    "title": "Skewness Boxplot Practice",
    "section": "What to look for:",
    "text": "What to look for:\n\nIs the box from median to quartile longer on one side than the other?\nIs the whisker longer on one side than the other?\nAre there outliers on one side and not the other?\n\nAll three of these are signs of skewness in that direction (longer box, whisker, or outliers)."
  },
  {
    "objectID": "IntroStats/SkewnessBoxplot.html#related-pages",
    "href": "IntroStats/SkewnessBoxplot.html#related-pages",
    "title": "Skewness Boxplot Practice",
    "section": "Related Pages",
    "text": "Related Pages\nHere are the other exercises in this series:\n\nSkewness Practice:\n\nHistograms\nBoxplots\nQ-Q Plots\n\nKurtosis Practice:\n\nHistograms\nBoxplots\nQ-Q Plots"
  },
  {
    "objectID": "IntroStats/LawOfLargeNumbersAnimated.html",
    "href": "IntroStats/LawOfLargeNumbersAnimated.html",
    "title": "Law of Large Numbers",
    "section": "",
    "text": "This is pretty close to the frequency definition of probability. Suppose the probability of some event is \\(p\\). Suppose further than we sample \\(N\\) times from the process that generates this event. Let \\(p_N\\) be the proportion of times the event occurs in \\(N\\) trials. As \\(N\\) gets bigger and bigger, \\(p_N\\) gets closer and closer to \\(p\\).\n\n(Skip this unless you are good with calculus.)_ This is one of those epsilon-delta theorems. So let \\(\\delta\\) be a difference from \\(p\\) and let \\(\\epsilon\\) be a small probability. For any \\(\\epsilon\\) and \\(\\delta\\), there exists an \\(N\\) such that \\(P(|p_N-p|&gt;\\delta) &lt; \\epsilon\\).\n\n\n\nIn the picture below, pick a probability \\(p\\) and a sample size \\(N\\). The computer will generate samples up to \\(N\\) and plot \\(p_N\\).\nThe \\(\\delta\\)-line is an error bound plus or minus \\(\\delta\\) units from the target \\(p\\). This is a target so you can judge how close you got.\n\n\n\n\n\n\nMaximum Sample Size:\n\n50\n100\n200\n500\n1000\n\n\n\n\n\n\nProbability of event (p)\n\n\n\n\n\nDistance of reference line from target (delta)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can use the Law of Large Numbers to prove an important theorem. As the sample size gets larger and larger, the sample looks more and more like the population it is drawn from.\n\nTechnically, the Law of Large Numbers refers to the result above. But we can use it so show a very important basis of statistics. Suppose we have some kind of distribution, \\(F(x)\\), that generates numbers, \\(X\\). Recall that the definition of \\(F(x)=\\Pr(X \\leq x)\\).\nDraw a sample of size \\(N\\) from this distribution. Now consider the sampled data points \\(X_1,\\ldots,X_N\\), and consider sampling a new value \\(Y\\) from that distribution. Let \\(F_N(y) = \\Pr(Y \\leq y)\\). This is sometimes called the bootstrap distribution.\nBy the law of large numbers, for every \\(y\\), as \\(N\\) gets large \\(F_N(y) \\rightarrow F(y)\\). So the sample distribution \\(F_N()\\) converges to the \\(F()\\).\n\n\n\n\nPick a distribution: * Normal – standard normal * Exponential – highly skewed * Gamma (shape = 3) – skewed * T (df =3) – high kurtosis\nSlide the sample size up and down, notice how the empirical distribution function and histogram coverge to the theoretical distribution function and density.\n\n\n\n\n\n\nDistribution Type\n\nNormal\nExponential\nGamma\nT\n\n\n\n\n\n\nMaximum Sample Size:\n\n50\n100\n200\n500\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSee also the non-animated version."
  },
  {
    "objectID": "IntroStats/LawOfLargeNumbersAnimated.html#a-demonstration.",
    "href": "IntroStats/LawOfLargeNumbersAnimated.html#a-demonstration.",
    "title": "Law of Large Numbers",
    "section": "",
    "text": "In the picture below, pick a probability \\(p\\) and a sample size \\(N\\). The computer will generate samples up to \\(N\\) and plot \\(p_N\\).\nThe \\(\\delta\\)-line is an error bound plus or minus \\(\\delta\\) units from the target \\(p\\). This is a target so you can judge how close you got.\n\n\n\n\n\n\nMaximum Sample Size:\n\n50\n100\n200\n500\n1000\n\n\n\n\n\n\nProbability of event (p)\n\n\n\n\n\nDistance of reference line from target (delta)"
  },
  {
    "objectID": "IntroStats/LawOfLargeNumbersAnimated.html#convergence-of-distributions-boot-strap-distribution",
    "href": "IntroStats/LawOfLargeNumbersAnimated.html#convergence-of-distributions-boot-strap-distribution",
    "title": "Law of Large Numbers",
    "section": "",
    "text": "We can use the Law of Large Numbers to prove an important theorem. As the sample size gets larger and larger, the sample looks more and more like the population it is drawn from.\n\nTechnically, the Law of Large Numbers refers to the result above. But we can use it so show a very important basis of statistics. Suppose we have some kind of distribution, \\(F(x)\\), that generates numbers, \\(X\\). Recall that the definition of \\(F(x)=\\Pr(X \\leq x)\\).\nDraw a sample of size \\(N\\) from this distribution. Now consider the sampled data points \\(X_1,\\ldots,X_N\\), and consider sampling a new value \\(Y\\) from that distribution. Let \\(F_N(y) = \\Pr(Y \\leq y)\\). This is sometimes called the bootstrap distribution.\nBy the law of large numbers, for every \\(y\\), as \\(N\\) gets large \\(F_N(y) \\rightarrow F(y)\\). So the sample distribution \\(F_N()\\) converges to the \\(F()\\)."
  },
  {
    "objectID": "IntroStats/LawOfLargeNumbersAnimated.html#demonstration-of-convergence-of-distributions.",
    "href": "IntroStats/LawOfLargeNumbersAnimated.html#demonstration-of-convergence-of-distributions.",
    "title": "Law of Large Numbers",
    "section": "",
    "text": "Pick a distribution: * Normal – standard normal * Exponential – highly skewed * Gamma (shape = 3) – skewed * T (df =3) – high kurtosis\nSlide the sample size up and down, notice how the empirical distribution function and histogram coverge to the theoretical distribution function and density.\n\n\n\n\n\n\nDistribution Type\n\nNormal\nExponential\nGamma\nT\n\n\n\n\n\n\nMaximum Sample Size:\n\n50\n100\n200\n500\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSee also the non-animated version."
  },
  {
    "objectID": "IntroStats/NormalCalculator.html",
    "href": "IntroStats/NormalCalculator.html",
    "title": "Normal Calculator",
    "section": "",
    "text": "In this tool, you input a z score, and get a corresponding normal probability.\n\n\n\n\n\n\nWhich tails\n\nUpper tail: Pr(z &lt; Z)\nLower tail: Pr(Z &lt; z)\nBoth tails: Pr(Z &lt;-z or z&lt; Z)\nMiddle: Pr(-z &lt; Z &lt; z)\n\n\n\n\n\n\nNormal quantile (z):"
  },
  {
    "objectID": "IntroStats/NormalCalculator.html#normal-probabilities.",
    "href": "IntroStats/NormalCalculator.html#normal-probabilities.",
    "title": "Normal Calculator",
    "section": "",
    "text": "In this tool, you input a z score, and get a corresponding normal probability.\n\n\n\n\n\n\nWhich tails\n\nUpper tail: Pr(z &lt; Z)\nLower tail: Pr(Z &lt; z)\nBoth tails: Pr(Z &lt;-z or z&lt; Z)\nMiddle: Pr(-z &lt; Z &lt; z)\n\n\n\n\n\n\nNormal quantile (z):"
  },
  {
    "objectID": "IntroStats/NormalCalculator.html#normal-quantiles.",
    "href": "IntroStats/NormalCalculator.html#normal-quantiles.",
    "title": "Normal Calculator",
    "section": "Normal Quantiles.",
    "text": "Normal Quantiles.\nIn this tool, you input a probability, and get a corresponding z score.\n\n\n\n\n\n\nWhich tails\n\nUpper tail: Pr(z &lt; Z)\nLower tail: Pr(Z &lt; z)\nBoth tails: Pr(Z &lt;-z or z&lt; Z)\nMiddle: Pr(-z &lt; Z &lt; z)\n\n\n\n\n\n\nProbability of shaded region:"
  },
  {
    "objectID": "IntroStats/Correlation.html",
    "href": "IntroStats/Correlation.html",
    "title": "Correlation Coefficient",
    "section": "",
    "text": "This demonstration will use some random data. Lets start by generating the random data. So give a random seed and pick a sample size for your sample.\nSample Size:\n\n25\n50\n100\n250\n500\n1000\n\n\n\n\n\n\nRandom number Seed (integer)"
  },
  {
    "objectID": "IntroStats/Correlation.html#regression.",
    "href": "IntroStats/Correlation.html#regression.",
    "title": "Correlation Coefficient",
    "section": "Regression.",
    "text": "Regression.\nGalton’s discovery was that if you want to predict \\(Y\\) from \\(X\\), then you want to regress that prediction towards the mean. If \\(X\\) and \\(Y\\) were perfectly correlated, then the \\(z\\)-score for a variable on the \\(X\\) scale would be the same for the variable on the \\(Y\\) scale, so all we need to do is change units. The ratio \\(\\sigma_Y/\\sigma_X\\) changes units from \\(X\\) to \\(Y\\). We also want the mean of \\(X\\) to map to the mean of \\(Y\\). The equation for this change-of-units line, the SD-line, is: \\[ \\widetilde y = \\frac{\\sigma_Y}{\\sigma_X} x + \\left ( \\mu_Y - \\frac{\\sigma_Y}{\\sigma_X} \\mu_X\\right ) .\\] The first term is the change of units, the second term makes sure the line goes through the mean of \\(X\\) and the mean of \\(Y\\).\nThe ideal discount is the correlation coefficient \\(\\rho_{XY}\\). This gives the the following final regression line: \\[\\widehat y = \\rho_{XY}\\frac{\\sigma_Y}{\\sigma_X} x + \\left ( \\mu_Y - \\rho_{XY}\\frac{\\sigma_Y}{\\sigma_X} \\mu_X\\right ) .\\] Because second term has \\(\\rho_{XY}\\) in it as well, this will make the predicted value closer to the mean of \\(Y\\), \\(\\mu_Y\\).\n The notations \\(\\widetilde{y}\\) and \\(\\widehat{y}\\) indicate predicted values for \\(y\\). The y hat (\\(\\widehat y\\)) notation is reserved for what is called maximum likelihood predictions. In the case of a regression with normally distributed errors, the maximum likelihood predictor is also the least squares estimator. Usually, the estimators use the sample values, thus in the regression equation: \\[ \\widehat{b_1} = r_{XY}\\frac{s_X}{s_Y}\\;;\\qquad \\widehat{b_0}=\\bar Y -r_{XY}\\frac{s_X}{s_Y} \\bar X\\; .\\]"
  },
  {
    "objectID": "IntroStats/Correlation.html#lets-try-it.",
    "href": "IntroStats/Correlation.html#lets-try-it.",
    "title": "Correlation Coefficient",
    "section": "Lets Try it.",
    "text": "Lets Try it.\nIn the graph below, you can set the mean and standard deviation of both X and Y as well as the correlation coefficient. The SD line is a dashed blue, and the regression line is a solid red. Play around for a bit.\n\n\n\n\n\n\nMean of X:\n\n\n\n\n\nStandard Deviation of X:\n\n\n\n\n\nMean of Y:\n\n\n\n\n\nStandard Deviation of Y:\n\n\n\n\n\nCorrelation between X and Y:\n\n\n\n\n\n\n\n\n\n\nNotice how changing the means and standard deviations doesn’t change much except the numbers in the equations and the labels on the axis. This is because R is automatically adjusting the scale of the graphs to fit the data. In this view, the SD line, the one that is a change of scale, is a perfect 45 degress, no matter what.\nActually, if you set the correlation coefficient to be the same in the two plots, they should look the same. The two plots show the same data, its just in the lower plot, the data are transformed to fit the statistis. Correlation is a property that is independent from Mean and SD."
  },
  {
    "objectID": "IntroStats/Correlation.html#for-the-more-mathematically-inclined.",
    "href": "IntroStats/Correlation.html#for-the-more-mathematically-inclined.",
    "title": "Correlation Coefficient",
    "section": "For the more mathematically inclined.",
    "text": "For the more mathematically inclined.\nThis is how I generated the data with the given correlations.\nFor the first plot, I first generated two vectors of standard normal numbers, \\({\\bf X}\\) and \\({\\bf e}\\), that is with mean 0 and standard deviation 1. Then I defined \\({\\bf Y}\\) with the following equation: \\[ Y_i = \\rho_{XY}X_i + \\sqrt{(1-\\rho_{XY}^2)}\\ e_i\\] As \\(\\rho_{XY}^2 + (1-\\rho_{XY}^2) =1\\), \\({\\bf Y}\\) also has mean 0 and standard deviation 1.\nFor the second plot, I used the following expressions: \\[ XX_i = \\mu_X + \\sigma_X X_i\\;; \\qquad YY_i = \\mu_X + \\sigma_Y Y_i\\;.\\] The transformation is exactly the same."
  },
  {
    "objectID": "IntroStats/Chi2calculator.html",
    "href": "IntroStats/Chi2calculator.html",
    "title": "Chi-squared Calculator",
    "section": "",
    "text": "In this tool, you input a \\(\\chi^2\\) score and the degrees of freedom, and get a corresponding \\(p\\)-value.\n\n\n\n\n\n\nWhich tails\n\nUpper tail: Pr(x^2 &lt; X^2)\nLower tail: Pr(X^2 &lt; x^2)\n\n\n\n\n\n\nchi-squared value:\n\n\n\n\n\nDegrees of Freedom"
  },
  {
    "objectID": "IntroStats/Chi2calculator.html#chi-squared-probabilities.",
    "href": "IntroStats/Chi2calculator.html#chi-squared-probabilities.",
    "title": "Chi-squared Calculator",
    "section": "",
    "text": "In this tool, you input a \\(\\chi^2\\) score and the degrees of freedom, and get a corresponding \\(p\\)-value.\n\n\n\n\n\n\nWhich tails\n\nUpper tail: Pr(x^2 &lt; X^2)\nLower tail: Pr(X^2 &lt; x^2)\n\n\n\n\n\n\nchi-squared value:\n\n\n\n\n\nDegrees of Freedom"
  },
  {
    "objectID": "IntroStats/Chi2calculator.html#chi-square-quantiles-critical-values.",
    "href": "IntroStats/Chi2calculator.html#chi-square-quantiles-critical-values.",
    "title": "Chi-squared Calculator",
    "section": "Chi-square Quantiles (Critical values).",
    "text": "Chi-square Quantiles (Critical values).\nIn this tool, you input a probability and degrees of freedom, and get a corresponding \\(X^2\\) score.\n\n\n\n\n\n\nWhich tails\n\nUpper tail: Pr(x^2 &lt; X^2)\nLower tail: Pr(X^2 &lt; x^2)\n\n\n\n\n\n\nProbability of shaded region:\n\n\n\n\n\nDegrees of Freedom\n\n\n\n\n\n\n\n\n\n\nFor \\(\\chi^2\\) tests, one almost always looks at upper tail, and rarely look at the lower tail (does the data fit better than expected). Two-tailed tests are never (“What never?”, “No Never!”, “What Never?”, “Well hardly ever!”, Gilbert & Sulivan, HMS Pinfore) done."
  },
  {
    "objectID": "IntroStats/CovidVaccines.html",
    "href": "IntroStats/CovidVaccines.html",
    "title": "Covid-19 Vaccines",
    "section": "",
    "text": "Good News! In November, both Pfizer and Moderna announce Phase 3 Vaccine Trials with promising results.\nWith over 30,000 participants in each study they reported the following data.\ncovidVaccines &lt;- tibble(\n  Treatment=c(\"Placebo\",\"Vaccine\"),\n  Pfizer=c(90,5),ModernaAll=c(95,5),\n  ModernaSevere=c(11,0),\n  N=c(15000,15000))\ncovidVaccines.N &lt;- 15000\nkable(covidVaccines)\n\n\n\n\nTreatment\nPfizer\nModernaAll\nModernaSevere\nN\n\n\n\n\nPlacebo\n90\n95\n11\n15000\n\n\nVaccine\n5\n5\n0\n15000"
  },
  {
    "objectID": "IntroStats/CovidVaccines.html#measures-of-effectiveness",
    "href": "IntroStats/CovidVaccines.html#measures-of-effectiveness",
    "title": "Covid-19 Vaccines",
    "section": "Measures of Effectiveness",
    "text": "Measures of Effectiveness\nWe start with a cross-tab\n\n\n\nTreatment\nSick\nHealthy\nTotal\n\n\n\n\nPlacebo\nSP\nHP\nNP\n\n\nVaccine\nSV\nHV\nNV\n\n\nTotal\nNS\nNH\nN\n\n\n\nOdds of getting sick\nPlacebo: $ SP/HP $ Vaccine: $ SV/HV $\n\nCross Product (Odds) Ratio\n\\[ OR = \\frac{SP/HP}{SV/HV} \\] How much does your odds of getting sick increase if you get the placebo instead of the vaccine.\n\n\nRisk Ratio\n\\[ RR = \\frac{SP/NP}{SV/NV} \\] How much does your probiliby of getting sick increase if you get the placebo instead of the vaccine.\n\n\nVaccine Effectiveness\n\\[ VE = 100 (1 - \\frac{1}{RR}) \\]\n\n\nChi-square test\nNull hypothesis is that getting the disease is independent of the vaccine. In other words, \\(OR=RR=1\\).\n\\[ SV/NV = SP/NP \\]\nLarge chi-squared value incidates that cross product rate is not 1 (but doesn’t tell if placebo or vaccine is better!\n\n\nZ-score test\nAnother way to work with these data is to calculate probabilities of infection for each group and the standard errors. Then can use the \\(z\\)-test to compare.\n\\[p_V = p(S|V) = SV/NV \\qquad SE(p_V) = \\sqrt{p_V(1-p_V)/NV} \\] \\[p_P = p(S|P) = SP/NP \\qquad SE(p_P) = \\sqrt{p_P(1-p_P)/NP} \\] \\[ z = \\frac{p_P-p_V}{\\sqrt{SE(p_V)^2 + SE(p_P)^2}}\\]"
  },
  {
    "objectID": "IntroStats/CovidVaccines.html#pfizer-vaccine",
    "href": "IntroStats/CovidVaccines.html#pfizer-vaccine",
    "title": "Covid-19 Vaccines",
    "section": "Pfizer Vaccine",
    "text": "Pfizer Vaccine\nThere were around 30,000 volunteers in the Phase 3 trials; 15,000 in each arm.\n\ncovidVaccines %&gt;% \n  mutate(p.Pfizer=Pfizer/N) %&gt;%\n  mutate(s.Pfizer=sqrt(p.Pfizer*(1-p.Pfizer)/N)) -&gt;\n  covidVaccines\n\nselect(covidVaccines,Treatment,contains(\"Pfizer\")) %&gt;% kable(digits=c(4,5))\n\n\n\n\nTreatment\nPfizer\np.Pfizer\ns.Pfizer\n\n\n\n\nPlacebo\n90\n6e-03\n0.00063\n\n\nVaccine\n5\n3e-04\n0.00015\n\n\n\n\n\n\nggplot(covidVaccines,aes(x=Treatment,y=p.Pfizer,ymin=p.Pfizer-2*s.Pfizer,ymax=p.Pfizer+2*s.Pfizer)) + geom_pointrange()\n\n\n\n\n\n\n\n\n\nX^2 – Pfizer\nIn SPSS this is done by producing a cross-tab. We don’t have the number of negative cases in each arm of the study, but up to rounding error it is just the sample size, so we will use that.\n\n\n\nPfizer Cross Tab\n\n\n\n\n\nPhizer X2\n\n\n\nchisq.test(as.matrix(select(covidVaccines,Pfizer,N)))\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  as.matrix(select(covidVaccines, Pfizer, N))\nX-squared = 74.034, df = 1, p-value &lt; 2.2e-16\n\n\n\np &lt;- pull(covidVaccines,p.Pfizer)\ns &lt;- pull(covidVaccines,s.Pfizer)\n\nz &lt;- (p[1]-p[2])/sqrt(sum(s^2))\npz &lt;- 1-pnorm(z)\ncat(\"Z = \",round(z,2), \"p = \",round(pz,3),\"\\n\")\n\nZ =  8.75 p =  0 \n\n\n\n\n\nPfizer Risk Ratio\n\n\n\np &lt;- pull(covidVaccines,p.Pfizer)\nRR &lt;- p[1]/p[2]\nVE &lt;- 100*(1-1/RR)\ncat(\"Risk Ratio: \",round(RR,2),\n    \"Vaccine Effectiveness: \",round(VE,1),\"\\n\")\n\nRisk Ratio:  18 Vaccine Effectiveness:  94.4"
  },
  {
    "objectID": "IntroStats/CovidVaccines.html#moderna-vaccine-all-cases",
    "href": "IntroStats/CovidVaccines.html#moderna-vaccine-all-cases",
    "title": "Covid-19 Vaccines",
    "section": "Moderna Vaccine – All Cases",
    "text": "Moderna Vaccine – All Cases\nThere were around 30,000 volunteers in the Phase 3 trials; 15,000 in each arm.\n\ncovidVaccines %&gt;% \n  mutate(p.ModernaAll=ModernaAll/N) %&gt;%\n  mutate(s.ModernaAll=sqrt(p.ModernaAll*(1-p.ModernaAll)/N)) -&gt;\n  covidVaccines\n\nselect(covidVaccines,Treatment,contains(\"ModernaAll\")) %&gt;% kable(digits=c(4,5))\n\n\n\n\nTreatment\nModernaAll\np.ModernaAll\ns.ModernaAll\n\n\n\n\nPlacebo\n95\n0.0063\n0.00065\n\n\nVaccine\n5\n0.0003\n0.00015\n\n\n\n\n\n\nggplot(covidVaccines,aes(x=Treatment,y=p.ModernaAll,ymin=p.ModernaAll-2*s.ModernaAll,ymax=p.ModernaAll+2*s.ModernaAll)) + geom_pointrange()\n\n\n\n\n\n\n\n\n\nX^2 – Moderna (All Cases)\nIn SPSS this is done by producing a cross-tab. We don’t have the number of negative cases in each arm of the study, but up to rounding error it is just the sample size, so we will use that.\n\n\n\nModerna All Cases Cross Tab\n\n\n\n\n\nModerna All Cases X2\n\n\n\nchisq.test(as.matrix(select(covidVaccines,ModernaAll,N)))\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  as.matrix(select(covidVaccines, ModernaAll, N))\nX-squared = 78.942, df = 1, p-value &lt; 2.2e-16\n\n\n\np &lt;- pull(covidVaccines,p.ModernaAll)\ns &lt;- pull(covidVaccines,s.ModernaAll)\n\nz &lt;- (p[1]-p[2])/sqrt(sum(s^2))\npz &lt;- 1-pnorm(z)\ncat(\"Z = \",round(z,2), \"p = \",round(pz,3),\"\\n\")\n\nZ =  9.03 p =  0 \n\n\n\n\n\nModerna All Cases Risk Ratio\n\n\n\np &lt;- pull(covidVaccines,p.ModernaAll)\nRR &lt;- p[1]/p[2]\nVE &lt;- 100*(1-1/RR)\ncat(\"Risk Ratio: \",round(RR,2),\n    \"Vaccine Effectiveness: \",round(VE,1),\"\\n\")\n\nRisk Ratio:  19 Vaccine Effectiveness:  94.7"
  },
  {
    "objectID": "IntroStats/CovidVaccines.html#moderna-vaccine-severe-cases",
    "href": "IntroStats/CovidVaccines.html#moderna-vaccine-severe-cases",
    "title": "Covid-19 Vaccines",
    "section": "Moderna Vaccine – Severe Cases",
    "text": "Moderna Vaccine – Severe Cases\n\ncovidVaccines %&gt;% \n  mutate(p.ModernaSevere=ModernaSevere/N) %&gt;%\n  mutate(s.ModernaSevere=sqrt(p.ModernaSevere*(1-p.ModernaSevere)/N)) -&gt;\n  covidVaccines\n\nselect(covidVaccines,Treatment,contains(\"ModernaSevere\")) %&gt;% kable(digits=c(4,5))\n\n\n\n\nTreatment\nModernaSevere\np.ModernaSevere\ns.ModernaSevere\n\n\n\n\nPlacebo\n11\n7e-04\n0.00022\n\n\nVaccine\n0\n0e+00\n0.00000\n\n\n\n\n\n\nggplot(covidVaccines,aes(x=Treatment,y=p.ModernaSevere,ymin=p.ModernaSevere-2*s.ModernaSevere,ymax=p.ModernaSevere+2*s.ModernaSevere)) + geom_pointrange()\n\n\n\n\n\n\n\n\n\nX^2 – Moderna (All Cases)\nIn SPSS this is done by producing a cross-tab. We don’t have the number of negative cases in each arm of the study, but up to rounding error it is just the sample size, so we will use that.\n\n\n\nModerna Severe Cases Cross Tab\n\n\n\n\n\nModerna Severe Cases X2\n\n\n\nchisq.test(as.matrix(select(covidVaccines,ModernaSevere,N)))\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  as.matrix(select(covidVaccines, ModernaSevere, N))\nX-squared = 9.0869, df = 1, p-value = 0.002574\n\n\n\np &lt;- pull(covidVaccines,p.ModernaSevere)\ns &lt;- pull(covidVaccines,s.ModernaSevere)\n\nz &lt;- (p[1]-p[2])/sqrt(sum(s^2))\npz &lt;- 1-pnorm(z)\ncat(\"Z = \",round(z,2), \"p = \",round(pz,3),\"\\n\")\n\nZ =  3.32 p =  0 \n\n\n\np &lt;- pull(covidVaccines,p.ModernaSevere)\nRR &lt;- p[1]/p[2]\nVE &lt;- 100*(1-1/RR)\ncat(\"Risk Ratio: \",round(RR,2),\n    \"Vaccine Effectiveness: \",round(VE,1),\"\\n\")\n\nRisk Ratio:  Inf Vaccine Effectiveness:  100 \n\n\nYikes! The estimate for the chances of getting Severe Covid-19 with the virus is 0. Divide by zero error!\nBut probability zero means impossible. That is not right!\n\n\nContinuity Correction\nFix this by adding a conditinuity correction. We add 1/2 to all of the entries in the table.\nIn particular, this makes the estimated rate for getting severe COVID-19 \\(\\frac{1}{2}/(N+1)\\).\n\ncovidVaccines %&gt;% \n  mutate(p.ModernaSevere=(ModernaSevere+.5)/(N+1)) %&gt;%\n  mutate(s.ModernaSevere=sqrt(p.ModernaSevere*(1-p.ModernaSevere)/(N+1))) -&gt;\n  covidVaccines\n\nselect(covidVaccines,Treatment,contains(\"ModernaSevere\")) %&gt;% kable(digits=c(5,5))\n\n\n\n\nTreatment\nModernaSevere\np.ModernaSevere\ns.ModernaSevere\n\n\n\n\nPlacebo\n11\n0.00077\n0.00023\n\n\nVaccine\n0\n0.00003\n0.00005\n\n\n\n\n\n\nggplot(covidVaccines,aes(x=Treatment,y=p.ModernaSevere,ymin=p.ModernaSevere-2*s.ModernaSevere,ymax=p.ModernaSevere+2*s.ModernaSevere)) + geom_pointrange()\n\n\n\n\n\n\n\n\n\np &lt;- pull(covidVaccines,p.ModernaSevere)\nRR &lt;- p[1]/p[2]\nVE &lt;- 100*(1-1/RR)\ncat(\"Risk Ratio: \",round(RR,2),\n    \"Vaccine Effectiveness: \",round(VE,1),\"\\n\")\n\nRisk Ratio:  23 Vaccine Effectiveness:  95.7"
  },
  {
    "objectID": "IntroStats/CovidVaccines.html#references",
    "href": "IntroStats/CovidVaccines.html#references",
    "title": "Covid-19 Vaccines",
    "section": "References:",
    "text": "References:\n\nStatNews article on Pfizer vaccine: https://www.statnews.com/2020/11/09/covid-19-vaccine-from-pfizer-and-biontech-is-strongly-effective-early-data-from-large-trial-indicate/\nOfficial Protocol document from Pfizer: https://www.pfizer.com/science/coronavirus\nPfizer Press Release: https://www.pfizer.com/news/press-release/press-release-detail/pfizer-and-biontech-announce-vaccine-candidate-against\nModerna Press Release: https://investors.modernatx.com/news-releases/news-release-details/modernas-covid-19-vaccine-candidate-meets-its-primary-efficacy\nEntries from Andrew Gelman’s Blog: https://statmodeling.stat.columbia.edu/2020/11/16/estimating-efficacy-of-the-vaccine-from-95-true-infections/\n\nhttps://statmodeling.stat.columbia.edu/2020/11/11/the-pfizer-biontech-vaccine-may-be-a-lot-more-effective-than-you-think/\n\nHow to use SPSS to obtain Odd Ratio and Relative Risk http://brahms.emu.edu.tr/icetin/spss8-RR-OR.pdf"
  },
  {
    "objectID": "IntroStats/EffectSize.html",
    "href": "IntroStats/EffectSize.html",
    "title": "Effect Size Calculator",
    "section": "",
    "text": "If the units of a test are well known, the size of an effect is pretty easy to understand. For example, if a study found that on average, people on this diet lost about 5 lbs (2.5 kg) in a week, most people would know what that means.\nIn many other cases, the size of the statistic depends on the measure used to determine it. For example, if a researcher finds that students on average gain 5 points on a math test after playing a mathematical game, is that a big gain or a small gain? Unlike the weight loss experiment, we don’t have the experience with that test to judge.\nThere are a number of measures that can be used to put the size of the effect into perspective. Jacob Cohen proposed dividing the difference by the population standard deviation: \\[ d = \\frac{\\mu_1 - \\mu_0}{\\sigma} .\\] This has the advantage of putting things on a readily apparent scale. So, suppose in the example of the math game above, the effect size was \\(d=.1\\). If the game was just a short thing that took an hour, that would be a pretty big deal. On the other hand, if the students needed to play all year to get that effect, it might not be so good."
  },
  {
    "objectID": "IntroStats/EffectSize.html#simple-case-one-group",
    "href": "IntroStats/EffectSize.html#simple-case-one-group",
    "title": "Effect Size Calculator",
    "section": "Simple Case – One Group",
    "text": "Simple Case – One Group\nWhen we are looking at a single group, the definition of the effect size is fairly simple. The single variable usually is a difference score; e.g., posttest - pretest. The standard deviation of interest is the standard deviation of the population of difference scores (estimated by the sample).\n\n\n\n\n\n\nMean Difference:\n\n\n\n\n\nStandard Deviation of Difference:"
  },
  {
    "objectID": "IntroStats/EffectSize.html#complex-case-two-groups",
    "href": "IntroStats/EffectSize.html#complex-case-two-groups",
    "title": "Effect Size Calculator",
    "section": "Complex Case – Two Groups",
    "text": "Complex Case – Two Groups\nConceptually, the two group case is just as simple. The numerator is the difference between the group means. The denominator is the standard deviation of the population. There is a problem: we don’t have one population, we have two: Group 1 and Group 2. What to do?\nThe answer is to take an average. Actually, we take the average of the variances, and then take the square root. There is one further complication, the two groups might have different sizes. In this case, we take a weighted average, weighting by the degrees of freedom (sample size -1). Here is the formula: \\[ \\sigma^2_{pooled} = \\frac{(N_1 -1)\\sigma^2_1 +(N_2 -1)\\sigma^2_2}{N_1+N_2-1} ,\\] take the square root of that to get the standard deviation.\nAlthough one could calculate that by hand, the calculator below will do the job for you, and then calculate the effect size at the same time. Note that the pooled SD should always be in between the SDs of group 1 and group 2. For a rough and ready estimate, you could just take the number halfway between the two.\n\n\n\n\n\n\n\nStatistics for Group 1 (experimental/focal)\n\nMean:\n\n\n\nSD:\n\n\n\nN:\n\n\n\n\nStatistics for Group 2 (control/reference)\n\nMean:\n\n\n\nSD:\n\n\n\nN:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you want to do these calculations away from the internet, you can download this Excel spreadsheet which has the formula baked in: https://pluto.coe.fsu.edu/effectSize_d.xls\n\nHow big is big? Cohen’s guide to effect sizes.\nReally, the answer is entirely discipline specific. In Physics, it is quite common to be able to exert large forces and to measure very accurately, and hence be able to get very large effect sizes. In the social sciences and education, it often hard to control all of the variables that might affect the outcome, so the typical effect sizes are quite small.\nJacob Cohen presented a guideline for use in power analyses. This was really just for when you had no idea of what the size of the effect would be. He suggested:\n\n\n\nEffect\nd\n\n\n\n\nSmall\n.2\n\n\nMedium\n.5\n\n\nLarge\n.8\n\n\n\nHowever, these are really not designed for interpreting effects. There instead, you should compare to what other similar interventions are achieving.\nDylan Wiliams suggests that for educational applications, you might try dividing by the effect size of a year’s growth for that grade level. This changes rapidly with 1st graders growing nearly 2 SDs while a years worth of growth for a high school student is closer to .5 SDs. In high school, an effect size of .25 would be half a years growth, which is considerable.\n(Of course to measure small effects, you also need a very sensitive instrument, which in education means a longer test; the cost of testing is often prohibitive.)"
  },
  {
    "objectID": "IntroStats/KurtosisQQ.html",
    "href": "IntroStats/KurtosisQQ.html",
    "title": "Kurtosis Practice using Quantile-Quantile Plots",
    "section": "",
    "text": "In this exercise, the computer will generate 3 datasets: X, Y and Z. These will be randomly assigned to high (leptokurtic), medium (mesokurtic) and low (platykurtic) distributions. The data (sorted in order) are plotted on the Y axis and the quantiles of a standard normal (qnorm) distribution are plotted on the X axis. A normal distribution should appear as a straight line; leptokurtic and platykurtic distributions as ‘S’ or ‘Z’ curves. (A ‘U’ or ‘C’ shaped curve indicates skewness, not kurtosis.) Note: SPSS plots the normal quantiles on the Y axis and the data on the X: Which means that leptokurtic and platykurtic distributions will curve in the opposite direction from these Q-Q plots.\nYou can redraw from the same distributions by changing the sample size (higher sample sizes are easier to see).\n\n\n\n\n\n\nSample Size:\n\n50\n100\n500\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentify the kurtosis of each distribution.\n\n\n\n\n\n\nX\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\nY\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\nZ\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo try again with different distributions, reload the page. If you are having trouble, try increasing the sample size: sometimes a small sample won’t display the characteristics of the distribution strongly.\nHere are the other exercises in this series:\n\nSkewness Practice:\n\nHistograms\nBoxplots\nQ-Q Plots\n\nKurtosis Practice:\n\nHistograms\nBoxplots\nQ-Q Plots"
  },
  {
    "objectID": "IntroStats/KurtosisQQ.html#kurtosis-determination-exercise.",
    "href": "IntroStats/KurtosisQQ.html#kurtosis-determination-exercise.",
    "title": "Kurtosis Practice using Quantile-Quantile Plots",
    "section": "",
    "text": "In this exercise, the computer will generate 3 datasets: X, Y and Z. These will be randomly assigned to high (leptokurtic), medium (mesokurtic) and low (platykurtic) distributions. The data (sorted in order) are plotted on the Y axis and the quantiles of a standard normal (qnorm) distribution are plotted on the X axis. A normal distribution should appear as a straight line; leptokurtic and platykurtic distributions as ‘S’ or ‘Z’ curves. (A ‘U’ or ‘C’ shaped curve indicates skewness, not kurtosis.) Note: SPSS plots the normal quantiles on the Y axis and the data on the X: Which means that leptokurtic and platykurtic distributions will curve in the opposite direction from these Q-Q plots.\nYou can redraw from the same distributions by changing the sample size (higher sample sizes are easier to see).\n\n\n\n\n\n\nSample Size:\n\n50\n100\n500\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentify the kurtosis of each distribution.\n\n\n\n\n\n\nX\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\nY\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\nZ\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo try again with different distributions, reload the page. If you are having trouble, try increasing the sample size: sometimes a small sample won’t display the characteristics of the distribution strongly.\nHere are the other exercises in this series:\n\nSkewness Practice:\n\nHistograms\nBoxplots\nQ-Q Plots\n\nKurtosis Practice:\n\nHistograms\nBoxplots\nQ-Q Plots"
  },
  {
    "objectID": "IntroStats/ConditionalProbability.html",
    "href": "IntroStats/ConditionalProbability.html",
    "title": "ConditionalProbability",
    "section": "",
    "text": "On Oct 1, Merck announced exciting results for a new drug, Molnupiravir a pill for treating patients with COVID-19. The results were good enough by 29 days into the study, that they submitted the results to the FDA for emergency approval. (Note that the link above is to a press release and not to a scientific paper that has been peer or FDA review; but if the results hold up to scrutiny, this could be exciting.)\nHere is the relevant bit of the press release: &gt; At the interim analysis, molnupiravir reduced the risk of hospitalization or death by approximately 50%; 7.3% of patients who received molnupiravir were either hospitalized or died through Day 29 following randomization (28/385), compared with 14.1% of placebo-treated patients (53/377); p=0.0012.\nFrom this we can construct the following data table.\n\nn &lt;- c(drug=385,placebo=377)\nhd &lt;- c(drug=28,placebo=53)\nnhd &lt;- n-hd\ntab &lt;- data.frame(hd,nhd,n)\ntab1 &lt;- rbind(tab,Total=colSums(tab))\ntab1\n\n        hd nhd   n\ndrug    28 357 385\nplacebo 53 324 377\nTotal   81 681 762\n\n\nA mosaic plot allows us to look at this table graphically.\n\n## Subset to just the inner part of the table.\nppddat &lt;- as.matrix(tab[,1:2])\nnames(dimnames(ppddat)) &lt;- c(\"Treatment\",\"Outcome\")\nstrucplot(ppddat,labeling=labeling_values(\"observed\"))"
  },
  {
    "objectID": "IntroStats/ConditionalProbability.html#some-example-data.",
    "href": "IntroStats/ConditionalProbability.html#some-example-data.",
    "title": "ConditionalProbability",
    "section": "",
    "text": "On Oct 1, Merck announced exciting results for a new drug, Molnupiravir a pill for treating patients with COVID-19. The results were good enough by 29 days into the study, that they submitted the results to the FDA for emergency approval. (Note that the link above is to a press release and not to a scientific paper that has been peer or FDA review; but if the results hold up to scrutiny, this could be exciting.)\nHere is the relevant bit of the press release: &gt; At the interim analysis, molnupiravir reduced the risk of hospitalization or death by approximately 50%; 7.3% of patients who received molnupiravir were either hospitalized or died through Day 29 following randomization (28/385), compared with 14.1% of placebo-treated patients (53/377); p=0.0012.\nFrom this we can construct the following data table.\n\nn &lt;- c(drug=385,placebo=377)\nhd &lt;- c(drug=28,placebo=53)\nnhd &lt;- n-hd\ntab &lt;- data.frame(hd,nhd,n)\ntab1 &lt;- rbind(tab,Total=colSums(tab))\ntab1\n\n        hd nhd   n\ndrug    28 357 385\nplacebo 53 324 377\nTotal   81 681 762\n\n\nA mosaic plot allows us to look at this table graphically.\n\n## Subset to just the inner part of the table.\nppddat &lt;- as.matrix(tab[,1:2])\nnames(dimnames(ppddat)) &lt;- c(\"Treatment\",\"Outcome\")\nstrucplot(ppddat,labeling=labeling_values(\"observed\"))"
  },
  {
    "objectID": "IntroStats/ConditionalProbability.html#pr-ohd",
    "href": "IntroStats/ConditionalProbability.html#pr-ohd",
    "title": "ConditionalProbability",
    "section": "Pr (O=HD)",
    "text": "Pr (O=HD)\nWith no bar, indicating no conditioning, we are looking at the probability among all people in the sample. This is called the marginal probability because it comes from the margins (sums) of the table.\n\n\n\n\n\n\n\n\n\nThe marginal (no condition) probability of a negative (hospitalization or death) outcome is 81 (red areas)/762 (blue areas) = 0.106."
  },
  {
    "objectID": "IntroStats/ConditionalProbability.html#pr-ohd-t-drug",
    "href": "IntroStats/ConditionalProbability.html#pr-ohd-t-drug",
    "title": "ConditionalProbability",
    "section": "Pr (O=HD | T = Drug)",
    "text": "Pr (O=HD | T = Drug)\nThe bar conditions or restricts the sample to just the people who meet the condition, in this case, those that have taken the active treatment.\n\n\n\n\n\n\n\n\n\nThe condtional probability of a negative (hospitalization or death) outcome given the drug is 28 (red areas)/385 (purple areas) = 0.073."
  },
  {
    "objectID": "IntroStats/ConditionalProbability.html#pr-ohd-t-placebo",
    "href": "IntroStats/ConditionalProbability.html#pr-ohd-t-placebo",
    "title": "ConditionalProbability",
    "section": "Pr (O=HD | T = Placebo)",
    "text": "Pr (O=HD | T = Placebo)\nThe bar conditions or restricts the sample to just the people who meet the condition, in this case, those that have taken the placebo treatment.\n\n\n\n\n\n\n\n\n\nTThe condtional probability of a negative (hospitalization or death) outcome given the placebo is 53 (red areas)/377 (purple areas) = 0.141."
  },
  {
    "objectID": "IntroStats/ConditionalProbability.html#independence",
    "href": "IntroStats/ConditionalProbability.html#independence",
    "title": "ConditionalProbability",
    "section": "Independence",
    "text": "Independence\nIf the drug was independent of hospitalization, then the two probabilities we calculated above should be the same. In fact, when we calculate the ratio, we get 0.517. A huge improvement. No wonder Merck was so excited.\nWe should also be able to look at the column probabilities in the same way. The original sample is close to a 50-50 split between drug and placebo. (It actually 0.505, probably because they were not finished their recruting. This little bit of unbalance doesn’t matter). However, when we look at the fraction of the people who were hospitalized who got the drug \\(\\Pr(Treatment=drug | Outcome=HD)\\) we see it is 0.346. Once again this is very different.\nSuppose we have two variables \\(A\\) with possible outcomes \\(\\{a_1,\\ldots,a_J\\}\\) and \\(B\\) with possible outcomes \\(\\{b_1,\\ldots,b_K\\}\\). If \\(A\\) and \\(B\\) are independent, three different relationships will hold:\n\n\\(\\Pr(A=a_j | B=b_k) = \\Pr(A=a_j | B=b_k') = \\Pr(A=a_j) \\qquad \\forall j,k,k'\\)\n\\(\\Pr(B=b_k | A=a_j) = \\Pr(B=b_k | A=a_j') = \\Pr(B=b_j) \\qquad \\forall j,j',k\\)\n\\(\\Pr(A=a_j \\wedge B=b_k) = \\Pr(A=a_j) \\Pr(B=b_k) \\qquad \\forall j,k\\) (where \\(\\wedge\\) means and).\n\nRecall that the conditional probability is formally defined as \\[ \\Pr(A=a_j|B=b_k) = \\frac{\\Pr(A=a_j \\wedge B=b_k)}{\\Pr(B=b_k)} \\ .\\] So if \\(\\Pr(B=b_k)=0\\) for any \\(k\\), then the first expression doesn’t quite work right because we need to divide by zero. Similarly, the second has problems if \\(\\Pr(A=a_j)=0\\) for any \\(j\\). Therfore, the thrid expression is used as the definition, because it avoids the technical problems (divide by zero in certain cases) of the first two. But I think the first two give better intuition for what independence means."
  },
  {
    "objectID": "IntroStats/ConditionalProbability.html#the-chi-square-test",
    "href": "IntroStats/ConditionalProbability.html#the-chi-square-test",
    "title": "ConditionalProbability",
    "section": "The Chi-square test",
    "text": "The Chi-square test\nNote that even though the experimental design called for equal numbers in the drug and placebo arms of the studies, random events in how the participants were recruited made them slightly unequal. This is likely just luck, and not a serious problem.\nNow look at the different probabilities of a negative outcome for the drug and the placebo. How do we know if that is real, or just luck? One way we could answer that question is to build a statistical model for “just luck” and calculate how like the the observed data are if that model is true.\nIn this case, the model for “just luck” is the independence value up above. Using that we can calculate the expected values."
  },
  {
    "objectID": "IntroStats/LawOfLargeNumbers.html",
    "href": "IntroStats/LawOfLargeNumbers.html",
    "title": "Law of Large Numbers",
    "section": "",
    "text": "This is pretty close to the frequency definition of probability. Suppose the probability of some event is \\(p\\). Suppose further than we sample \\(N\\) times from the process that generates this event. Let \\(p_N\\) be the proportion of times the event occurs in \\(N\\) trials. As \\(N\\) gets bigger and bigger, \\(p_N\\) gets closer and closer to \\(p\\).\n(Skip this unless you are good with calculus.) This is one of those epsilon-delta theorems. So let \\(\\delta\\) be a difference from \\(p\\) and let \\(\\epsilon\\) be a small probability. For any \\(\\epsilon\\) and \\(\\delta\\), there exists an \\(N\\) such that \\(P(|p_N-p|&gt;\\delta) &lt; \\epsilon\\).\n\n\nIn the picture below, pick a probability \\(p\\) and a sample size \\(N\\). The computer will generate samples up to \\(N\\) and plot \\(p_N\\).\nThe \\(\\delta\\)-line is an error bound plus or minus \\(\\delta\\) units from the target \\(p\\). This is a target so you can judge how close you got.\n\n\n\n\n\n\nMaximum Sample Size:\n\n50\n100\n200\n500\n1000\n\n\n\n\n\n\nProbability of event (p)\n\n\n\n\n\nDistance of reference line from target (delta)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can use the Law of Large Numbers to prove an important theorem. As the sample size gets larger and larger, the sample looks more and more like the population it is drawn from.\n Technically, the Law of Large Numbers refers to the result above. But we can use it so show a very important basis of statistics. Suppose we have some kind of distribution, \\(F(x)\\), that generates numbers, \\(X\\). Recall that the definition of \\(F(x)=\\Pr(X \\leq x)\\).\n Draw a sample of size \\(N\\) from this distribution. Now consider the sampled data points \\(X_1,\\ldots,X_N\\), and consider sampling a new value \\(Y\\) from that distribution. Let \\(F_N(y) = \\Pr(Y \\leq y)\\). This is sometimes called the bootstrap distribution.\n By the law of large numbers, for every \\(y\\), as \\(N\\) gets large \\(F_N(y) \\rightarrow F(y)\\). So the sample distribution \\(F_N()\\) converges to the \\(F()\\).\n\n\n\nPick a distribution: * Normal – standard normal * Exponential – highly skewed * Gamma (shape = 3) – skewed * T (df =3) – high kurtosis\nSlide the sample size up and down, notice how the empirical distribution function and histogram coverge to the theoretical distribution function and density.\n\n\n\n\n\n\nDistribution Type\n\nNormal\nExponential\nGamma\nT\n\n\n\n\n\n\nMaximum Sample Size:\n\n\n\n\n\n\n\n\n\n\nSee also the animated version."
  },
  {
    "objectID": "IntroStats/LawOfLargeNumbers.html#a-demonstration.",
    "href": "IntroStats/LawOfLargeNumbers.html#a-demonstration.",
    "title": "Law of Large Numbers",
    "section": "",
    "text": "In the picture below, pick a probability \\(p\\) and a sample size \\(N\\). The computer will generate samples up to \\(N\\) and plot \\(p_N\\).\nThe \\(\\delta\\)-line is an error bound plus or minus \\(\\delta\\) units from the target \\(p\\). This is a target so you can judge how close you got.\n\n\n\n\n\n\nMaximum Sample Size:\n\n50\n100\n200\n500\n1000\n\n\n\n\n\n\nProbability of event (p)\n\n\n\n\n\nDistance of reference line from target (delta)"
  },
  {
    "objectID": "IntroStats/LawOfLargeNumbers.html#convergence-of-distributions-boot-strap-distribution",
    "href": "IntroStats/LawOfLargeNumbers.html#convergence-of-distributions-boot-strap-distribution",
    "title": "Law of Large Numbers",
    "section": "",
    "text": "We can use the Law of Large Numbers to prove an important theorem. As the sample size gets larger and larger, the sample looks more and more like the population it is drawn from.\n Technically, the Law of Large Numbers refers to the result above. But we can use it so show a very important basis of statistics. Suppose we have some kind of distribution, \\(F(x)\\), that generates numbers, \\(X\\). Recall that the definition of \\(F(x)=\\Pr(X \\leq x)\\).\n Draw a sample of size \\(N\\) from this distribution. Now consider the sampled data points \\(X_1,\\ldots,X_N\\), and consider sampling a new value \\(Y\\) from that distribution. Let \\(F_N(y) = \\Pr(Y \\leq y)\\). This is sometimes called the bootstrap distribution.\n By the law of large numbers, for every \\(y\\), as \\(N\\) gets large \\(F_N(y) \\rightarrow F(y)\\). So the sample distribution \\(F_N()\\) converges to the \\(F()\\)."
  },
  {
    "objectID": "IntroStats/LawOfLargeNumbers.html#demonstration-of-convergence-of-distributions.",
    "href": "IntroStats/LawOfLargeNumbers.html#demonstration-of-convergence-of-distributions.",
    "title": "Law of Large Numbers",
    "section": "",
    "text": "Pick a distribution: * Normal – standard normal * Exponential – highly skewed * Gamma (shape = 3) – skewed * T (df =3) – high kurtosis\nSlide the sample size up and down, notice how the empirical distribution function and histogram coverge to the theoretical distribution function and density.\n\n\n\n\n\n\nDistribution Type\n\nNormal\nExponential\nGamma\nT\n\n\n\n\n\n\nMaximum Sample Size:\n\n\n\n\n\n\n\n\n\n\nSee also the animated version."
  },
  {
    "objectID": "IntroStats/NormalParams.html",
    "href": "IntroStats/NormalParams.html",
    "title": "Normal Parameters",
    "section": "",
    "text": "A parameter is a value that can be changed in a statistical model. For example, the mean and standard deviation are the parameters of the normal distribution, which is a model for a population. Changing the value of a parameter, changes the model. We can see that in the illustration below. Try changing the values of the mean and standard deviation and see what happens to the shape of the curve."
  },
  {
    "objectID": "IntroStats/NormalParams.html#inputs-and-outputs",
    "href": "IntroStats/NormalParams.html#inputs-and-outputs",
    "title": "Normal Parameters",
    "section": "Inputs and Outputs",
    "text": "Inputs and Outputs\n\n\n\n\n\n\nMean:\n\n\n\n\n\nStandard Deviation:"
  },
  {
    "objectID": "IntroStats/NormalParams.html#scale-and-location-parameters",
    "href": "IntroStats/NormalParams.html#scale-and-location-parameters",
    "title": "Normal Parameters",
    "section": "Scale and Location Parameters",
    "text": "Scale and Location Parameters\nThe mean has a special role in the normal distribution; it determines where the center of the curve is. This makes it a location parameter.\nThe standard deviation has a special role in the normal distribution; it stretches and shrinks the curve around the mean. This makes it a scale parameter.\nSometimes, the effects of scale and location parameters can be hard to see. This is because most statistical graphics packages adjust the axis of the graph, so that the curve will always appear centered in the plotting window. In the normal curve above, I fixed the plotting window so that you can see the curve move. In the example below, I let the plotting window adjust with the curve. Notice how the curve stays the same, but the labels on the axis change.\n\n\n\n\n\n\nMean:\n\n\n\n\n\nStandard Deviation:"
  },
  {
    "objectID": "IntroStats/KurtosisBoxplots.html",
    "href": "IntroStats/KurtosisBoxplots.html",
    "title": "Kurtosis Boxplot Practice",
    "section": "",
    "text": "In this exercise, the computer will generate 3 datasets: X, Y and Z. These will be randomly assigned to high (leptokurtic), medium (mesokurtic) and low (platykurtic) distributions.\nYou can redraw from the same distributions by changing the sample size.\n\n\n\n\n\n\nSample Size:\n\n50\n100\n500\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentify the kurtosis of each distribution.\n\n\n\n\n\n\nX\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\nY\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\nZ\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo try again with different distributions, reload the page. If you are having trouble, try increasing the sample size: sometimes a small sample won’t display the characteristics of the distribution strongly."
  },
  {
    "objectID": "IntroStats/KurtosisBoxplots.html#kurtosis-determination-exercise.",
    "href": "IntroStats/KurtosisBoxplots.html#kurtosis-determination-exercise.",
    "title": "Kurtosis Boxplot Practice",
    "section": "",
    "text": "In this exercise, the computer will generate 3 datasets: X, Y and Z. These will be randomly assigned to high (leptokurtic), medium (mesokurtic) and low (platykurtic) distributions.\nYou can redraw from the same distributions by changing the sample size.\n\n\n\n\n\n\nSample Size:\n\n50\n100\n500\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentify the kurtosis of each distribution.\n\n\n\n\n\n\nX\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\nY\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\nZ\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo try again with different distributions, reload the page. If you are having trouble, try increasing the sample size: sometimes a small sample won’t display the characteristics of the distribution strongly."
  },
  {
    "objectID": "IntroStats/KurtosisBoxplots.html#what-to-look-for",
    "href": "IntroStats/KurtosisBoxplots.html#what-to-look-for",
    "title": "Kurtosis Boxplot Practice",
    "section": "What to look for",
    "text": "What to look for\n\nIn a normal distribution the whiskers extend about 1.5 box-lengths (IQR)s from the hinges (sides of the box).\n\nIf the box is long compared to the whiskers, this is a sign the distribuiton is platykurtic.\nIf the box is short compared to the whiskers, this is a sign the distribution is leptokurtic.\n\nThe whiskers only extend to the farthest data point within 1.5 IQRs from the box. So if there is high kurtosis, this will show up as lots of outliers.\n\nWith a normal distribution, there is often 1–2 outliers per 100 data points. Much more than that is a sign of high kurtosis.\n\nIs the length of the box (IQR) long compared to the length of the whiskers?"
  },
  {
    "objectID": "IntroStats/KurtosisBoxplots.html#related-pages",
    "href": "IntroStats/KurtosisBoxplots.html#related-pages",
    "title": "Kurtosis Boxplot Practice",
    "section": "Related Pages:",
    "text": "Related Pages:\nHere are the other exercises in this series:\n\nSkewness Practice:\n\nHistograms\nBoxplots\nQ-Q Plots\n\nKurtosis Practice:\n\nHistograms\nBoxplots\nQ-Q Plots"
  },
  {
    "objectID": "IntroStats/RegressionPrediction.html",
    "href": "IntroStats/RegressionPrediction.html",
    "title": "Regression Prediction Error",
    "section": "",
    "text": "We will work with an example data set from Ezekiel (1930) which provides information about the speed of a number of cars and the stopping distance in feet.\n\nhelp(cars)\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00  \n\n\nLets take a quick look at these data.\n\nplot(dist~speed,data=cars,xlab=\"Speed (mph)\", ylab=\"Stopping Distance (ft)\")\nabline(lm(dist~speed,data=cars))\nlines(lowess(cars),col=2,lty=2)\n\n\n\n\n\n\n\n\nThe solid black line is the least squares regression line, or basic model.\nThe dashed red line is a lowess curve fit to the same date.\n\nThere may be a little bit of a curve here, but it is hard to see.\n\nWe will go ahead and fit a regression using least squares. (This the the lm or linear model function in R.)\n\ncars.fit &lt;- lm (dist~speed,data=cars)\nprint(cars.fit)\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nCoefficients:\n(Intercept)        speed  \n    -17.579        3.932  \n\n\nThe method of least squares or maximum likelihood (which in the case of simple regression are the same) finds the single best fitting line.\n\nLeast Squares means the line has the smallest some of squared residuals.\nMaximum Likelihood means that these are the parameters (slope and intercept) that have the highest probability of generating the target data."
  },
  {
    "objectID": "IntroStats/RegressionPrediction.html#the-cars-data-set",
    "href": "IntroStats/RegressionPrediction.html#the-cars-data-set",
    "title": "Regression Prediction Error",
    "section": "",
    "text": "We will work with an example data set from Ezekiel (1930) which provides information about the speed of a number of cars and the stopping distance in feet.\n\nhelp(cars)\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00  \n\n\nLets take a quick look at these data.\n\nplot(dist~speed,data=cars,xlab=\"Speed (mph)\", ylab=\"Stopping Distance (ft)\")\nabline(lm(dist~speed,data=cars))\nlines(lowess(cars),col=2,lty=2)\n\n\n\n\n\n\n\n\nThe solid black line is the least squares regression line, or basic model.\nThe dashed red line is a lowess curve fit to the same date.\n\nThere may be a little bit of a curve here, but it is hard to see.\n\nWe will go ahead and fit a regression using least squares. (This the the lm or linear model function in R.)\n\ncars.fit &lt;- lm (dist~speed,data=cars)\nprint(cars.fit)\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nCoefficients:\n(Intercept)        speed  \n    -17.579        3.932  \n\n\nThe method of least squares or maximum likelihood (which in the case of simple regression are the same) finds the single best fitting line.\n\nLeast Squares means the line has the smallest some of squared residuals.\nMaximum Likelihood means that these are the parameters (slope and intercept) that have the highest probability of generating the target data."
  },
  {
    "objectID": "IntroStats/RegressionPrediction.html#lots-of-different-regression-lines",
    "href": "IntroStats/RegressionPrediction.html#lots-of-different-regression-lines",
    "title": "Regression Prediction Error",
    "section": "Lots of different regression lines",
    "text": "Lots of different regression lines\nI’ll try the regression using a different method (Markov Chain Monte Carlo, or MCMC). In this method we sample 4000 different plausible sets of parameters that could have given rise to the data. (These are sampled with a probability proportional to how likely they are to have generated the observed data).\nThe printed summary shows the median of the 4000 samples. It should be close to, but not exactly the same as, the least squares/maximum likelihood estimate.\n\n\n\n\n\n\nNote\n\n\n\nI can’t run stan on the server, so I’ll run it first on my local file and save the result.\n\nlibrary(rstanarm)  \n\nLoading required package: Rcpp\n\n\nThis is rstanarm version 2.32.2\n\n\n- See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!\n\n\n- Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.\n\n\n- For execution on a local, multicore CPU with excess RAM we recommend calling\n\n\n  options(mc.cores = parallel::detectCores())\n\ncars.mcmc &lt;- stan_glm(dist~speed,data=cars,refresh=0)\nsaveRDS(cars.mcmc,\"cars_mcmc.RDS\")\n\nThe next bit of code will read the results from a file instead of running stan.\n\n\n\ncars.mcmc &lt;- readRDS(\"cars_mcmc.RDS\")\ncars.coef &lt;- as.matrix(cars.mcmc$stanfit)\nprint(cars.mcmc)\n\nstan_glm\n family:       gaussian [identity]\n formula:      dist ~ speed\n observations: 50\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) -17.4    6.7 \nspeed         3.9    0.4 \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 15.6    1.6  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg"
  },
  {
    "objectID": "IntroStats/RegressionPrediction.html#mean-confidence-interval",
    "href": "IntroStats/RegressionPrediction.html#mean-confidence-interval",
    "title": "Regression Prediction Error",
    "section": "Mean Confidence Interval",
    "text": "Mean Confidence Interval\nThe MCMC approach is useful because it helps us remember that the estimates that are produced by the [least squares] regression are not the truth, but rather just the most likely set of parameters. There are other possibilities that are nearly as likely.\nThe next graph is designed to show this.\nThe first N (you can adjust using the slider) samples from the MCMC are plotted as gray lines.\nThe least squares line is plotted in black.\n\n\n\n\n\n\nConfidence Level:\n\n50\n68\n90\n95\n99\n\n\n\n\n\n\nNumber of plausible values to plot\n\n\n\n\n\n\n\n\n\n\nNote the dashed curves surrounding the regression line.\nThese are the confidence interval for the regression line.\nThe level of the confidence interval is how many of these plausible regression lines should fit between the dashed curves (expressed as a percentage).\nSPSS calls this the “mean” prediction interval. R calls it the “confidence” interval.\nYou can use the graph (or the R predict function, or the prediction option in SPSS) to get a prediction for the average (over a number of trials) stopping time at a given speed."
  },
  {
    "objectID": "IntroStats/RegressionPrediction.html#individual-prediction-interval",
    "href": "IntroStats/RegressionPrediction.html#individual-prediction-interval",
    "title": "Regression Prediction Error",
    "section": "Individual Prediction Interval",
    "text": "Individual Prediction Interval\nThe mean confidence interval above is for the average over many attempts at stopping the car.\nWe don’t expect a single attempt to fall exactly on the line.\n\n68% of the time we expect to be one standard error above or below the line.\n95% of the time we expect to be two standard errors above or below the line.\nTo get the total error, we need to add\n\nThe error in the regression line (see above)\nThe error around the regression line.\n\n\n(Actually, we add these on the squared variance scale).\nThe picture below shows the individual prediction interval. Once again, you can pick your confidence level.\nSPSS calls this the individual prediction interval. R calls it the prediction interval.\n\n\n\n\n\n\nConfidence Level:\n\n50\n68\n90\n95\n99\n\n\n\n\n\n\n\n\n\n\n\nLook at the area in the graph which is colored cyan.\nThese are predictions that the car will stop in negative distance. Impossible!\nThe model is wrong.\nThat shouldn’t worry us, models are always wrong. The just might be close enough to be right to be useful.\nWe might say that the linear model is useful, but only if the car is going 5 mph or more."
  },
  {
    "objectID": "IntroStats/RegressionPrediction.html#model-checking",
    "href": "IntroStats/RegressionPrediction.html#model-checking",
    "title": "Regression Prediction Error",
    "section": "Model Checking",
    "text": "Model Checking\nNote that there was a slight curve in the lowess line in the scatterplot at the top of this analysis.\nSometimes the curve is easier to see if we take the linear trend out.\nWe can do this by plotting the residuals versus the fitted values.\n In a simple regression, this is the same as plotting against \\(X\\), as the fitted values are just a linear transformation of \\(X\\) (and the graph will just be rescaled to fit). For multiple regression, the fitted values are a mix of all the \\(X\\) values, so this plot is a useful summary.\n\n\n\n\n\n\n\n\n\nLooking a little more closely, we can see the curve.\nIt would be easy to miss without the lowess line, but the lowess line points it out to us.\nThere is a little bit of curvature, curving up at the lower distances, keeping the stopping distances in positive territory.\nSo what to conclude?\n\nIn the range of 5 mph – 25 mph the linear model looks pretty good.\nFor low speeds, we need a better model.\nMaybe we need a better model for higher speeds as well."
  },
  {
    "objectID": "IntroStats/CorrelationExamples.html",
    "href": "IntroStats/CorrelationExamples.html",
    "title": "Scatterplot examples",
    "section": "",
    "text": "This demonstration will use some random data. Lets start by generating the random data. So give a [random seed][seed] and pick a sample size for your sample.\nSample Size:\n\n25\n50\n100\n250\n500\n1000\n\n\n\n\n\n\nRandom number Seed (integer)"
  },
  {
    "objectID": "IntroStats/CorrelationExamples.html#mostly-linear",
    "href": "IntroStats/CorrelationExamples.html#mostly-linear",
    "title": "Scatterplot examples",
    "section": "Mostly linear",
    "text": "Mostly linear\nThis happens when we have a moderately high to strong correlation.\n\n\n\n\n\n\nCorrelation Coefficient:\n\n\n\n\n\n\n\n\nNegative Correlation"
  },
  {
    "objectID": "IntroStats/CorrelationExamples.html#blobby-elipse",
    "href": "IntroStats/CorrelationExamples.html#blobby-elipse",
    "title": "Scatterplot examples",
    "section": "Blobby Elipse",
    "text": "Blobby Elipse\nAs the correlation coefficient gets lower, the scatterplot looks more blobby, but you can still tell that there is a slope. This is a weak to moderate correlation.\n\n\n\n\n\n\nCorrelation Coefficient:\n\n\n\n\n\n\n\n\nNegative Correlation"
  },
  {
    "objectID": "IntroStats/CorrelationExamples.html#no-relationship",
    "href": "IntroStats/CorrelationExamples.html#no-relationship",
    "title": "Scatterplot examples",
    "section": "No Relationship",
    "text": "No Relationship\nNot much is going on here. One thing that confuses people is the idea that linear regression doesn’t work here. Actually, it gives a quite accurate picture: it tells you that not much is going on, which is what is actually happening. The prediction from the regression will be that \\(\\bar Y\\) is the best predicted value for \\(Y\\).\n\n\n\n\n\n\nCorrelation Coefficient:\n\n\n\n\n\n\n\n\nNegative Correlation"
  },
  {
    "objectID": "IntroStats/CorrelationExamples.html#curve",
    "href": "IntroStats/CorrelationExamples.html#curve",
    "title": "Scatterplot examples",
    "section": "Curve",
    "text": "Curve\nA curved relationship doesn’t look like a line.\nConsider a quadradic relationship: \\[ Y = b_2 X^2 + b_1 X + b_0 + \\epsilon\\] This is a multiple (or quadradic) regression. You can adjust the coefficients in the plot below.\n\n\n\n\n\n\nQuadradic Term Slope:\n\n\n\n\n\nLinear Term Slope:\n\n\n\n\n\nIntercept:\n\n\n\n\n\nError Standard Deviation:\n\n\n\n\n\n\n\n\n\n\nIf we try to run a linear regression when the relationship is curved, it will only tell us part of the story. The story it will tell is the red line, and not the blue curve."
  },
  {
    "objectID": "IntroStats/CorrelationExamples.html#broken-lines",
    "href": "IntroStats/CorrelationExamples.html#broken-lines",
    "title": "Scatterplot examples",
    "section": "Broken Lines",
    "text": "Broken Lines\nSometimes the reltionship changes somewhere through the range of the data. Often this is a ceiling effect: the effect of \\(X\\) on \\(Y\\) hits a ceiling. For example, in the first couple of years of teaching, the ability of new teachers rises very rapidly as they gain experience. But after 3–5 years, the effect levels out and the teachers grow much more slowly.\nIdeally we would fit two linear regression to these data splitting at a certain value of \\(X\\), \\(x_0\\). So,\n\\[ Y = \\begin{cases}\nb_{11} X + b_{01} + \\epsilon & \\text {when} X \\leq x_0 \\\\\nb_{12} X + b_{02} + \\epsilon & \\text {when} X \\ge x_0\n\\end{cases}\n\\]\n\n\n\n\n\n\nFirst Slope:\n\n\n\n\n\nSecond Slope:\n\n\n\n\n\nCrossover Point (x[0])\n\n\n\n\n\nError Standard Deviation:\n\n\n\n\n\n\n\n\n\n\nCheck out this page to practice identifying these."
  },
  {
    "objectID": "IntroStats/Geyser.html",
    "href": "IntroStats/Geyser.html",
    "title": "Histogram Bins",
    "section": "",
    "text": "The data show the duration (in minutes) of eruptions of the Geyser ‘Old Faithful’ in Yellowstone National Park, Wyoming. Härdle (1991)."
  },
  {
    "objectID": "IntroStats/Geyser.html#bin-size-and-bandwidth.",
    "href": "IntroStats/Geyser.html#bin-size-and-bandwidth.",
    "title": "Histogram Bins",
    "section": "Bin size and bandwidth.",
    "text": "Bin size and bandwidth.\nThe number of histograms in a describe how much detail you get. Try adjusting thing number to see the effect. Too few bins might cause the viewer to miss out on important details. Too many bins might cause the viewer to see details that are just an artifact of the sample (which might be different if the data were taken from a different window of time).\nThe smooth line is a kernel smoother (a technique which available in R, but not SPSS). The bandwidth of the smoother is like the bin size of the histogram. Large bandwidths provide less detail and smaller provide more.\nWith both bin sizes and bandwidths: * The analyst sometimes need to try a few values to find the best one for your purposes. * The defaults in the stat packages are usually a good starting point. Try more bins (or smaller bandwidth) and fewer bins (less bandwidth) than the automatically chosen starting point.\n\n\n\n\n\n\nNumber of bins:\n\n10\n20\n35\n50\n\n\n\n\n\n\nBandwidth adjustment:\n\n\n\n\n\n\n\n\n\n\nHärdle, W. (1991). Smoothing Techniques with Implementation in S. New York: Springer."
  },
  {
    "objectID": "IntroStats/Geyser.html#credits",
    "href": "IntroStats/Geyser.html#credits",
    "title": "Histogram Bins",
    "section": "Credits",
    "text": "Credits\nI borrowed this document from one of the Shiny sample documents. Original instructions below. (Russell Almond).\nThis R Markdown document is made interactive using Shiny. Unlike the more traditional workflow of creating static reports, you can now create documents that allow your readers to change the assumptions underlying your analysis and see the results immediately.\nTo learn more, see Interactive Documents."
  },
  {
    "objectID": "IntroStats/SkewnessQQ.html",
    "href": "IntroStats/SkewnessQQ.html",
    "title": "Skewness Practice with Quantile-Quantile Plots",
    "section": "",
    "text": "In this exercise, the computer will generate 3 datasets: A, B and C. These will be randomly assigned to a positively skewed, negatively skewed, and symmetric distribution type. The data (sorted in order) are plotted on the Y axis and the quantiles of a standard normal (qnorm) distribution are plotted on the X axis. A normal distribution should appear as a straight line; positively skewed distributions will yield a ‘U’ shaped curve, and negatively skewed distributions a ‘C’ shaped curve. (An ‘S’ or ‘Z’ shaped curve indicates kurtosis, not skewness). Note: SPSS plots the normal quantiles on the Y axis and the data on the X: Which means that leptokurtic and platykurtic distributions will curve in the opposite direction from these Q-Q plots.\nYou can redraw from the same distributions by changing the sample size. (Bigger sample sizes are easier.)\n\n\n\n\n\n\nSample Size:\n\n50\n100\n500\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentify the skewness of each distribution.\n\n\n\n\n\n\nA\n\nUnknown\nNegatively Skewed\nPositively Skewed\nSymmetric\n\n\n\n\n\n\nB\n\nUnknown\nNegatively Skewed\nPositively Skewed\nSymmetric\n\n\n\n\n\n\nC\n\nUnknown\nNegatively Skewed\nPositively Skewed\nSymmetric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo try again with different distributions, reload the page. If you are having trouble, try increasing the sample size: sometimes a small sample won’t display the characteristics of the distribution strongly.\nHere are the other exercises in this series:\n\nSkewness Practice:\n\nHistograms\nBoxplots\nQ-Q Plots\n\nKurtosis Practice:\n\nHistograms\nBoxplots\nQ-Q Plots"
  },
  {
    "objectID": "IntroStats/SkewnessQQ.html#skewness-determination-exercise.",
    "href": "IntroStats/SkewnessQQ.html#skewness-determination-exercise.",
    "title": "Skewness Practice with Quantile-Quantile Plots",
    "section": "",
    "text": "In this exercise, the computer will generate 3 datasets: A, B and C. These will be randomly assigned to a positively skewed, negatively skewed, and symmetric distribution type. The data (sorted in order) are plotted on the Y axis and the quantiles of a standard normal (qnorm) distribution are plotted on the X axis. A normal distribution should appear as a straight line; positively skewed distributions will yield a ‘U’ shaped curve, and negatively skewed distributions a ‘C’ shaped curve. (An ‘S’ or ‘Z’ shaped curve indicates kurtosis, not skewness). Note: SPSS plots the normal quantiles on the Y axis and the data on the X: Which means that leptokurtic and platykurtic distributions will curve in the opposite direction from these Q-Q plots.\nYou can redraw from the same distributions by changing the sample size. (Bigger sample sizes are easier.)\n\n\n\n\n\n\nSample Size:\n\n50\n100\n500\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentify the skewness of each distribution.\n\n\n\n\n\n\nA\n\nUnknown\nNegatively Skewed\nPositively Skewed\nSymmetric\n\n\n\n\n\n\nB\n\nUnknown\nNegatively Skewed\nPositively Skewed\nSymmetric\n\n\n\n\n\n\nC\n\nUnknown\nNegatively Skewed\nPositively Skewed\nSymmetric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo try again with different distributions, reload the page. If you are having trouble, try increasing the sample size: sometimes a small sample won’t display the characteristics of the distribution strongly.\nHere are the other exercises in this series:\n\nSkewness Practice:\n\nHistograms\nBoxplots\nQ-Q Plots\n\nKurtosis Practice:\n\nHistograms\nBoxplots\nQ-Q Plots"
  },
  {
    "objectID": "IntroStats/PoissonParms.html",
    "href": "IntroStats/PoissonParms.html",
    "title": "Poisson Params",
    "section": "",
    "text": "The Poisson distribution is a distribution for counts of events.\nAssume the following things:\n\nEvents happen at a rate \\(\\lambda\\) per unit interval on average.\nCount the number of events in a time interval of \\(T\\) units.\nAssume that the events happen at a uniform rate throughout the interval (e.g., we don’t get more customers in the morning than the afternoon).\n\nThe the number of events, \\(X\\), follows a Poisson distribution.\n\\[P(X=x) = \\frac{(\\lambda T)^x}{x!}e^{-\\lambda T}\\] The distribution looks like:\n\n\n\n\n\n\nExpected number of events per unit time\n\n\n\n\n\nTime Interval:\n\n\n\n\n\n\n\n\n\n\nThe mean and variance of the Poisson distribution are \\(\\lambda T\\) and \\(\\lambda T\\).\nAs the variance grows pretty quickly, statisticians will often take the square root of count data (especially if there is heteroscedasticity) to stabilize the variance."
  },
  {
    "objectID": "IntroStats/index.html",
    "href": "IntroStats/index.html",
    "title": "Index of R Demonstrations for Intro Stat",
    "section": "",
    "text": "Florida State University\nThese are demonstrations which were written for EDF 5400, which is an introductory statistics class taught at the graduate level.",
    "crumbs": [
      "Home",
      "Intro Stats"
    ]
  },
  {
    "objectID": "IntroStats/index.html#index-of-r-demonstrations",
    "href": "IntroStats/index.html#index-of-r-demonstrations",
    "title": "Index of R Demonstrations for Intro Stat",
    "section": "Index of R Demonstrations",
    "text": "Index of R Demonstrations\n\n\n\n\n\n\n\n\nCC-BY\n\n\nThese are licensed under the creative commons CC BY license. You many distribute, remix, adapt, and build upon the material in any medium or format, so long as attribution is given to the creator.\nFor more information contact Russell Almond.\nThe Source files for these demonstrations can be found at https://pluto.coe.fsu.edu/svn/common/rgroup-shiny/IntroStats",
    "crumbs": [
      "Home",
      "Intro Stats"
    ]
  },
  {
    "objectID": "IntroStats/ConfidenceInterval.html",
    "href": "IntroStats/ConfidenceInterval.html",
    "title": "Confidence Intervals",
    "section": "",
    "text": "The simple heuristic for the confidence interval is that if we take a sample and calculate an estimator and a standard error for that estimator; 95% of the time the estimand will be within 2 standard errors of the estimate. This heuristic works best for sample means, because by the central limit theorem the distribution of the sample mean will be approximately normal. It also works fairly well for other statistics, like regression slopes.\nLet the goal be to produce an interval which \\((1-\\alpha)\\)% of the time captures the estimand. If we assume that the estimate, \\(\\widetilde{f(Y)}\\), is approximately normally distributed with a mean of the estimand, \\(f(Y)\\) and a standard deviation of the standard error of the estimate, \\(\\sigma_{\\widetilde{f(Y)}}\\), (i.e., we are assuming that central limit theorem holds at least approximately for \\(\\widetilde{f(Y)}\\)), then we can produce a confidence interval using the following expression: \\[ \\widetilde{f(Y)} \\pm z_{1-\\alpha/2}\\ \\sigma_{\\widetilde{f(Y)}}\\; .\\]\nUgh. Lets break this apart using an example. Let’s say what we are interested in is the sample mean. Then \\(\\widetilde{f(Y)} = \\bar Y\\) is just the sample mean. The quantile \\(z_{1-\\alpha/2}\\) depends on the desired accuracy. The default choice is \\(1-\\alpha=.95\\), so \\(1-\\alpha/2 = .975\\), and looking this up on the normal table \\(z_{.975}\\approx 1.96 \\approx 2\\). Finally, \\(\\sigma_{\\widetilde{f(Y)}}\\) is the standard error of the mean, so if the population standard deviation of \\(Y\\) is \\(\\sigma_Y\\) and the sample size is \\(N\\), then \\(\\sigma_{\\widetilde{f(Y)}} = \\sigma_Y/\\sqrt{N}\\). This gives us the slightly easier to look at: \\[\\bar Y \\pm 1.96 \\sigma_Y/\\sqrt{N} \\approx \\bar Y \\pm 2 \\sigma_Y/\\sqrt{N}\\ ;\\] Note that we are assuming that \\(\\sigma_Y\\) is known here. If we need to estimate it from the data, we need a slightly different formula given later."
  },
  {
    "objectID": "IntroStats/ConfidenceInterval.html#confidence-intervals",
    "href": "IntroStats/ConfidenceInterval.html#confidence-intervals",
    "title": "Confidence Intervals",
    "section": "",
    "text": "The simple heuristic for the confidence interval is that if we take a sample and calculate an estimator and a standard error for that estimator; 95% of the time the estimand will be within 2 standard errors of the estimate. This heuristic works best for sample means, because by the central limit theorem the distribution of the sample mean will be approximately normal. It also works fairly well for other statistics, like regression slopes.\nLet the goal be to produce an interval which \\((1-\\alpha)\\)% of the time captures the estimand. If we assume that the estimate, \\(\\widetilde{f(Y)}\\), is approximately normally distributed with a mean of the estimand, \\(f(Y)\\) and a standard deviation of the standard error of the estimate, \\(\\sigma_{\\widetilde{f(Y)}}\\), (i.e., we are assuming that central limit theorem holds at least approximately for \\(\\widetilde{f(Y)}\\)), then we can produce a confidence interval using the following expression: \\[ \\widetilde{f(Y)} \\pm z_{1-\\alpha/2}\\ \\sigma_{\\widetilde{f(Y)}}\\; .\\]\nUgh. Lets break this apart using an example. Let’s say what we are interested in is the sample mean. Then \\(\\widetilde{f(Y)} = \\bar Y\\) is just the sample mean. The quantile \\(z_{1-\\alpha/2}\\) depends on the desired accuracy. The default choice is \\(1-\\alpha=.95\\), so \\(1-\\alpha/2 = .975\\), and looking this up on the normal table \\(z_{.975}\\approx 1.96 \\approx 2\\). Finally, \\(\\sigma_{\\widetilde{f(Y)}}\\) is the standard error of the mean, so if the population standard deviation of \\(Y\\) is \\(\\sigma_Y\\) and the sample size is \\(N\\), then \\(\\sigma_{\\widetilde{f(Y)}} = \\sigma_Y/\\sqrt{N}\\). This gives us the slightly easier to look at: \\[\\bar Y \\pm 1.96 \\sigma_Y/\\sqrt{N} \\approx \\bar Y \\pm 2 \\sigma_Y/\\sqrt{N}\\ ;\\] Note that we are assuming that \\(\\sigma_Y\\) is known here. If we need to estimate it from the data, we need a slightly different formula given later."
  },
  {
    "objectID": "IntroStats/ConfidenceInterval.html#catching-fish-in-our-net",
    "href": "IntroStats/ConfidenceInterval.html#catching-fish-in-our-net",
    "title": "Confidence Intervals",
    "section": "Catching Fish in Our Net",
    "text": "Catching Fish in Our Net\nThere are two interpretations of confidence intervals (c.i.s): classical and Bayesian (although the latter are often called credibility intervals to distinguish them). As the Bayesian interpretation requires fewer assumptions, we will explore it first.\nIn the classical interpretation the c.i., the c.i. is like a net that is cast into the sea. It either will or will not catch the fish (the estimand). On average, the c.i. will catch the fish \\((1-\\alpha)\\)% of the time; this probability comes from the sampling. On a given time, we either will or will not have the fish in the net, but if we through it out many times, we will catch the fish \\((1-\\alpha)\\)% of the time."
  },
  {
    "objectID": "IntroStats/ConfidenceInterval.html#random-points",
    "href": "IntroStats/ConfidenceInterval.html#random-points",
    "title": "Confidence Intervals",
    "section": "Random Points",
    "text": "Random Points\nSelect the number of repetitions (how many times we through the net), the sample size (the size of the net) as well as the parameters of the population.\nYou may need to press the regenerate button to get the graph to have the right symbols.\n\n\n\n\n\n\nNumber of Repetitions:\n\n50\n100\n200\n\n\n\n\n\n\nSample Size:\n\n1\n5\n10\n25\n50\n100\n\n\n\n\n\n\nMean of Y:\n\n\n\n\n\nStandard Deviation of Y:\n\n\n\n\n(Re)Generate\n\n\n\n\n\n\n\n\n\n\n\n\nApproximaly 2/3 of the data points should be within 1 SE of the mean (plotted as circles)\nApproximately 95 percent of the data points should be within 1 SE of the mean (circles and triangles).\nApproximately 5 percent of the data points should be 2 SEs or more away from the mean (plotted at diamonds).\n\nNote that changing the mean and sd of the population only changes the scales on the graph, not the structure of the problem."
  },
  {
    "objectID": "IntroStats/ConfidenceInterval.html#random-intervals",
    "href": "IntroStats/ConfidenceInterval.html#random-intervals",
    "title": "Confidence Intervals",
    "section": "Random Intervals",
    "text": "Random Intervals\nTaking the sample mean and going plus or minus two standard errors produces a confidence interval.\nActually, the two standard error rule is based on looking up the .975 (1-.05/2) point on the normal table. We could put other values in there as well (50%, 75%, 90% and 99% are common choices). This will adjust the length of the slider.\n\n\n\n\n\n\nConfidence\n\n\n\n\n\n\n\n\n\n\nThe number of confidence intervals that don’t overlap the target line should be around \\(\\alpha\\) (the number in the slider) of the total number of intervals.\n\nThis graph and the random points above are based on the same data. For the 95% interval; the data points where the intervals don’t cross correspond to the data points outside of the 95% region."
  },
  {
    "objectID": "IntroStats/ConfidenceInterval.html#interpreting-confidence-intervals",
    "href": "IntroStats/ConfidenceInterval.html#interpreting-confidence-intervals",
    "title": "Confidence Intervals",
    "section": "Interpreting Confidence Intervals",
    "text": "Interpreting Confidence Intervals\nThe number \\(\\alpha\\), most often 95%, is known as the level of the confidence interval. The level is interpreted as a probability, but there are two schools of thought for interpreting it.\n\nClassical Approach\nThe classical statistical paradigm regards the population mean as a fixed but unknown quantity. The true value is either in or not in the interval, we don’t know which.\nRandom variability comes from the sampling, therefore the 95% comes from imagining different worlds in which we repeated the same sampling and analysis over and over again. 95% of the time, our net (the interval) will catch the fish.\nIn the classical paradigm, we can’t say that their is a 95% chance that the fish is in the net, as we can’t express the position of the fish as a probability: only the position of the net.\n\n\nBayesian Approach\nThe Bayesian paradigm makes the position of the fish a random variable. To do that, however, it needs an additional assumption: a probability distribution for the initial position of the fish.\nFor simplicity, assume that all positions of the fish are [equally likely]1. Using that assumption and Bayes’s Theorem, calculate the posterior probability of the fish (after observing the data). The interval that is constructed is called a credibility interval. There is a 95% chance that the fish is in the credibility interval (at least if the model, both the prior assumption and the normal distribution of the data is correct).\n\n\nWhich approach is better\nActually, most people want both interpretations to hold. They want a proceedure that catches the fish 95% of the time and they want the fish to be in the net 95% of the time.\nFortunately, when we use the approximation that all positions of the fish are equally likely, the two intervals are the same. The abbreviation c.i. could stand for either confidence or credibility interval.\nThe Bayesian interpretation relies on an additional assumption, but both intervals rely on assumptions about the distribution of the observed data. In particular, in this example, we are using the normal distribution to calculate the intervals. That means that the distribution must be close enough to normal that by the central limit theorem, it is reasonable to think that the mean is approximately normally distributed.\nBoth c.i.s break down if the there is a problem with the sample. If this was a convenience sample and not a random one, the normal distribution around the population mean might not be at all right. The c.i. only talks about random error and not bias."
  },
  {
    "objectID": "IntroStats/ConfidenceInterval.html#footnotes",
    "href": "IntroStats/ConfidenceInterval.html#footnotes",
    "title": "Confidence Intervals",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe equally likely assumption is actually a bit nonsensical as we probably expect the fish to in the middle of the pond and not out past the orbit of Pluto. However, if we have enough data, the assumption will not play a big role in our estimate.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Teaching Demos",
    "section": "",
    "text": "These are a series of small shiny apps I developed for teaching. Please feel free to use them in your courses or for your personal learning.\nAs learning from my R code is part of the learning, the source for these pages can be found on my github account: github:ralmond/TeachingDemos.\nThe following sets of demos are available.\n\nIntro Stats – Demos for an introductory statistics class, includes an online normal distribution calculator.\n\n\n\nR Intro – Some various talks I’ve given on how to use R (no narration).\n\nCopyright 2026 Russell G. Almond. Permission to reuse and remix granted under the basis of the CC-BY 4.0 license.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "RIntro/RNotebook.html",
    "href": "RIntro/RNotebook.html",
    "title": "R Notebook",
    "section": "",
    "text": "This is an R Markdown Notebook. When you execute code within the notebook, the results appear beneath the code.\nTry executing this chunk by clicking the Run button within the chunk or by placing your cursor inside it and pressing Cmd+Shift+Enter.\n\nplot(cars)\n\n\n\n\n\n\n\n\nAdd a new chunk by clicking the Insert Chunk button on the toolbar or by pressing Cmd+Option+I.\nWhen you save the notebook, an HTML file containing the code and output will be saved alongside it (click the Preview button or press Cmd+Shift+K to preview the HTML file).\nThe preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike Knit, Preview does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.\nLoading packages we need today\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.2.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggplot2)\nlibrary(psych)\n\n\nAttaching package: 'psych'\n\nThe following objects are masked from 'package:ggplot2':\n\n    %+%, alpha\n\n#If you don't already have these packages, please install first\nif (FALSE) {\n  install.packages(\"tidyverse\")\n  install.packages(\"ggplot2\")\n  install.packages(\"psych\")\n}\n\nOpen dataset\n\nd&lt;-as.data.frame(state.x77)\n#What does each variable measure?\n?state.x77\n\nREVIEW for mutate(): Make a new variable by combining two other variables, or transforming a single variable.\nPopulation density = Population/Area\n\nWithDen&lt;-as.data.frame(d%&gt;% mutate(Density = Population/Area))\n#OR\nd$Density&lt;-d$Population/d$Area\n\nAdd Comments\n\n## Covid Cases by State, 2020-04-03.  CDC website.\ncovid.x2020 &lt;- read.csv(\"../data/covidCases.csv\", header=FALSE)\n\nstat.covid &lt;- data.frame(state.x77,Covid=covid.x2020[[2]], region=state.region)\n\nCalculate summary statistics (e.g., Mean, Median, SD, Skewness & Kurtosis statistics) for a variable\n\nmean(d$Income)\n\n[1] 4435.8\n\nmedian(d$Income)\n\n[1] 4519\n\nsd(d$Income)\n\n[1] 614.4699\n\nskew(d$Income)\n\n[1] 0.2046903\n\nkurtosi(d$Income)\n\n[1] 0.2445701\n\ndescribe(d$Income)\n\n   vars  n   mean     sd median trimmed    mad  min  max range skew kurtosis\nX1    1 50 4435.8 614.47   4519 4430.08 581.18 3098 6315  3217  0.2     0.24\n     se\nX1 86.9\n\n\n\ndescribe(stat.covid$Covid)\n\n   vars  n    mean       sd median trimmed     mad min   max range skew\nX1    1 50 4763.66 13090.24   1408  2308.6 1647.91 147 90279 90132 5.64\n   kurtosis      se\nX1    33.56 1851.24\n\nstat.covid %&gt;% mutate(CovidDensity = Covid/Area) -&gt; covid1\npng(\"covidCases.png\")\nhist(covid1$CovidDensity, main=\"Covid Density by State 2020-04-03\",xlab=\"Covid cases per square mile\" )\ndev.off()\n\npng \n  2 \n\n\nCalculate summary statistics separately for groups of cases\n\nWealthy &lt;- d %&gt;% \n  filter(Income &gt; 4435.8)\nNotWealthy &lt;- d %&gt;% \n  filter(Income &lt; 4435.8)\ndescribe(NotWealthy)\n\n           vars  n     mean       sd   median  trimmed      mad     min\nPopulation    1 21  2907.95  2647.22  2341.00  2484.47  2172.01  472.00\nIncome        2 21  3855.19   326.41  3875.00  3873.82   372.13 3098.00\nIlliteracy    3 21     1.46     0.75     1.60     1.43     1.04    0.50\nLife Exp      4 21    70.28     1.35    70.39    70.29     1.53   67.96\nMurder        5 21     8.64     3.99     9.70     8.71     4.89    1.70\nHS Grad       6 21    48.33     8.81    47.40    47.75    10.82   37.80\nFrost         7 21    97.86    50.50    95.00    98.65    51.89   12.00\nArea          8 21 66375.00 55741.94 50708.00 56932.94 29337.69 9027.00\nDensity       9 21     0.06     0.03     0.05     0.06     0.05    0.01\n                 max     range  skew kurtosis       se\nPopulation  12237.00  11765.00  1.98     4.56   577.67\nIncome       4347.00   1249.00 -0.37    -0.69    71.23\nIlliteracy      2.80      2.30  0.07    -1.58     0.16\nLife Exp       72.90      4.94 -0.05    -0.97     0.29\nMurder         15.10     13.40 -0.17    -1.38     0.87\nHS Grad        67.30     29.50  0.42    -1.21     1.92\nFrost         174.00    162.00  0.04    -1.28    11.02\nArea       262134.00 253107.00  2.08     4.63 12163.89\nDensity         0.11      0.11 -0.10    -1.42     0.01\n\n\nMake a histogram, adjust the number of bins, and add a normal curve\n\nggplot(data = d) +\n  geom_histogram(mapping = aes(x = Income), binwidth = 100) \n\n\n\n\n\n\n\n#OR\n\nhist(d$Income,breaks = 30,main = \"Income\", freq=FALSE, col=\"gray\", xlab=\"Income\")\ncurve(dnorm(x, mean=mean(d$Income), sd=sd(d$Income)), add=TRUE, col=\"red\")\n\n\n\n\n\n\n\n\nNote that in the call to hist the option freq = FALSE was set so that the histogram would be a on the same density scale as the normal curve (area under all bars adds to 1, not sample size).\nPanel histograms by rows according to a grouping variable - Income in geographically bigger vs smaller state Creating a group variable based on area\n\nmean(d$Area)\n\n[1] 70735.88\n\nd$BigOrSmall&lt;-ifelse(d$Area&gt;=70735.08,\"Big\",\"Small\")\n  #Histogram of population based on geographically big vs small state\nggplot(data = d, mapping = aes(x = Population)) + \n  geom_freqpoly(mapping = aes(colour = BigOrSmall), binwidth = 100)\n\n\n\n\n\n\n\n\nMake boxplots to compare groups of variables - population in geographically bigger or smaller states\n\nggplot(data = d, mapping = aes(x = BigOrSmall, y = Population)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nRemove outliers or groups\n\nNoOutliers&lt;-d\nNoOutliers$Population[NoOutliers$Population &gt; 10000] &lt;-NA\n\nChecking the boxplots again\n\nggplot(data = NoOutliers, mapping = aes(x = BigOrSmall, y = Population)) +\n  geom_boxplot()\n\nWarning: Removed 6 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nSave your output to a picture file. (jpeg is also possible).\n\n#png file\npng(\"boxplot.png\", width = 350, height = 350) \nggplot(data = NoOutliers, mapping = aes(x = BigOrSmall, y = Population)) +\n  geom_boxplot()\n\nWarning: Removed 6 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\ndev.off()\n\npng \n  2 \n\n\n#OR if you would like a PDF export\n\npdf(\"boxplot.pdf\") \nggplot(data = NoOutliers, mapping = aes(x = BigOrSmall, y = Population)) +\n  geom_boxplot()\n\nWarning: Removed 6 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\ndev.off()\n\npng \n  2 \n\n\nWhere did I save this image to?\n\ngetwd()\n\n[1] \"/home/ralmond/ralmond1/Projects/TeachingDemos/RIntro\"\n\nlist.files()\n\n [1] \"boxplot.pdf\"                      \"boxplot.png\"                     \n [3] \"covidCases.png\"                   \"EDAwithGGPlot.qmd\"               \n [5] \"ErrorHandling.qmd\"                \"foo.html\"                        \n [7] \"foo.png\"                          \"index.qmd\"                       \n [9] \"InstallingR.qmd\"                  \"MatrixesAndDataFrames.qmd\"       \n[11] \"R and R Studio Presentation2.qmd\" \"RDataStructures.qmd\"             \n[13] \"RNotebook_files\"                  \"RNotebook.qmd\"                   \n[15] \"RNotebook.rmarkdown\"              \"TidyStrings.qmd\"                 \n[17] \"WorkingWithRData.qmd\"",
    "crumbs": [
      "R Notebook"
    ]
  },
  {
    "objectID": "RIntro/InstallingR.html",
    "href": "RIntro/InstallingR.html",
    "title": "Installing R",
    "section": "",
    "text": "When you finish this lesson, you will be able to 1) Start and Stop R and R Studio 2) Download, install and run the tidyverse package. 3) Get help on R functions.",
    "crumbs": [
      "Installing R"
    ]
  },
  {
    "objectID": "RIntro/InstallingR.html#objectives",
    "href": "RIntro/InstallingR.html#objectives",
    "title": "Installing R",
    "section": "",
    "text": "When you finish this lesson, you will be able to 1) Start and Stop R and R Studio 2) Download, install and run the tidyverse package. 3) Get help on R functions.",
    "crumbs": [
      "Installing R"
    ]
  },
  {
    "objectID": "RIntro/InstallingR.html#what-you-need-to-download",
    "href": "RIntro/InstallingR.html#what-you-need-to-download",
    "title": "Installing R",
    "section": "What you need to Download",
    "text": "What you need to Download\nR is a programming language for statistics. Generally, the way that you will work with R code is you will write scripts—small programs—that do the analysis you want to do. You will also need a development environment which will allow you to edit and run the scripts. I recommend RStudio, which is pretty easy to learn.\nIn general, you will need three things for an analysis job:\n\nR itself. R can be downloaded from https://cloud.r-project.org.\n\n\nIf you use a package manager on your computer, R is likely available there. The most common package managers are homebrew on Mac OS, apt-get on Debian Linux, yum on Red hat Linux, or chocolatey on Windows. You may need to search for ‘cran’ to find the name of the right package. For Debian Linux (including Ubuntu and Pop_OS), it is called r-base.\n\nR Studio development environment. R Studio https://rstudio.com/products/rstudio/download/. The free version is fine for what we are doing.\n\n\nThere are other choices for development environments. I use Emacs and ESS, Emacs Speaks Statistics, but that is mostly because I’ve been using Emacs for 20 years.\n\n\nA number of R packages for specific analyses. These can be downloaded from the Comprehensive R Archive Network, or CRAN. Go to https://cloud.r-project.org and click on the ‘Packages’ tab. We will cover package installation later.\n\nYou may want to bookmark the R-project.org web site, as it has lots of useful information, including links to documentation and of course the CRAN library of packages.\nGo ahead and download and install R and R Studio using the instructions on those pages.\n\nA brief Tour of R Studio\n\n\n\nRStudio with four regions labeled.\n\n\nWhen you open R Studio, the screen is split into four regions. (You can adjust the size of these regions if you like.)\n\nRegion 1 in the script editor. This is where you will do the bulk of your work.\nRegion 2 is the R command line. This is where R Studio communicates with the underlying R program.\nRegion 3 has a number of different purposes. Probably the most useful one early on is the “Environment” tab which has all of the R objects you have created.\nRegion 4 again as a number of the tabs. The most useful early on will be “Help” and “Plots”.\n\n\n\nThe Command Line\nAlthough Region 1 is where we will do most of our work, I’m going to start with Region 2. This the R console. R is an interactive programming language. It prompts you that it is waiting for a command with a &gt;. You can type a command at that prompt and hit return. R will then print the result of the expression. You can try this. Try typing 2+2 and then hit return. R should respond [1] 4.\n\nWhy the [1]. This is because R always works with vectors. This indicates that the answer is a vector and the first element is 4.\n\n\nR is a separate program from R Studio. The console window communicates between the two programs. In fact, if you open a terminal window (or command window on Windows) in your operating system, and type R, you will get a similar command prompt and can interact directly with R without R Studio.\n\n\n\nThe Script Editor\nRegion 1 contains an editor for R scripts. When I’m doing data analysis, I want to keep a record of all of the steps I took in doing the analysis. That is the script. An R script file is just a series of R commands, one R command per line. These are put in a text file (can be edited by many different programs) with an extension of .R (note the capital; important for case-sensitive file systems, like Linux).\nTo generate a new script file in R Studio, go to the file menu and select “New File … &gt; R Script”. This will open a new window in Region 1. I generally save it right away, so that I can give it a name that reflects my purpose.\nGenerally how I work in R is I build up a script for my analysis. In R Studio, I can put my cursor on the line I want run and press the Run button at the top of the script window. This will copy the line to the console and run it. If it didn’t work quite right (which often happens) I edit the line and try again. This way I don’t keep the mistakes around in my script, just the stuff that worked.\nSometimes I type things directly in the console. These are usually things I just want to try at the moment to see how they work, or maybe to get more information. For example, I might type names(cars) to get information about the variables in the data set cars or maybe help(var) to remind myself of how the command var works.\n\nI said that there was one command per line, but there are a couple of exceptions. First, you can put two commands per line if you separate them with a semicolon (;). Second, if R doesn’t think the command is complete on one line, it will look for the rest on the next line. I seldom use the semicolon to put two lines together, but I often need to split long lines when writing complex code.\n\n\nThe key to successfully splitting a line is letting R know that there is more to come. Consider the following example.\n\n1 +\n  2\n\n[1] 3\n\n\nPutting the plus sign at the end of the line tells R that there is more to come. So R interprets this as one expression 1+2. If I put the plus sign on the second line instead, R would interpret this as two expressions: 1 and +2.\n\n\nIf R thinks there is still more to come, it will prompt with a + instead of a &gt;. Try this. Type (1+2 and then return at the R command prompt. R will prompt you with + because the expression is not complete. Type ) to finish the expression. This is a fairly common mistake to make; if R is unexpectedly prompting you with its continuation prompt, it usually means you forgot a closing quotation mark or quote.\n\nYou can add comments to you R code by using the pound sign (or hash tag), #. When it sees the pound sign, R ignores everything up to the end of the line (unless the pound sign is in a string.)\n\nI use the following convention, which comes from Lisp programming. I use a single pound sign for a comment which comes after the code. This is usually one tab away from the end of the line. I use two pound signs for comments that are in the code. The are aligned with the start of the code line. I use three pound signs for big comments that describe a whole block of code. These are aligned flush left. Finally, I use a whole line of pound signs to separate different parts of a long script file.\n\n\n\nR Markdown\nR Studio introduces a new kind of script file that I find much more useful than the plain R script. An R Markdown (.Rmd) document can be created by selecting `New File … &gt; R Notebook ” from the “File” menu in R Studio.\nAn R Markdown document has three parts. The first part, separated by --- and --- is the YAML header (YAML=Yet Another Markup Language). This contains meta-data about the document, like title, author and date. It also contains instructions to Markdown about how you want to compile the document.\nThe rest of the document alternates between text chunks in the markdown language and code chunks in R. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For the most part, it looks like plain text, but there are some characters that have special meanings. For example, a line that starts with two pound signs starts a new section. If you select “Help &gt; Markdown Quick Reference” you will get a summary of all of the commands. One of the things I like about Markdown is that if you don’t know the markdown syntax, it pretty much looks like plain text, so just about everybody can read it.\nYou can embed an R code chunk like this:\n::: {.cell}\n\n```{.r .cell-code}\nsummary(cars)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00  \n```\n\n\n:::\n:::\nThe code chunk starts and ends with three backquotes. When editing this in R, R puts a little green triangle up in the top right corner of the chunk. Pressing that will run the chunk:\n\n\n\nGreen Triangle location\n\n\nPressing Control-Alt-I will insert a chunk into the document you are currently working on. This is a handy command to use.\n\nAfter the three backquotes to open the code chunk, there is a bit in curly braces. The first thing after the brace is the language used in the chunk. At first, you will almost always use r for R, but there are other languages that R Studio supports as well. Next is an optional name for the chunk; in the example above cars. Then follows a series of instructions to markdown separated by commas. For example, echo=FALSE will suppress printing the code. fig.cap=\"Figure caption.\" will add a caption to a figure.\n\nAnother advantage of using markdown is that it can compile the document into high quality papers or slide shows. When you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. For more details on using R Markdown see http://rmarkdown.rstudio.com.\n\nRmarkdown has been mostly superceded by a new system called Quarto, which uses .qmd files. Quarto is very similar to Rmarkdown, but has extra support for building more complex collections of documents like books and websites. The website https://quarto.org has documentation for Quarto.\n\n\n\nR Environment\nR wouldn’t be very useful if we had to type out data in every time. So R allows us to store data (and functions, and other things) in an environment. An environment maps names to values. To assign a value to a name, use the assignment, &lt;-, operator.\n\nsqrts &lt;- sqrt(1:10)\nGreeting &lt;- \"Holla!\"\nlong_silly_name &lt;- NA\nmean.1.10 &lt;- mean(1:10)\n\nNote that when you run these commands, the names appear in the “Environments” tab (by default in Region 3 on the R Studio window). You can click on the names to see the values. You can also use the names in later expressions instead of the values. The easiest way to see the value referred to by the name is to type the name in the console.\n\nsqrts\n\n [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427\n [9] 3.000000 3.162278\n\nGreeting\n\n[1] \"Holla!\"\n\nlong_silly_name\n\n[1] NA\n\nmean.1.10\n\n[1] 5.5\n\n\nNames in R need to follow certain rules. They need to start with a letter, and then they can’t contain any characters except letters, numbers, underscores _ and periods .. Note that capital and lower case letters are different in R, so Greeting and greeting are different variables.\nAn alternative to the &lt;- assignment operator is the equal sign, =. A single equals, = means assignment and a double equals == is a test if two values are equal. Be careful as these are easy to confuse. You need to use the single equals for assignment when you are calling functions. For example, the expression mean(x,na.rm=TRUE) asks R to calculate the mean of x, while setting the optional argument na.rm to TRUE.\n\nThe inside of a function is a different environment than the global environment. Variables inside a function can have local definitions that aren’t saved in the global environment. If R doesn’t find a variable in the local environment of a function, it will look in the global environment. The global environment is called .GlobalEnv.\n\nThe R function ls() will list all of the names in the current (usually the global) environment. It can be handy if you forgot how you abbreviated some variable name.\nWhen you quit R, it saves the global environment in a file called .RData, usually in your home directory. (Because the file name starts with a dot, it is usually invisible.) Actually, you don’t need to save the environment; R will ask if you want to save at the end of your session. I often don’t. I save all the commands I needed to create all of my variables in my script, and I’d often rather rerun the script to get a clean start.\n\n\nInstalling and Loading Packages\nThe base R distribution comes with somewhere around 2000 commands for analyzing data. You might think that this is enough, but actually one of the best parts of R is that you can easily extend it by writing new functions. These new functions can be bundled up into a package and shared with others. The most common place to share packages is in the “CRAN” archive on https://cloud.r-project.org. There were over 16,000 packages there the last time I visited.\nA lot of the packages are for doing very specialized analyses (e.g., working with spatial data, or sequencing DNA), but some are improvements to make R easier to use. I’m going to recommend one such bundle called tidyverse. Tidyverse is actually not a single package, but rather a meta-package which will load a number of useful packages.\nThe command install.packages() installs packages, that is, it downloads them from the CRAN library to your local computer. The command library() tells R that you want to use that package in this session. You need to run library() every time, but you only need to run install.packages() once.\n\nif (!(\"tidyverse\" %in% row.names(installed.packages()))) {\n  install.packages(\"tidyverse\",repos=\"https://cloud.r-project.org\",dependencies=TRUE)\n}\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.2.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nThere is also a menu function in R Studio for installing packages. Note that sometimes it will prompt you to choose a “mirror”. This is a local machine which has a copy of the archive. All mirrors work, but mirrors in your country (or at least continent) are likely to be faster.\n\nThe command above is a bit more complex than you strictly need. The first line of the script, starting with if checks to see if the package is already installed. (Installing takes a long time, don’t want to do it if we already have it.) The second line asks to install the tidyverse package. The bit saying repos=\"https://cloud.r-project.org\" specifies that R should use the special cloud.r-project.org mirror, which tries to find a local machine that is not too busy. The bit saying dependencies=TRUE indicates that R should install the other packages that tidyverse says it needs.\n\n\nEach package contains an environment which has all of its functions. When you issue the library() command, you add the package environments into your search path. You can see the search path by using the search() command.\n\nsearch()\n\n [1] \".GlobalEnv\"        \"package:lubridate\" \"package:forcats\"  \n [4] \"package:stringr\"   \"package:dplyr\"     \"package:purrr\"    \n [7] \"package:readr\"     \"package:tidyr\"     \"package:tibble\"   \n[10] \"package:ggplot2\"   \"package:tidyverse\" \"package:stats\"    \n[13] \"package:graphics\"  \"package:grDevices\" \"package:utils\"    \n[16] \"package:datasets\"  \"package:methods\"   \"Autoloads\"        \n[19] \"package:base\"     \n\n\nThe list always starts with .GlobalEnv. Most of the things up to package:tidyverse were added by loading tidyverse. The package tools:rstudio is a special package for communicating between R Studio and R. The packages stats, graphics, utils and base are part of the default R distribution.\n\n\n\nGetting Help\nWith several thousand R functions, and over 16,000 packages packages adding more functions, there is no way anybody can remember them all. Add to this that most R functions have optional arguments which change the way the function behaves (e.g., telling mean() to ignore the missing values with the na.rm=TRUE option). There is no way that even the members of the R Core Team can remember them all.\nFortunately, R provides really good help. Every function in base R and almost all of the functions in the packages have a manual page. You can access the manual page in one of two ways. You can use the function help() or the shortcut ?. Try this:\n\n?var\n\nThe manual page will pop up in the 4th region of R Studio. The top of the manual page gives all of the arguments and explanations of what they do. The bottom gives examples of how the function works. You can copy and paste these examples into the console to see how the function works.\nAnother way to access help in R Studio is to search (by the magnifying glass) in the Help tab. Sometimes this will give you a list of commands you can choose from.\nYet another way to search for help is to use the help.search() function. Its argument is a string that you might find in the short description.\n\nhelp.search(\"variance\")\n\nThis will give you a long list. The function names will look like package::function, so you can tell which package the function comes from.\nYet another good trick is to ask Dr. Google (or you favorite search engine). Often you can find bits of other people’s R code which do more or less what you want.\nFinally, if you want to find out what functions are in a package, try library(help=package)\n\nlibrary(help=tibble)\n\nThis pops up a window telling you about help for the package. Note that all R packages have manuals that can be downloaded from the CRAN library, and many have vignettes—examples of how they can be used.\n\n\nMaking Plots\nIf you are using script commands, or you are executing the command in the console window, the commands appear in the Plots tab in Region 4 of R Studio. Try typing hist(faithful$eruptions) in the console window.\nIt might look something like this:\n\n\n\nRStudio with Plot\n\n\nThere are a number of useful buttons up at the top. If you have made a bunch of plots, the left and right arrows allow you to cycle through them. The Zoom button pops up a window with the plot. The Export menu allows you to save the file as a picture or copy it to the pasteboard to put in another document.\nWhen you are using R markdown (i.e., an R Notebook) the plots get also embedded in the document. For example, try pressing the green triangle after this chunk.\n\n\n\n\n\n\n\n\n\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot.\nNote that here you can use the fig.cap option to add a caption to the figure.\n\n\n\n\n\nVapor Pressure of Mercury as a Function of Temperature\n\n\n\n\nThe caption doesn’t show in the notebook mode, but it will if you use knit to make a document.\n\n\nCiting R\nIf you find R useful, you should give credit to the developers. To find out how to properly cite R use the citation command:\n\ncitation()\n\nTo cite R in publications use:\n\n  R Core Team (2025). _R: A Language and Environment for Statistical\n  Computing_. R Foundation for Statistical Computing, Vienna, Austria.\n  &lt;https://www.R-project.org/&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2025},\n    url = {https://www.R-project.org/},\n  }\n\nWe have invested a lot of time and effort in creating R, please cite it\nwhen using it for data analysis. See also 'citation(\"pkgname\")' for\nciting R packages.\n\n\nTo cite a package, check citation(\"packagename\").\n\ncitation(\"tidyverse\")\n\nTo cite package 'tidyverse' in publications use:\n\n  Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R,\n  Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller\n  E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V,\n  Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). \"Welcome to\n  the tidyverse.\" _Journal of Open Source Software_, *4*(43), 1686.\n  doi:10.21105/joss.01686 &lt;https://doi.org/10.21105/joss.01686&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Article{,\n    title = {Welcome to the {tidyverse}},\n    author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},\n    year = {2019},\n    journal = {Journal of Open Source Software},\n    volume = {4},\n    number = {43},\n    pages = {1686},\n    doi = {10.21105/joss.01686},\n  }\n\n\nFinally, R and R Studio are two separate programs, so deserve a separate citations. Use the help menu (“Help &gt; RStudio Docs”) for more information.\n\n\nNext Lesson\nCongrats. You should now know how to install and launch R and RStudio. The next step is learn to work with R data.\nNext Lesson",
    "crumbs": [
      "Installing R"
    ]
  },
  {
    "objectID": "RIntro/InstallingR.html#a-brief-tour-of-r-studio",
    "href": "RIntro/InstallingR.html#a-brief-tour-of-r-studio",
    "title": "Installing R",
    "section": "A brief Tour of R Studio",
    "text": "A brief Tour of R Studio\n\n\n\nRStudio with four regions labeled.\n\n\nWhen you open R Studio, the screen is split into four regions. (You can adjust the size of these regions if you like.)\n\nRegion 1 in the script editor. This is where you will do the bulk of your work.\nRegion 2 is the R command line. This is where R Studio communicates with the underlying R program.\nRegion 3 has a number of different purposes. Probably the most useful one early on is the “Environment” tab which has all of the R objects you have created.\nRegion 4 again as a number of the tabs. The most useful early on will be “Help” and “Plots”.",
    "crumbs": [
      "Installing R"
    ]
  },
  {
    "objectID": "RIntro/InstallingR.html#the-command-line",
    "href": "RIntro/InstallingR.html#the-command-line",
    "title": "Installing R",
    "section": "The Command Line",
    "text": "The Command Line\nAlthough Region 1 is where we will do most of our work, I’m going to start with Region 2. This the R console. R is an interactive programming language. It prompts you that it is waiting for a command with a &gt;. You can type a command at that prompt and hit return. R will then print the result of the expression. You can try this. Try typing 2+2 and then hit return. R should respond [1] 4.\n\nWhy the [1]. This is because R always works with vectors. This indicates that the answer is a vector and the first element is 4.\n\n\nR is a separate program from R Studio. The console window communicates between the two programs. In fact, if you open a terminal window (or command window on Windows) in your operating system, and type R, you will get a similar command prompt and can interact directly with R without R Studio.",
    "crumbs": [
      "Installing R"
    ]
  },
  {
    "objectID": "RIntro/InstallingR.html#the-script-editor",
    "href": "RIntro/InstallingR.html#the-script-editor",
    "title": "Installing R",
    "section": "The Script Editor",
    "text": "The Script Editor\nRegion 1 contains an editor for R scripts. When I’m doing data analysis, I want to keep a record of all of the steps I took in doing the analysis. That is the script. An R script file is just a series of R commands, one R command per line. These are put in a text file (can be edited by many different programs) with an extension of .R (note the capital; important for case-sensitive file systems, like Linux).\nTo generate a new script file in R Studio, go to the file menu and select “New File … &gt; R Script”. This will open a new window in Region 1. I generally save it right away, so that I can give it a name that reflects my purpose.\nGenerally how I work in R is I build up a script for my analysis. In R Studio, I can put my cursor on the line I want run and press the Run button at the top of the script window. This will copy the line to the console and run it. If it didn’t work quite right (which often happens) I edit the line and try again. This way I don’t keep the mistakes around in my script, just the stuff that worked.\nSometimes I type things directly in the console. These are usually things I just want to try at the moment to see how they work, or maybe to get more information. For example, I might type names(cars) to get information about the variables in the data set cars or maybe help(var) to remind myself of how the command var works.\n\nI said that there was one command per line, but there are a couple of exceptions. First, you can put two commands per line if you separate them with a semicolon (;). Second, if R doesn’t think the command is complete on one line, it will look for the rest on the next line. I seldom use the semicolon to put two lines together, but I often need to split long lines when writing complex code.\n\n\nThe key to successfully splitting a line is letting R know that there is more to come. Consider the following example.\n\n1 +\n  2\n\n[1] 3\n\n\nPutting the plus sign at the end of the line tells R that there is more to come. So R interprets this as one expression 1+2. If I put the plus sign on the second line instead, R would interpret this as two expressions: 1 and +2.\n\n\nIf R thinks there is still more to come, it will prompt with a + instead of a &gt;. Try this. Type (1+2 and then return at the R command prompt. R will prompt you with + because the expression is not complete. Type ) to finish the expression. This is a fairly common mistake to make; if R is unexpectedly prompting you with its continuation prompt, it usually means you forgot a closing quotation mark or quote.\n\nYou can add comments to you R code by using the pound sign (or hash tag), #. When it sees the pound sign, R ignores everything up to the end of the line (unless the pound sign is in a string.)\n\nI use the following convention, which comes from Lisp programming. I use a single pound sign for a comment which comes after the code. This is usually one tab away from the end of the line. I use two pound signs for comments that are in the code. The are aligned with the start of the code line. I use three pound signs for big comments that describe a whole block of code. These are aligned flush left. Finally, I use a whole line of pound signs to separate different parts of a long script file.",
    "crumbs": [
      "Installing R"
    ]
  },
  {
    "objectID": "RIntro/InstallingR.html#r-markdown",
    "href": "RIntro/InstallingR.html#r-markdown",
    "title": "Installing R",
    "section": "R Markdown",
    "text": "R Markdown\nR Studio introduces a new kind of script file that I find much more useful than the plain R script. An R Markdown (.Rmd) document can be created by selecting `New File … &gt; R Notebook ” from the “File” menu in R Studio.\nAn R Markdown document has three parts. The first part, separated by --- and --- is the YAML header (YAML=Yet Another Markup Language). This contains meta-data about the document, like title, author and date. It also contains instructions to Markdown about how you want to compile the document.\nThe rest of the document alternates between text chunks in the markdown language and code chunks in R. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For the most part, it looks like plain text, but there are some characters that have special meanings. For example, a line that starts with two pound signs starts a new section. If you select “Help &gt; Markdown Quick Reference” you will get a summary of all of the commands. One of the things I like about Markdown is that if you don’t know the markdown syntax, it pretty much looks like plain text, so just about everybody can read it.\nYou can embed an R code chunk like this:\n::: {.cell}\n\n```{.r .cell-code}\nsummary(cars)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00  \n```\n\n\n:::\n:::\nThe code chunk starts and ends with three backquotes. When editing this in R, R puts a little green triangle up in the top right corner of the chunk. Pressing that will run the chunk:\n\n\n\nGreen Triangle location\n\n\nPressing Control-Alt-I will insert a chunk into the document you are currently working on. This is a handy command to use.\n\nAfter the three backquotes to open the code chunk, there is a bit in curly braces. The first thing after the brace is the language used in the chunk. At first, you will almost always use r for R, but there are other languages that R Studio supports as well. Next is an optional name for the chunk; in the example above cars. Then follows a series of instructions to markdown separated by commas. For example, echo=FALSE will suppress printing the code. fig.cap=\"Figure caption.\" will add a caption to a figure.\n\nAnother advantage of using markdown is that it can compile the document into high quality papers or slide shows. When you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. For more details on using R Markdown see http://rmarkdown.rstudio.com.\n\nRmarkdown has been mostly superceded by a new system called Quarto, which uses .qmd files. Quarto is very similar to Rmarkdown, but has extra support for building more complex collections of documents like books and websites. The website https://quarto.org has documentation for Quarto.",
    "crumbs": [
      "Installing R"
    ]
  },
  {
    "objectID": "RIntro/InstallingR.html#r-environment",
    "href": "RIntro/InstallingR.html#r-environment",
    "title": "Installing R",
    "section": "R Environment",
    "text": "R Environment\nR wouldn’t be very useful if we had to type out data in every time. So R allows us to store data (and functions, and other things) in an environment. An environment maps names to values. To assign a value to a name, use the assignment, &lt;-, operator.\n\nsqrts &lt;- sqrt(1:10)\nGreeting &lt;- \"Holla!\"\nlong_silly_name &lt;- NA\nmean.1.10 &lt;- mean(1:10)\n\nNote that when you run these commands, the names appear in the “Environments” tab (by default in Region 3 on the R Studio window). You can click on the names to see the values. You can also use the names in later expressions instead of the values. The easiest way to see the value referred to by the name is to type the name in the console.\n\nsqrts\n\n [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427\n [9] 3.000000 3.162278\n\nGreeting\n\n[1] \"Holla!\"\n\nlong_silly_name\n\n[1] NA\n\nmean.1.10\n\n[1] 5.5\n\n\nNames in R need to follow certain rules. They need to start with a letter, and then they can’t contain any characters except letters, numbers, underscores _ and periods .. Note that capital and lower case letters are different in R, so Greeting and greeting are different variables.\nAn alternative to the &lt;- assignment operator is the equal sign, =. A single equals, = means assignment and a double equals == is a test if two values are equal. Be careful as these are easy to confuse. You need to use the single equals for assignment when you are calling functions. For example, the expression mean(x,na.rm=TRUE) asks R to calculate the mean of x, while setting the optional argument na.rm to TRUE.\n\nThe inside of a function is a different environment than the global environment. Variables inside a function can have local definitions that aren’t saved in the global environment. If R doesn’t find a variable in the local environment of a function, it will look in the global environment. The global environment is called .GlobalEnv.\n\nThe R function ls() will list all of the names in the current (usually the global) environment. It can be handy if you forgot how you abbreviated some variable name.\nWhen you quit R, it saves the global environment in a file called .RData, usually in your home directory. (Because the file name starts with a dot, it is usually invisible.) Actually, you don’t need to save the environment; R will ask if you want to save at the end of your session. I often don’t. I save all the commands I needed to create all of my variables in my script, and I’d often rather rerun the script to get a clean start.",
    "crumbs": [
      "Installing R"
    ]
  },
  {
    "objectID": "RIntro/InstallingR.html#installing-and-loading-packages",
    "href": "RIntro/InstallingR.html#installing-and-loading-packages",
    "title": "Installing R",
    "section": "Installing and Loading Packages",
    "text": "Installing and Loading Packages\nThe base R distribution comes with somewhere around 2000 commands for analyzing data. You might think that this is enough, but actually one of the best parts of R is that you can easily extend it by writing new functions. These new functions can be bundled up into a package and shared with others. The most common place to share packages is in the “CRAN” archive on https://cloud.r-project.org. There were over 16,000 packages there the last time I visited.\nA lot of the packages are for doing very specialized analyses (e.g., working with spatial data, or sequencing DNA), but some are improvements to make R easier to use. I’m going to recommend one such bundle called tidyverse. Tidyverse is actually not a single package, but rather a meta-package which will load a number of useful packages.\nThe command install.packages() installs packages, that is, it downloads them from the CRAN library to your local computer. The command library() tells R that you want to use that package in this session. You need to run library() every time, but you only need to run install.packages() once.\n\nif (!(\"tidyverse\" %in% row.names(installed.packages()))) {\n  install.packages(\"tidyverse\",repos=\"https://cloud.r-project.org\",dependencies=TRUE)\n}\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.2.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nThere is also a menu function in R Studio for installing packages. Note that sometimes it will prompt you to choose a “mirror”. This is a local machine which has a copy of the archive. All mirrors work, but mirrors in your country (or at least continent) are likely to be faster.\n\nThe command above is a bit more complex than you strictly need. The first line of the script, starting with if checks to see if the package is already installed. (Installing takes a long time, don’t want to do it if we already have it.) The second line asks to install the tidyverse package. The bit saying repos=\"https://cloud.r-project.org\" specifies that R should use the special cloud.r-project.org mirror, which tries to find a local machine that is not too busy. The bit saying dependencies=TRUE indicates that R should install the other packages that tidyverse says it needs.\n\n\nEach package contains an environment which has all of its functions. When you issue the library() command, you add the package environments into your search path. You can see the search path by using the search() command.\n\nsearch()\n\n [1] \".GlobalEnv\"        \"package:lubridate\" \"package:forcats\"  \n [4] \"package:stringr\"   \"package:dplyr\"     \"package:purrr\"    \n [7] \"package:readr\"     \"package:tidyr\"     \"package:tibble\"   \n[10] \"package:ggplot2\"   \"package:tidyverse\" \"package:stats\"    \n[13] \"package:graphics\"  \"package:grDevices\" \"package:utils\"    \n[16] \"package:datasets\"  \"package:methods\"   \"Autoloads\"        \n[19] \"package:base\"     \n\n\nThe list always starts with .GlobalEnv. Most of the things up to package:tidyverse were added by loading tidyverse. The package tools:rstudio is a special package for communicating between R Studio and R. The packages stats, graphics, utils and base are part of the default R distribution.",
    "crumbs": [
      "Installing R"
    ]
  },
  {
    "objectID": "RIntro/InstallingR.html#getting-help",
    "href": "RIntro/InstallingR.html#getting-help",
    "title": "Installing R",
    "section": "Getting Help",
    "text": "Getting Help\nWith several thousand R functions, and over 16,000 packages packages adding more functions, there is no way anybody can remember them all. Add to this that most R functions have optional arguments which change the way the function behaves (e.g., telling mean() to ignore the missing values with the na.rm=TRUE option). There is no way that even the members of the R Core Team can remember them all.\nFortunately, R provides really good help. Every function in base R and almost all of the functions in the packages have a manual page. You can access the manual page in one of two ways. You can use the function help() or the shortcut ?. Try this:\n\n?var\n\nThe manual page will pop up in the 4th region of R Studio. The top of the manual page gives all of the arguments and explanations of what they do. The bottom gives examples of how the function works. You can copy and paste these examples into the console to see how the function works.\nAnother way to access help in R Studio is to search (by the magnifying glass) in the Help tab. Sometimes this will give you a list of commands you can choose from.\nYet another way to search for help is to use the help.search() function. Its argument is a string that you might find in the short description.\n\nhelp.search(\"variance\")\n\nThis will give you a long list. The function names will look like package::function, so you can tell which package the function comes from.\nYet another good trick is to ask Dr. Google (or you favorite search engine). Often you can find bits of other people’s R code which do more or less what you want.\nFinally, if you want to find out what functions are in a package, try library(help=package)\n\nlibrary(help=tibble)\n\nThis pops up a window telling you about help for the package. Note that all R packages have manuals that can be downloaded from the CRAN library, and many have vignettes—examples of how they can be used.",
    "crumbs": [
      "Installing R"
    ]
  },
  {
    "objectID": "RIntro/InstallingR.html#making-plots",
    "href": "RIntro/InstallingR.html#making-plots",
    "title": "Installing R",
    "section": "Making Plots",
    "text": "Making Plots\nIf you are using script commands, or you are executing the command in the console window, the commands appear in the Plots tab in Region 4 of R Studio. Try typing hist(faithful$eruptions) in the console window.\nIt might look something like this:\n\n\n\nRStudio with Plot\n\n\nThere are a number of useful buttons up at the top. If you have made a bunch of plots, the left and right arrows allow you to cycle through them. The Zoom button pops up a window with the plot. The Export menu allows you to save the file as a picture or copy it to the pasteboard to put in another document.\nWhen you are using R markdown (i.e., an R Notebook) the plots get also embedded in the document. For example, try pressing the green triangle after this chunk.\n\n\n\n\n\n\n\n\n\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot.\nNote that here you can use the fig.cap option to add a caption to the figure.\n\n\n\n\n\nVapor Pressure of Mercury as a Function of Temperature\n\n\n\n\nThe caption doesn’t show in the notebook mode, but it will if you use knit to make a document.",
    "crumbs": [
      "Installing R"
    ]
  },
  {
    "objectID": "RIntro/InstallingR.html#citing-r",
    "href": "RIntro/InstallingR.html#citing-r",
    "title": "Installing R",
    "section": "Citing R",
    "text": "Citing R\nIf you find R useful, you should give credit to the developers. To find out how to properly cite R use the citation command:\n\ncitation()\n\nTo cite R in publications use:\n\n  R Core Team (2025). _R: A Language and Environment for Statistical\n  Computing_. R Foundation for Statistical Computing, Vienna, Austria.\n  &lt;https://www.R-project.org/&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2025},\n    url = {https://www.R-project.org/},\n  }\n\nWe have invested a lot of time and effort in creating R, please cite it\nwhen using it for data analysis. See also 'citation(\"pkgname\")' for\nciting R packages.\n\n\nTo cite a package, check citation(\"packagename\").\n\ncitation(\"tidyverse\")\n\nTo cite package 'tidyverse' in publications use:\n\n  Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R,\n  Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller\n  E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V,\n  Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). \"Welcome to\n  the tidyverse.\" _Journal of Open Source Software_, *4*(43), 1686.\n  doi:10.21105/joss.01686 &lt;https://doi.org/10.21105/joss.01686&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Article{,\n    title = {Welcome to the {tidyverse}},\n    author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},\n    year = {2019},\n    journal = {Journal of Open Source Software},\n    volume = {4},\n    number = {43},\n    pages = {1686},\n    doi = {10.21105/joss.01686},\n  }\n\n\nFinally, R and R Studio are two separate programs, so deserve a separate citations. Use the help menu (“Help &gt; RStudio Docs”) for more information.",
    "crumbs": [
      "Installing R"
    ]
  },
  {
    "objectID": "RIntro/InstallingR.html#next-lesson",
    "href": "RIntro/InstallingR.html#next-lesson",
    "title": "Installing R",
    "section": "Next Lesson",
    "text": "Next Lesson\nCongrats. You should now know how to install and launch R and RStudio. The next step is learn to work with R data.\nNext Lesson",
    "crumbs": [
      "Installing R"
    ]
  },
  {
    "objectID": "RIntro/ErrorHandling.html",
    "href": "RIntro/ErrorHandling.html",
    "title": "Error Handling in R",
    "section": "",
    "text": "Let’s start by making an error.\n\nplot(c00rs)\n\n\n\nError in eval(expr, envir) : object 'c00rs' not found\n\n\n\nplotit &lt;- function (x)\n  plot(x)\n\n\nplotit(c00rs)\n\n\n\nError in eval(expr, envir) : object 'c00rs' not found\n\n\nNote that this function doesn’t make sense if the input is negative.\n\nscaleme &lt;- function (x, sf) {\n  x/sqrt(sf)\n}\nscaleme(1:10,-3)\n\nWarning in sqrt(sf): NaNs produced\n\n\n [1] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n\n\nThis produces a NaN. That might be a problem as we might not notice that there is a problem until several calculations down the line.\nCan check for negative scale factor and provide a more informative error message.\n\nscaleme &lt;- function (x, sf) {\n  if (!is.numeric(sf) || sf &lt;= 0) \n    stop(\"Expected sf to be a positive number, got \", sf)\n  x/sqrt(sf)\n}\n\n\nscaleme(1:10,-3)\n\n\n\nError in scaleme(1:10, -3) : Expected sf to be a positive number, got -3\n\n\nOr maybe, just issue a warning and let the NaN go through\n\nscaleme &lt;- function (x, sf) {\n  if (!is.numeric(sf) || sf &lt;= 0) \n    warning(\"Expected sf to be a positive number, got \", sf,\"\\n\")\n  x/sqrt(sf)\n}\nscaleme(1:10,-3)\n\nWarning in scaleme(1:10, -3): Expected sf to be a positive number, got -3\n\n\nWarning in sqrt(sf): NaNs produced\n\n\n [1] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN",
    "crumbs": [
      "Error Handling in R"
    ]
  },
  {
    "objectID": "RIntro/ErrorHandling.html#some-simple-examples",
    "href": "RIntro/ErrorHandling.html#some-simple-examples",
    "title": "Error Handling in R",
    "section": "",
    "text": "Let’s start by making an error.\n\nplot(c00rs)\n\n\n\nError in eval(expr, envir) : object 'c00rs' not found\n\n\n\nplotit &lt;- function (x)\n  plot(x)\n\n\nplotit(c00rs)\n\n\n\nError in eval(expr, envir) : object 'c00rs' not found\n\n\nNote that this function doesn’t make sense if the input is negative.\n\nscaleme &lt;- function (x, sf) {\n  x/sqrt(sf)\n}\nscaleme(1:10,-3)\n\nWarning in sqrt(sf): NaNs produced\n\n\n [1] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n\n\nThis produces a NaN. That might be a problem as we might not notice that there is a problem until several calculations down the line.\nCan check for negative scale factor and provide a more informative error message.\n\nscaleme &lt;- function (x, sf) {\n  if (!is.numeric(sf) || sf &lt;= 0) \n    stop(\"Expected sf to be a positive number, got \", sf)\n  x/sqrt(sf)\n}\n\n\nscaleme(1:10,-3)\n\n\n\nError in scaleme(1:10, -3) : Expected sf to be a positive number, got -3\n\n\nOr maybe, just issue a warning and let the NaN go through\n\nscaleme &lt;- function (x, sf) {\n  if (!is.numeric(sf) || sf &lt;= 0) \n    warning(\"Expected sf to be a positive number, got \", sf,\"\\n\")\n  x/sqrt(sf)\n}\nscaleme(1:10,-3)\n\nWarning in scaleme(1:10, -3): Expected sf to be a positive number, got -3\n\n\nWarning in sqrt(sf): NaNs produced\n\n\n [1] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN",
    "crumbs": [
      "Error Handling in R"
    ]
  },
  {
    "objectID": "RIntro/ErrorHandling.html#r-options-for-handling-warnings.",
    "href": "RIntro/ErrorHandling.html#r-options-for-handling-warnings.",
    "title": "Error Handling in R",
    "section": "R Options for handling warnings.",
    "text": "R Options for handling warnings.\nSetting - options(warn=0) – Print warnings at end of function. - options(warn=1) – Print warnings as they happen - options(warn=2) – Stop on warnings\n\noptions(warn=1)\nscaleme(1:10,-3)\n\nWarning in scaleme(1:10, -3): Expected sf to be a positive number, got -3\n\n\nWarning in sqrt(sf): NaNs produced\n\n\n [1] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n\n\n\noptions(warn=2)\nscaleme(1:10,-3)\n\nAdding a call to recover() where the error occurs will allow the programmer to look around at what is going on when the error occurs.\n\nscaleme &lt;- function (x, sf) {\n  if (!is.numeric(sf) || sf &lt;= 0) {\n    warning(\"Expected sf to be a positive number, got \", sf,\"\\n\")\n    recover()\n  }\n  x/sqrt(sf)\n}\nscaleme(1:10,-3)",
    "crumbs": [
      "Error Handling in R"
    ]
  },
  {
    "objectID": "RIntro/R and R Studio Presentation2.html",
    "href": "RIntro/R and R Studio Presentation2.html",
    "title": "R and RStudio",
    "section": "",
    "text": "R is a language and environment for statistical computing and graphics. {data-background=Rlogo.png data-background-size=250px data-background-position=right}\n\n\n\ndata manipulation and calculations\ngraphical displays\nstatistical techniques\n\n\n\n\n\nfor UNIX platforms, Windows and MacOS\nfor download from (https://cloud.r-project.org/)\nCurrent version: R 3.5.2",
    "crumbs": [
      "R and RStudio"
    ]
  },
  {
    "objectID": "RIntro/R and R Studio Presentation2.html#rstudio-desktop-can-be-downloaded-from",
    "href": "RIntro/R and R Studio Presentation2.html#rstudio-desktop-can-be-downloaded-from",
    "title": "R and RStudio",
    "section": "RStudio Desktop can be downloaded from",
    "text": "RStudio Desktop can be downloaded from\n\n(https://www.rstudio.com/products/rstudio/download/)\nYou can use R with any editor, including RStudio, to write scripts. However, RStudio itself is not very useful without R.\n\n\nAdditional IDE features of RStudio can be located here\n\n(https://www.rstudio.com/products/rstudio/features/)\n\nRStudio Resources\n\n\nWebinars\n\n(https://resources.rstudio.com/webinars)\n\n\n\nCheat Sheets\n\n(https://www.rstudio.com/resources/cheatsheets/#ide)",
    "crumbs": [
      "R and RStudio"
    ]
  },
  {
    "objectID": "RIntro/index.html",
    "href": "RIntro/index.html",
    "title": "Index of R Demonstrations for R Group",
    "section": "",
    "text": "Florida State University\nThese are demonstrations which were written for the FSU R user group. Many are related to introductory topics in R.",
    "crumbs": [
      "Home",
      "Intro to R"
    ]
  },
  {
    "objectID": "RIntro/index.html#index-of-r-demonstrations",
    "href": "RIntro/index.html#index-of-r-demonstrations",
    "title": "Index of R Demonstrations for R Group",
    "section": "Index of R Demonstrations",
    "text": "Index of R Demonstrations\n\n\n\n\nEDAwithGGPlot.qmd\n\n\nErrorHandling.qmd\n\n\nindex.qmd\n\n\nInstallingR.qmd\n\n\nMatrixesAndDataFrames.qmd\n\n\nR and R Studio Presentation2.qmd\n\n\nRDataStructures.qmd\n\n\nRNotebook.qmd\n\n\nTidyStrings.qmd\n\n\nWorkingWithRData.qmd\n\n\n\n\n These are licensed under the creative commons CC BY license. You many distribute, remix, adapt, and build upon the material in any medium or format, so long as attribution is given to the creator.\nThe Source files for these demonstrations can be found at https://github.com/ralmond/TeachingDemos/RIntro.",
    "crumbs": [
      "Home",
      "Intro to R"
    ]
  }
]