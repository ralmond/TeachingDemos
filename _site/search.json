[
  {
    "objectID": "RIntro/MatrixesAndDataFrames.html",
    "href": "RIntro/MatrixesAndDataFrames.html",
    "title": "Arrays, Matrixes and Data Frames",
    "section": "",
    "text": "Vector – ordered collection of objects of the same storage mode ([ extract)\n\nNamed Vector – adds a names attribute (Can use names in subscripts)\nMatrix, Array – adds a dim and dimnames attribute\n\nList – ordered collection of objects of any type or mode ([[ extract)\n\nNamed List – add names attribute (Can use $ to extract elements)\nS3 Class – adds a class attribute\ndata.frame – a list of columns in a spreadsheet. Uses ([ extract).\n\nS4 Class – formal class mechanism. Uses @ instead of $.\n\n\n\nThe mode funciton in R refers to storage modes, not the mode of a distribution.\n\nmode(123)\n\n[1] \"numeric\"\n\nmode(123L)\n\n[1] \"numeric\"\n\nmode(TRUE)\n\n[1] \"logical\"\n\nmode(\"True\")\n\n[1] \"character\"\n\nmode(3.14)\n\n[1] \"numeric\"\n\nmode(t)\n\n[1] \"function\"\n\n?mode\n\nThe as.XXX and is.XXX functions can be used to convert between different types.\n\nis.integer(3)\n\n[1] FALSE\n\nis.integer(3L)\n\n[1] TRUE\n\nas.integer(3)\n\n[1] 3\n\nis.integer(as.integer(3))\n\n[1] TRUE\n\nas.integer(\"three\")\n\nWarning: NAs introduced by coercion\n\n\n[1] NA\n\nas.character(3)\n\n[1] \"3\"\n\nas.logical(3)\n\n[1] TRUE\n\n\n\n\n\nAll R objects are vectors: scalars in R are vectors of length 1.\n\ncat(\"The output will start with '[1]' to show that this is a vector.\\n\")\n\nThe output will start with '[1]' to show that this is a vector.\n\n3.14159  \n\n[1] 3.14159\n\n\nR implicitly loops over all the elements of a vector. Such implicit loops are faster than explicit for loops.\n\n1:11\n\n [1]  1  2  3  4  5  6  7  8  9 10 11\n\nmean(1:11)\n\n[1] 6\n\ny &lt;- (1:11 - mean(1:11))/sd(1:11)\nmean (y)\n\n[1] 0\n\nsd(y)\n\n[1] 1\n\n\n\n\nThe : operator produces sequences (of integers) between first and second argument.\n\n1:3\n\n[1] 1 2 3\n\n3:1\n\n[1] 3 2 1\n\n-1:1\n\n[1] -1  0  1\n\n-3:-1\n\n[1] -3 -2 -1\n\n\nThe c() function can be used to glue vectors together. (c stands for combine.)\n\nc(1:3, 10:12)\n\n[1]  1  2  3 10 11 12\n\nc(\"Haenzel\", \"Greatel\", \"Tedd\",\"Alice\")\n\n[1] \"Haenzel\" \"Greatel\" \"Tedd\"    \"Alice\"  \n\n\n\n\n\nR has a number of built in random number generators to generate random numbers. The most commonly used are runif, rnorm and sample. There are also many others, with names that look like rXXX (try substituting chisq, t, beta, gamma, &c for XXX).\n\nrunif(5)\n\n[1] 0.3114001 0.1532532 0.8512517 0.8840553 0.7097423\n\nrnorm(10)\n\n [1]  0.9884986 -0.0990681  0.1441367  0.4474792  1.7879468  1.4087663\n [7]  0.2188738 -0.6119803  1.3135611  1.5488097\n\nsample.int(5,5,replace=TRUE)\n\n[1] 3 3 1 4 4\n\n\n\n\n\n\nGenerate 100 random numbers with mean 50 and standard deviation 25.\n\n1a. Use the result of the previous question to generate a random sample of size 101 with one outlier of 200.\n\nGenerate random integers between 0 and 100\n\n\nThe variable state.area contains the areas of the 50 US states (in alphabetical area). Create a random sample of size 10 of the state areas.\n\n\n\n\n\nThe [] operator is used to subscript vectors. There are three different things you can put inside of the brackets: numbers, logical expressions and names (character values).\n\n\nNumbers are the most straightforward way to do indexing. R starts the indexes at 1 and it goes up to the length of the vector. The function `length() is useful in writing indexes. Giving multiple indexes with return a sub-vector (remember, there are not scalars in R, just vectors of length 1).\n\nint10 &lt;- 1:10\nint10[3]\n\n[1] 3\n\nint10[c(5:7,9)]\n\n[1] 5 6 7 9\n\nstate.area[c(1,length(state.area))]\n\n[1] 51609 97914\n\n\nAnother useful trick is to use negative indexes. These leave the numbered varaibles out.\n\nint10[-2]\n\n[1]  1  3  4  5  6  7  8  9 10\n\nint10[-(3:8)]\n\n[1]  1  2  9 10\n\n\nIndexing expressions can also be used on the LHS of assignment operators, to allow to assignment to just certain values.\n\nint10[3] &lt;- -3\nint10\n\n [1]  1  2 -3  4  5  6  7  8  9 10\n\n\n\n\n\nThe second option for indexing is to use a logical vector the same length as the vector you are indexing.\n\nint10&lt;0\n\n [1] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\nint10[int10&lt;0]\n\n[1] -3\n\nint10[int10&lt;0] &lt;- abs (int10[int10&lt;0])\nint10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nBe careful with NAs.\n\nint55 &lt;- -5:5\nsqrt(int55) &lt; 1.2\n\nWarning in sqrt(int55): NaNs produced\n\n\n [1]    NA    NA    NA    NA    NA  TRUE  TRUE FALSE FALSE FALSE FALSE\n\nint55[sqrt(int55) &lt; 1.2]\n\nWarning in sqrt(int55): NaNs produced\n\n\n[1] NA NA NA NA NA  0  1\n\n\nThe real power of logical indexes comes when we have two vectors of the same length. For example, state.abb gives the two letter postal codes of the states. Suppose we wanted to see all of the states that are bigger than average:\n\nstate.abb[state.area&gt;median(state.area)]\n\n [1] \"AK\" \"AZ\" \"CA\" \"CO\" \"FL\" \"GA\" \"ID\" \"IL\" \"IA\" \"KS\" \"MI\" \"MN\" \"MO\" \"MT\" \"NE\"\n[16] \"NV\" \"NM\" \"ND\" \"OK\" \"OR\" \"SD\" \"TX\" \"UT\" \"WA\" \"WY\"\n\n\n\n\n\nThe built in langauge primitive if is not vectorized. It is expecting a single value. The code below will not do what you think it will.\n\ntry({\nif (int55 &lt; 0) {\n  cat(\"Negative.\\n\")\n} else { \n  cat(\"Non-negative.\\n\")\n}\n})\n\nError in if (int55 &lt; 0) { : the condition has length &gt; 1\n\n\nThe functions any(), all() and isTRUE() are often useful here.\n\nif (all(int55 &gt;0)) {\n  cat(\"Positive.\\n\")\n} else { \n  cat(\"Not all positive.\\n\")\n}\n\nNot all positive.\n\n\nThe function ifelse() can be used to loop over if-else expressions. There are two differences. First the condition is a logical vector. Second, both the if-true and if-false argument are always evaluated, so they better not generate an error!\n\nifelse(int55&lt;0, \"-\",\"+\")\n\n [1] \"-\" \"-\" \"-\" \"-\" \"-\" \"+\" \"+\" \"+\" \"+\" \"+\" \"+\"\n\n\n\n\n\nIt would be really convenient if we could access the state data by name. Florida is the 9 state alphabetically, but I can’t remember that.\nWhat we can do is add names to a vector. Then we can select by name.\n\nnames(state.area) &lt;- state.abb\nhead(state.area)\n\n    AL     AK     AZ     AR     CA     CO \n 51609 589757 113909  53104 158693 104247 \n\nhead(names(state.area))\n\n[1] \"AL\" \"AK\" \"AZ\" \"AR\" \"CA\" \"CO\"\n\nstate.area[\"FL\"]\n\n   FL \n58560 \n\nstate.area[c(\"NY\",\"CA\")]\n\n    NY     CA \n 49576 158693 \n\n\nSometimes we need to make up names. The paste() command is handy for that. It is vectorized, so you can put a bunch of numbers in.\n\npaste(\"Student\",1:5,sep=\"_\")\n\n[1] \"Student_1\" \"Student_2\" \"Student_3\" \"Student_4\" \"Student_5\"\n\n\n\n\n\n\nWrite an expression that removes the outlier from the data you generated for 1b.\n\n\nSuppose the data you generated for problem 1 was suppose to have a minimum score of 0 and a maximum score of 100. Fix, the data set so that all of the values are between 0 and 100.\n\n\nFix my positive/negative test, so that it has a 0 as well\n\n\nFind all of the states that are bigger than Florida.\n\n\nGenerate a bunch of random integers between -10 and 10. Then turn all negative integers into NA.",
    "crumbs": [
      "Arrays, Matrixes and Data Frames"
    ]
  },
  {
    "objectID": "RIntro/MatrixesAndDataFrames.html#implicit-looping-in-vectors",
    "href": "RIntro/MatrixesAndDataFrames.html#implicit-looping-in-vectors",
    "title": "Arrays, Matrixes and Data Frames",
    "section": "",
    "text": "All R objects are vectors: scalars in R are vectors of length 1.\n\ncat(\"The output will start with '[1]' to show that this is a vector.\\n\")\n\nThe output will start with '[1]' to show that this is a vector.\n\n3.14159  \n\n[1] 3.14159\n\n\nR implicitly loops over all the elements of a vector. Such implicit loops are faster than explicit for loops.\n\n1:11\n\n [1]  1  2  3  4  5  6  7  8  9 10 11\n\nmean(1:11)\n\n[1] 6\n\ny &lt;- (1:11 - mean(1:11))/sd(1:11)\nmean (y)\n\n[1] 0\n\nsd(y)\n\n[1] 1\n\n\n\n\nThe : operator produces sequences (of integers) between first and second argument.\n\n1:3\n\n[1] 1 2 3\n\n3:1\n\n[1] 3 2 1\n\n-1:1\n\n[1] -1  0  1\n\n-3:-1\n\n[1] -3 -2 -1\n\n\nThe c() function can be used to glue vectors together. (c stands for combine.)\n\nc(1:3, 10:12)\n\n[1]  1  2  3 10 11 12\n\nc(\"Haenzel\", \"Greatel\", \"Tedd\",\"Alice\")\n\n[1] \"Haenzel\" \"Greatel\" \"Tedd\"    \"Alice\"  \n\n\n\n\n\nR has a number of built in random number generators to generate random numbers. The most commonly used are runif, rnorm and sample. There are also many others, with names that look like rXXX (try substituting chisq, t, beta, gamma, &c for XXX).\n\nrunif(5)\n\n[1] 0.3114001 0.1532532 0.8512517 0.8840553 0.7097423\n\nrnorm(10)\n\n [1]  0.9884986 -0.0990681  0.1441367  0.4474792  1.7879468  1.4087663\n [7]  0.2188738 -0.6119803  1.3135611  1.5488097\n\nsample.int(5,5,replace=TRUE)\n\n[1] 3 3 1 4 4\n\n\n\n\n\n\nGenerate 100 random numbers with mean 50 and standard deviation 25.\n\n1a. Use the result of the previous question to generate a random sample of size 101 with one outlier of 200.\n\nGenerate random integers between 0 and 100\n\n\nThe variable state.area contains the areas of the 50 US states (in alphabetical area). Create a random sample of size 10 of the state areas.",
    "crumbs": [
      "Arrays, Matrixes and Data Frames"
    ]
  },
  {
    "objectID": "RIntro/MatrixesAndDataFrames.html#three-ways-of-subscripting-an-array",
    "href": "RIntro/MatrixesAndDataFrames.html#three-ways-of-subscripting-an-array",
    "title": "Arrays, Matrixes and Data Frames",
    "section": "",
    "text": "The [] operator is used to subscript vectors. There are three different things you can put inside of the brackets: numbers, logical expressions and names (character values).\n\n\nNumbers are the most straightforward way to do indexing. R starts the indexes at 1 and it goes up to the length of the vector. The function `length() is useful in writing indexes. Giving multiple indexes with return a sub-vector (remember, there are not scalars in R, just vectors of length 1).\n\nint10 &lt;- 1:10\nint10[3]\n\n[1] 3\n\nint10[c(5:7,9)]\n\n[1] 5 6 7 9\n\nstate.area[c(1,length(state.area))]\n\n[1] 51609 97914\n\n\nAnother useful trick is to use negative indexes. These leave the numbered varaibles out.\n\nint10[-2]\n\n[1]  1  3  4  5  6  7  8  9 10\n\nint10[-(3:8)]\n\n[1]  1  2  9 10\n\n\nIndexing expressions can also be used on the LHS of assignment operators, to allow to assignment to just certain values.\n\nint10[3] &lt;- -3\nint10\n\n [1]  1  2 -3  4  5  6  7  8  9 10\n\n\n\n\n\nThe second option for indexing is to use a logical vector the same length as the vector you are indexing.\n\nint10&lt;0\n\n [1] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\nint10[int10&lt;0]\n\n[1] -3\n\nint10[int10&lt;0] &lt;- abs (int10[int10&lt;0])\nint10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nBe careful with NAs.\n\nint55 &lt;- -5:5\nsqrt(int55) &lt; 1.2\n\nWarning in sqrt(int55): NaNs produced\n\n\n [1]    NA    NA    NA    NA    NA  TRUE  TRUE FALSE FALSE FALSE FALSE\n\nint55[sqrt(int55) &lt; 1.2]\n\nWarning in sqrt(int55): NaNs produced\n\n\n[1] NA NA NA NA NA  0  1\n\n\nThe real power of logical indexes comes when we have two vectors of the same length. For example, state.abb gives the two letter postal codes of the states. Suppose we wanted to see all of the states that are bigger than average:\n\nstate.abb[state.area&gt;median(state.area)]\n\n [1] \"AK\" \"AZ\" \"CA\" \"CO\" \"FL\" \"GA\" \"ID\" \"IL\" \"IA\" \"KS\" \"MI\" \"MN\" \"MO\" \"MT\" \"NE\"\n[16] \"NV\" \"NM\" \"ND\" \"OK\" \"OR\" \"SD\" \"TX\" \"UT\" \"WA\" \"WY\"\n\n\n\n\n\nThe built in langauge primitive if is not vectorized. It is expecting a single value. The code below will not do what you think it will.\n\ntry({\nif (int55 &lt; 0) {\n  cat(\"Negative.\\n\")\n} else { \n  cat(\"Non-negative.\\n\")\n}\n})\n\nError in if (int55 &lt; 0) { : the condition has length &gt; 1\n\n\nThe functions any(), all() and isTRUE() are often useful here.\n\nif (all(int55 &gt;0)) {\n  cat(\"Positive.\\n\")\n} else { \n  cat(\"Not all positive.\\n\")\n}\n\nNot all positive.\n\n\nThe function ifelse() can be used to loop over if-else expressions. There are two differences. First the condition is a logical vector. Second, both the if-true and if-false argument are always evaluated, so they better not generate an error!\n\nifelse(int55&lt;0, \"-\",\"+\")\n\n [1] \"-\" \"-\" \"-\" \"-\" \"-\" \"+\" \"+\" \"+\" \"+\" \"+\" \"+\"\n\n\n\n\n\nIt would be really convenient if we could access the state data by name. Florida is the 9 state alphabetically, but I can’t remember that.\nWhat we can do is add names to a vector. Then we can select by name.\n\nnames(state.area) &lt;- state.abb\nhead(state.area)\n\n    AL     AK     AZ     AR     CA     CO \n 51609 589757 113909  53104 158693 104247 \n\nhead(names(state.area))\n\n[1] \"AL\" \"AK\" \"AZ\" \"AR\" \"CA\" \"CO\"\n\nstate.area[\"FL\"]\n\n   FL \n58560 \n\nstate.area[c(\"NY\",\"CA\")]\n\n    NY     CA \n 49576 158693 \n\n\nSometimes we need to make up names. The paste() command is handy for that. It is vectorized, so you can put a bunch of numbers in.\n\npaste(\"Student\",1:5,sep=\"_\")\n\n[1] \"Student_1\" \"Student_2\" \"Student_3\" \"Student_4\" \"Student_5\"\n\n\n\n\n\n\nWrite an expression that removes the outlier from the data you generated for 1b.\n\n\nSuppose the data you generated for problem 1 was suppose to have a minimum score of 0 and a maximum score of 100. Fix, the data set so that all of the values are between 0 and 100.\n\n\nFix my positive/negative test, so that it has a 0 as well\n\n\nFind all of the states that are bigger than Florida.\n\n\nGenerate a bunch of random integers between -10 and 10. Then turn all negative integers into NA.",
    "crumbs": [
      "Arrays, Matrixes and Data Frames"
    ]
  },
  {
    "objectID": "RIntro/MatrixesAndDataFrames.html#matrixes-and-arrays-are-vectors-with-a-dim-attribute",
    "href": "RIntro/MatrixesAndDataFrames.html#matrixes-and-arrays-are-vectors-with-a-dim-attribute",
    "title": "Arrays, Matrixes and Data Frames",
    "section": "Matrixes and Arrays are vectors with a dim attribute",
    "text": "Matrixes and Arrays are vectors with a dim attribute\n\nA matrix is an object with rows and columns.\nAn array can have any number of dimensions.\nBut they all the entries need to be the same type (mode).\nThere is a dim() attribute which shows the dimensions of the matrix.\n\n\ndim(state.x77)\n\n[1] 50  8\n\nhead(state.x77)\n\n           Population Income Illiteracy Life Exp Murder HS Grad Frost   Area\nAlabama          3615   3624        2.1    69.05   15.1    41.3    20  50708\nAlaska            365   6315        1.5    69.31   11.3    66.7   152 566432\nArizona          2212   4530        1.8    70.55    7.8    58.1    15 113417\nArkansas         2110   3378        1.9    70.66   10.1    39.9    65  51945\nCalifornia      21198   5114        1.1    71.71   10.3    62.6    20 156361\nColorado         2541   4884        0.7    72.06    6.8    63.9   166 103766\n\n\n\ngetting and setting dims\nThe dim() function is used to access the number of rows and columns. dim()[1] gets the number of rows and dim()[2] the number of columns.\nFor maxtrixes, the functions nrow() and ncol() are easier to use.\nSetting dim() will reset a vector into a matrix or array.\n\nnrow(state.x77)\n\n[1] 50\n\nncol(state.x77)\n\n[1] 8\n\nint12 &lt;- 1:12\ndim(int12) &lt;- c(3,4)\nint12\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\n\n\n\nmatrix() and array() functions\n\nSetting the dim() attribute directly is not recommended (makes for hard to read code).\nInstead use matrix() or array()\nR stores matrixes in row major order (like FORTRAN, not like c).\n\nUse byrow=TRUE to reverse this in matrix or array\n\n\n\nmatrix(1:12,3,4)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\nmatrix(1:12,3,4,byrow=TRUE)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n\narray(1:24,c(2,3,4))\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]    7    9   11\n[2,]    8   10   12\n\n, , 3\n\n     [,1] [,2] [,3]\n[1,]   13   15   17\n[2,]   14   16   18\n\n, , 4\n\n     [,1] [,2] [,3]\n[1,]   19   21   23\n[2,]   20   22   24",
    "crumbs": [
      "Arrays, Matrixes and Data Frames"
    ]
  },
  {
    "objectID": "RIntro/MatrixesAndDataFrames.html#numeric-and-logical-indexes",
    "href": "RIntro/MatrixesAndDataFrames.html#numeric-and-logical-indexes",
    "title": "Arrays, Matrixes and Data Frames",
    "section": "numeric and logical indexes",
    "text": "numeric and logical indexes\nFor matrixes and arrays, the [] operator does something a little bit different. In particular, x[i,j] picks out row \\(i\\) and column \\(j\\). Either the row or column selector could be\n\nA number or vector of numbers (pick those rows or columns)\nA negative number of vector of negative numbers (excluded those rows or columns)\nA logical vector of size nrow(x) or ncol(x) (select the rows/columns corresponding to true).\nA character vector (select rows or columns by name, see below).\nLeft blank, in which case all rows/columns are selected.\n\nIf a single row or column is selected, then it turns into a vector.\n\nstate.x77[1:5,1:5]\n\n           Population Income Illiteracy Life Exp Murder\nAlabama          3615   3624        2.1    69.05   15.1\nAlaska            365   6315        1.5    69.31   11.3\nArizona          2212   4530        1.8    70.55    7.8\nArkansas         2110   3378        1.9    70.66   10.1\nCalifornia      21198   5114        1.1    71.71   10.3\n\nstate.x77[1:5,]\n\n           Population Income Illiteracy Life Exp Murder HS Grad Frost   Area\nAlabama          3615   3624        2.1    69.05   15.1    41.3    20  50708\nAlaska            365   6315        1.5    69.31   11.3    66.7   152 566432\nArizona          2212   4530        1.8    70.55    7.8    58.1    15 113417\nArkansas         2110   3378        1.9    70.66   10.1    39.9    65  51945\nCalifornia      21198   5114        1.1    71.71   10.3    62.6    20 156361\n\nstate.x77[9,]\n\nPopulation     Income Illiteracy   Life Exp     Murder    HS Grad      Frost \n   8277.00    4815.00       1.30      70.66      10.70      52.60      11.00 \n      Area \n  54090.00 \n\ndim(state.x77[9,])\n\nNULL\n\nhead(state.x77[,3])\n\n   Alabama     Alaska    Arizona   Arkansas California   Colorado \n       2.1        1.5        1.8        1.9        1.1        0.7 \n\nstate.x77[9,,drop=FALSE]\n\n        Population Income Illiteracy Life Exp Murder HS Grad Frost  Area\nFlorida       8277   4815        1.3    70.66   10.7    52.6    11 54090\n\ndim(state.x77[9,,drop=FALSE])\n\n[1] 1 8",
    "crumbs": [
      "Arrays, Matrixes and Data Frames"
    ]
  },
  {
    "objectID": "RIntro/MatrixesAndDataFrames.html#dimnames-and-character-indexes",
    "href": "RIntro/MatrixesAndDataFrames.html#dimnames-and-character-indexes",
    "title": "Arrays, Matrixes and Data Frames",
    "section": "dimnames and character indexes",
    "text": "dimnames and character indexes\nTo use character indexes with matrixes, we need to set the rownames() and colnames() of the matrix. We can also use the dimnames() (although this will produce a list).\n\nrownames(state.x77)\n\n [1] \"Alabama\"        \"Alaska\"         \"Arizona\"        \"Arkansas\"      \n [5] \"California\"     \"Colorado\"       \"Connecticut\"    \"Delaware\"      \n [9] \"Florida\"        \"Georgia\"        \"Hawaii\"         \"Idaho\"         \n[13] \"Illinois\"       \"Indiana\"        \"Iowa\"           \"Kansas\"        \n[17] \"Kentucky\"       \"Louisiana\"      \"Maine\"          \"Maryland\"      \n[21] \"Massachusetts\"  \"Michigan\"       \"Minnesota\"      \"Mississippi\"   \n[25] \"Missouri\"       \"Montana\"        \"Nebraska\"       \"Nevada\"        \n[29] \"New Hampshire\"  \"New Jersey\"     \"New Mexico\"     \"New York\"      \n[33] \"North Carolina\" \"North Dakota\"   \"Ohio\"           \"Oklahoma\"      \n[37] \"Oregon\"         \"Pennsylvania\"   \"Rhode Island\"   \"South Carolina\"\n[41] \"South Dakota\"   \"Tennessee\"      \"Texas\"          \"Utah\"          \n[45] \"Vermont\"        \"Virginia\"       \"Washington\"     \"West Virginia\" \n[49] \"Wisconsin\"      \"Wyoming\"       \n\ncolnames(state.x77)\n\n[1] \"Population\" \"Income\"     \"Illiteracy\" \"Life Exp\"   \"Murder\"    \n[6] \"HS Grad\"    \"Frost\"      \"Area\"      \n\ndimnames(state.x77)\n\n[[1]]\n [1] \"Alabama\"        \"Alaska\"         \"Arizona\"        \"Arkansas\"      \n [5] \"California\"     \"Colorado\"       \"Connecticut\"    \"Delaware\"      \n [9] \"Florida\"        \"Georgia\"        \"Hawaii\"         \"Idaho\"         \n[13] \"Illinois\"       \"Indiana\"        \"Iowa\"           \"Kansas\"        \n[17] \"Kentucky\"       \"Louisiana\"      \"Maine\"          \"Maryland\"      \n[21] \"Massachusetts\"  \"Michigan\"       \"Minnesota\"      \"Mississippi\"   \n[25] \"Missouri\"       \"Montana\"        \"Nebraska\"       \"Nevada\"        \n[29] \"New Hampshire\"  \"New Jersey\"     \"New Mexico\"     \"New York\"      \n[33] \"North Carolina\" \"North Dakota\"   \"Ohio\"           \"Oklahoma\"      \n[37] \"Oregon\"         \"Pennsylvania\"   \"Rhode Island\"   \"South Carolina\"\n[41] \"South Dakota\"   \"Tennessee\"      \"Texas\"          \"Utah\"          \n[45] \"Vermont\"        \"Virginia\"       \"Washington\"     \"West Virginia\" \n[49] \"Wisconsin\"      \"Wyoming\"       \n\n[[2]]\n[1] \"Population\" \"Income\"     \"Illiteracy\" \"Life Exp\"   \"Murder\"    \n[6] \"HS Grad\"    \"Frost\"      \"Area\"      \n\nrownames(state.x77) &lt;- state.abb\nhead(state.x77)\n\n   Population Income Illiteracy Life Exp Murder HS Grad Frost   Area\nAL       3615   3624        2.1    69.05   15.1    41.3    20  50708\nAK        365   6315        1.5    69.31   11.3    66.7   152 566432\nAZ       2212   4530        1.8    70.55    7.8    58.1    15 113417\nAR       2110   3378        1.9    70.66   10.1    39.9    65  51945\nCA      21198   5114        1.1    71.71   10.3    62.6    20 156361\nCO       2541   4884        0.7    72.06    6.8    63.9   166 103766",
    "crumbs": [
      "Arrays, Matrixes and Data Frames"
    ]
  },
  {
    "objectID": "RIntro/MatrixesAndDataFrames.html#row-and-column-sums-and-averages",
    "href": "RIntro/MatrixesAndDataFrames.html#row-and-column-sums-and-averages",
    "title": "Arrays, Matrixes and Data Frames",
    "section": "Row and column sums and averages",
    "text": "Row and column sums and averages\nRemember that a matrix is just a vector with a dim attribute. Consequently, mean and other summary functions don’t do what we want:\n\nmean(state.x77)\n\n[1] 9956.887\n\nsd(state.x77)\n\n[1] 37801.78\n\nvar(state.x77)\n\n              Population        Income   Illiteracy      Life Exp       Murder\nPopulation 19931683.7588   571229.7796  292.8679592 -4.078425e+02  5663.523714\nIncome       571229.7796   377573.3061 -163.7020408  2.806632e+02  -521.894286\nIlliteracy      292.8680     -163.7020    0.3715306 -4.815122e-01     1.581776\nLife Exp       -407.8425      280.6632   -0.4815122  1.802020e+00    -3.869480\nMurder         5663.5237     -521.8943    1.5817755 -3.869480e+00    13.627465\nHS Grad       -3551.5096     3076.7690   -3.2354694  6.312685e+00   -14.549616\nFrost        -77081.9727     7227.6041  -21.2900000  1.828678e+01  -103.406000\nArea        8587916.9494 19049013.7510 4018.3371429 -1.229410e+04 71940.429959\n                 HS Grad        Frost          Area\nPopulation  -3551.509551 -77081.97265  8.587917e+06\nIncome       3076.768980   7227.60408  1.904901e+07\nIlliteracy     -3.235469    -21.29000  4.018337e+03\nLife Exp        6.312685     18.28678 -1.229410e+04\nMurder        -14.549616   -103.40600  7.194043e+04\nHS Grad        65.237894    153.99216  2.298732e+05\nFrost         153.992163   2702.00857  2.627039e+05\nArea       229873.192816 262703.89306  7.280748e+09\n\ncor(state.x77)\n\n            Population     Income  Illiteracy    Life Exp     Murder\nPopulation  1.00000000  0.2082276  0.10762237 -0.06805195  0.3436428\nIncome      0.20822756  1.0000000 -0.43707519  0.34025534 -0.2300776\nIlliteracy  0.10762237 -0.4370752  1.00000000 -0.58847793  0.7029752\nLife Exp   -0.06805195  0.3402553 -0.58847793  1.00000000 -0.7808458\nMurder      0.34364275 -0.2300776  0.70297520 -0.78084575  1.0000000\nHS Grad    -0.09848975  0.6199323 -0.65718861  0.58221620 -0.4879710\nFrost      -0.33215245  0.2262822 -0.67194697  0.26206801 -0.5388834\nArea        0.02254384  0.3633154  0.07726113 -0.10733194  0.2283902\n               HS Grad      Frost        Area\nPopulation -0.09848975 -0.3321525  0.02254384\nIncome      0.61993232  0.2262822  0.36331544\nIlliteracy -0.65718861 -0.6719470  0.07726113\nLife Exp    0.58221620  0.2620680 -0.10733194\nMurder     -0.48797102 -0.5388834  0.22839021\nHS Grad     1.00000000  0.3667797  0.33354187\nFrost       0.36677970  1.0000000  0.05922910\nArea        0.33354187  0.0592291  1.00000000\n\n\nTaking row and column sums are such a frequent operation, that there is a shortcut for them: rowSums(), colSums(), rowMeans(), colMeans()\n\ncolMeans(state.x77)\n\nPopulation     Income Illiteracy   Life Exp     Murder    HS Grad      Frost \n 4246.4200  4435.8000     1.1700    70.8786     7.3780    53.1080   104.4600 \n      Area \n70735.8800",
    "crumbs": [
      "Arrays, Matrixes and Data Frames"
    ]
  },
  {
    "objectID": "RIntro/MatrixesAndDataFrames.html#apply-and-sweep",
    "href": "RIntro/MatrixesAndDataFrames.html#apply-and-sweep",
    "title": "Arrays, Matrixes and Data Frames",
    "section": "Apply and Sweep",
    "text": "Apply and Sweep\nThe apply() operator can turn any summary function into a row or column function.\n\nhelp(apply)\n\nThe MARGIN argument to apply should be 1 for rows, 2 for columns and so forth for generaly arrays.\n\nint12\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\napply(int12,1,max)\n\n[1] 10 11 12\n\napply(int12,2,max)\n\n[1]  3  6  9 12\n\n\nThe sweep operator “subtracts” a vector from all of the rows or columns of the matrix.\n“Subtracts” is in quotes because actually any operator can be used here. Subtracts “-” and divides “/” are the most common.\n\nhelp(sweep)\nrow.min &lt;- apply(int12,1,min)\nsweep(int12,1,row.min,\"/\")\n\n     [,1] [,2] [,3] [,4]\n[1,]    1  4.0    7 10.0\n[2,]    1  2.5    4  5.5\n[3,]    1  2.0    3  4.0\n\ncol.min &lt;- apply(int12,2,min)\nsweep(int12,2,col.min,\"-\")\n\n     [,1] [,2] [,3] [,4]\n[1,]    0    0    0    0\n[2,]    1    1    1    1\n[3,]    2    2    2    2",
    "crumbs": [
      "Arrays, Matrixes and Data Frames"
    ]
  },
  {
    "objectID": "RIntro/MatrixesAndDataFrames.html#exercises-2",
    "href": "RIntro/MatrixesAndDataFrames.html#exercises-2",
    "title": "Arrays, Matrixes and Data Frames",
    "section": "Exercises:",
    "text": "Exercises:\n\nFind the population for all states whose area is bigger than Florida’s.\n\n\nCalculate the population density (population per area) for each state\n\n\nTurn the state.x77 data into z-scores by subtracting the column means and dividing by the column standard deviations.\n\n\nScale the state.x77 data from 0 (minimum in the column) to 1 (maximum in the column).",
    "crumbs": [
      "Arrays, Matrixes and Data Frames"
    ]
  },
  {
    "objectID": "RIntro/MatrixesAndDataFrames.html#single-and-double-extraction",
    "href": "RIntro/MatrixesAndDataFrames.html#single-and-double-extraction",
    "title": "Arrays, Matrixes and Data Frames",
    "section": "Single [ and double [[ extraction",
    "text": "Single [ and double [[ extraction",
    "crumbs": [
      "Arrays, Matrixes and Data Frames"
    ]
  },
  {
    "objectID": "RIntro/MatrixesAndDataFrames.html#named-lists-and-extraction",
    "href": "RIntro/MatrixesAndDataFrames.html#named-lists-and-extraction",
    "title": "Arrays, Matrixes and Data Frames",
    "section": "Named Lists and $ extraction",
    "text": "Named Lists and $ extraction",
    "crumbs": [
      "Arrays, Matrixes and Data Frames"
    ]
  },
  {
    "objectID": "RIntro/MatrixesAndDataFrames.html#lapply-and-sapply-for-looping-through-lists",
    "href": "RIntro/MatrixesAndDataFrames.html#lapply-and-sapply-for-looping-through-lists",
    "title": "Arrays, Matrixes and Data Frames",
    "section": "lapply and sapply for looping through lists",
    "text": "lapply and sapply for looping through lists",
    "crumbs": [
      "Arrays, Matrixes and Data Frames"
    ]
  },
  {
    "objectID": "RIntro/MatrixesAndDataFrames.html#classes-as-list-with-special-behavior",
    "href": "RIntro/MatrixesAndDataFrames.html#classes-as-list-with-special-behavior",
    "title": "Arrays, Matrixes and Data Frames",
    "section": "Classes as list with special behavior",
    "text": "Classes as list with special behavior\n\nGeneric functions and methods\n\n\n‘factor’ and ‘ordered’ classes\n\nS4 classes vs S3 classes",
    "crumbs": [
      "Arrays, Matrixes and Data Frames"
    ]
  },
  {
    "objectID": "RIntro/MatrixesAndDataFrames.html#data.frame-and-read.table",
    "href": "RIntro/MatrixesAndDataFrames.html#data.frame-and-read.table",
    "title": "Arrays, Matrixes and Data Frames",
    "section": "data.frame() and read.table()",
    "text": "data.frame() and read.table()",
    "crumbs": [
      "Arrays, Matrixes and Data Frames"
    ]
  },
  {
    "objectID": "RIntro/MatrixesAndDataFrames.html#matrix-like-behaior-using-subscripts",
    "href": "RIntro/MatrixesAndDataFrames.html#matrix-like-behaior-using-subscripts",
    "title": "Arrays, Matrixes and Data Frames",
    "section": "Matrix-like behaior – Using [ subscripts",
    "text": "Matrix-like behaior – Using [ subscripts\n\napply, rownames, colnames and colsum and rowsum",
    "crumbs": [
      "Arrays, Matrixes and Data Frames"
    ]
  },
  {
    "objectID": "RIntro/MatrixesAndDataFrames.html#list-like-behavior-using-and-subscripts",
    "href": "RIntro/MatrixesAndDataFrames.html#list-like-behavior-using-and-subscripts",
    "title": "Arrays, Matrixes and Data Frames",
    "section": "List-like behavior – Using [[ and $ subscripts",
    "text": "List-like behavior – Using [[ and $ subscripts",
    "crumbs": [
      "Arrays, Matrixes and Data Frames"
    ]
  },
  {
    "objectID": "RIntro/MatrixesAndDataFrames.html#names-lapply-and-sapply",
    "href": "RIntro/MatrixesAndDataFrames.html#names-lapply-and-sapply",
    "title": "Arrays, Matrixes and Data Frames",
    "section": "names, lapply and sapply",
    "text": "names, lapply and sapply",
    "crumbs": [
      "Arrays, Matrixes and Data Frames"
    ]
  },
  {
    "objectID": "RIntro/MatrixesAndDataFrames.html#as.matrix-and-as.data.frame",
    "href": "RIntro/MatrixesAndDataFrames.html#as.matrix-and-as.data.frame",
    "title": "Arrays, Matrixes and Data Frames",
    "section": "as.matrix and as.data.frame",
    "text": "as.matrix and as.data.frame",
    "crumbs": [
      "Arrays, Matrixes and Data Frames"
    ]
  },
  {
    "objectID": "RIntro/EDAwithGGPlot.html",
    "href": "RIntro/EDAwithGGPlot.html",
    "title": "Exploratory Data Analysis with GGplot",
    "section": "",
    "text": "For this example, we are going to use GGplot, which is part of the tidyverse. Tidyverse is an extra layer on top of R which makes it easy to manipulate data as a kind of a workflow. Note that tidyverse is actually a meta-package: it downloads a number of generally useful packages, including GGplot (GG stands for Grammar of Graphics, a book about how to build up complex plots from smaller pieces.)\nThe command install.packages() installs packages, that is, it downloads them from the CRAN library to your local computer. The command library() tells R that you want to use that package in this session. You need to run library() every time, but you only need to run install.packages() once.\n\nif (!(\"tidyverse\" %in% row.names(installed.packages()))) {\n  install.packages(\"tidyverse\",repos=\"https://cloud.r-project.org\",dependencies=TRUE)\n}\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.2.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "Exploratory Data Analysis with GGplot"
    ]
  },
  {
    "objectID": "RIntro/EDAwithGGPlot.html#tidyverse-software",
    "href": "RIntro/EDAwithGGPlot.html#tidyverse-software",
    "title": "Exploratory Data Analysis with GGplot",
    "section": "",
    "text": "For this example, we are going to use GGplot, which is part of the tidyverse. Tidyverse is an extra layer on top of R which makes it easy to manipulate data as a kind of a workflow. Note that tidyverse is actually a meta-package: it downloads a number of generally useful packages, including GGplot (GG stands for Grammar of Graphics, a book about how to build up complex plots from smaller pieces.)\nThe command install.packages() installs packages, that is, it downloads them from the CRAN library to your local computer. The command library() tells R that you want to use that package in this session. You need to run library() every time, but you only need to run install.packages() once.\n\nif (!(\"tidyverse\" %in% row.names(installed.packages()))) {\n  install.packages(\"tidyverse\",repos=\"https://cloud.r-project.org\",dependencies=TRUE)\n}\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.2.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "Exploratory Data Analysis with GGplot"
    ]
  },
  {
    "objectID": "RIntro/EDAwithGGPlot.html#tibbles",
    "href": "RIntro/EDAwithGGPlot.html#tibbles",
    "title": "Exploratory Data Analysis with GGplot",
    "section": "Tibbles",
    "text": "Tibbles\nFor this exercise we will use the data set state.x77 which comes with R. You can find more information about this data set by doing:\n\nhelp(state.x77)\n\nA tibble is a data structure with rows corresponding to cases and columns to variables. It is a tidy version of a data frame.\n\nas_tibble(state.x77) %&gt;% add_column(region=state.region,name=state.name,code=state.abb,center_x=state.center$x,center_y=state.center$y) -&gt; state77\nstate77\n\n# A tibble: 50 × 13\n   Population Income Illiteracy `Life Exp` Murder `HS Grad` Frost   Area region \n        &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;  \n 1       3615   3624        2.1       69.0   15.1      41.3    20  50708 South  \n 2        365   6315        1.5       69.3   11.3      66.7   152 566432 West   \n 3       2212   4530        1.8       70.6    7.8      58.1    15 113417 West   \n 4       2110   3378        1.9       70.7   10.1      39.9    65  51945 South  \n 5      21198   5114        1.1       71.7   10.3      62.6    20 156361 West   \n 6       2541   4884        0.7       72.1    6.8      63.9   166 103766 West   \n 7       3100   5348        1.1       72.5    3.1      56     139   4862 Northe…\n 8        579   4809        0.9       70.1    6.2      54.6   103   1982 South  \n 9       8277   4815        1.3       70.7   10.7      52.6    11  54090 South  \n10       4931   4091        2         68.5   13.9      40.6    60  58073 South  \n# ℹ 40 more rows\n# ℹ 4 more variables: name &lt;chr&gt;, code &lt;chr&gt;, center_x &lt;dbl&gt;, center_y &lt;dbl&gt;\n\n\nThe View() command opens the data frame/matrix/tibble in another window.\n\nView(state77)\n\n\nTry state77 in the console. The tibble is slightly different from the data frame in the way it prints.\nTibble and data frames are pretty much interchangeable. (Where they aren’t use as.data.frame() or as_tibble() to convert.\n\n\nNote the type of the variables are shown in the display of the tibble. The name and postal code are left as strings, but region is a factor (with four levels). In a data frame, the string variables are automatically converted to factors, which is not always what you want.\n\n\nUse read_csv() instead of read.csv() to load a CSV file as a tibble instead of a data frame.",
    "crumbs": [
      "Exploratory Data Analysis with GGplot"
    ]
  },
  {
    "objectID": "RIntro/EDAwithGGPlot.html#the-pipe",
    "href": "RIntro/EDAwithGGPlot.html#the-pipe",
    "title": "Exploratory Data Analysis with GGplot",
    "section": "The Pipe",
    "text": "The Pipe\nThe special operator %&gt;% (or |&gt;) can be used to chain operations together.\nThe expression above gives an example. The output of as_tibble() is passed to the add_column() which is then passed to the assignment operator -&gt;.\nNote the backward arrow -&gt;. This is like the usual assignment operator &lt;- except now the name of the variable is on the right instead of the left.\nA typical chain looks like:\ndata %&gt;% select(variables) %&gt;% filter(cases) %&gt;% analysis() -&gt; result\nOr maybe the analysis is replaced with a call to ggplot to make a plot.",
    "crumbs": [
      "Exploratory Data Analysis with GGplot"
    ]
  },
  {
    "objectID": "RIntro/EDAwithGGPlot.html#selecting-variables",
    "href": "RIntro/EDAwithGGPlot.html#selecting-variables",
    "title": "Exploratory Data Analysis with GGplot",
    "section": "Selecting Variables",
    "text": "Selecting Variables\nThe select() command can be used to select a subset of variables.\n\nstate77 %&gt;% select(code,Population,Income)\n\n# A tibble: 50 × 3\n   code  Population Income\n   &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n 1 AL          3615   3624\n 2 AK           365   6315\n 3 AZ          2212   4530\n 4 AR          2110   3378\n 5 CA         21198   5114\n 6 CO          2541   4884\n 7 CT          3100   5348\n 8 DE           579   4809\n 9 FL          8277   4815\n10 GA          4931   4091\n# ℹ 40 more rows\n\nstate77 %&gt;% select(code,region:code)\n\n# A tibble: 50 × 3\n   code  region    name       \n   &lt;chr&gt; &lt;fct&gt;     &lt;chr&gt;      \n 1 AL    South     Alabama    \n 2 AK    West      Alaska     \n 3 AZ    West      Arizona    \n 4 AR    South     Arkansas   \n 5 CA    West      California \n 6 CO    West      Colorado   \n 7 CT    Northeast Connecticut\n 8 DE    South     Delaware   \n 9 FL    South     Florida    \n10 GA    South     Georgia    \n# ℹ 40 more rows\n\nstate77 %&gt;% select(-name)\n\n# A tibble: 50 × 12\n   Population Income Illiteracy `Life Exp` Murder `HS Grad` Frost   Area region \n        &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;  \n 1       3615   3624        2.1       69.0   15.1      41.3    20  50708 South  \n 2        365   6315        1.5       69.3   11.3      66.7   152 566432 West   \n 3       2212   4530        1.8       70.6    7.8      58.1    15 113417 West   \n 4       2110   3378        1.9       70.7   10.1      39.9    65  51945 South  \n 5      21198   5114        1.1       71.7   10.3      62.6    20 156361 West   \n 6       2541   4884        0.7       72.1    6.8      63.9   166 103766 West   \n 7       3100   5348        1.1       72.5    3.1      56     139   4862 Northe…\n 8        579   4809        0.9       70.1    6.2      54.6   103   1982 South  \n 9       8277   4815        1.3       70.7   10.7      52.6    11  54090 South  \n10       4931   4091        2         68.5   13.9      40.6    60  58073 South  \n# ℹ 40 more rows\n# ℹ 3 more variables: code &lt;chr&gt;, center_x &lt;dbl&gt;, center_y &lt;dbl&gt;\n\nstate77 %&gt;% select(code,starts_with(\"center\"))\n\n# A tibble: 50 × 3\n   code  center_x center_y\n   &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 AL       -86.8     32.6\n 2 AK      -127.      49.2\n 3 AZ      -112.      34.2\n 4 AR       -92.3     34.7\n 5 CA      -120.      36.5\n 6 CO      -106.      38.7\n 7 CT       -72.4     41.6\n 8 DE       -75.0     38.7\n 9 FL       -81.7     27.9\n10 GA       -83.4     32.3\n# ℹ 40 more rows\n\n\nUsually having more columns than you need is harmless.\nFor example, using lm() to fit a regression of ggplot() to make a plot will just use the variables referenced in the model or plot description.\nHowever, sometimes is it easier to work with a smaller subset of the data with just the stuff you need.",
    "crumbs": [
      "Exploratory Data Analysis with GGplot"
    ]
  },
  {
    "objectID": "RIntro/EDAwithGGPlot.html#making-new-variables",
    "href": "RIntro/EDAwithGGPlot.html#making-new-variables",
    "title": "Exploratory Data Analysis with GGplot",
    "section": "Making New Variables",
    "text": "Making New Variables\nWe already saw the add_column() function for adding columns.\nThe mutate() function adds new columns as a function of the old ones:\n\nstate77 %&gt;% mutate(Pop_Density=Population/Area) -&gt; state77a\nstate77a\n\n# A tibble: 50 × 14\n   Population Income Illiteracy `Life Exp` Murder `HS Grad` Frost   Area region \n        &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;  \n 1       3615   3624        2.1       69.0   15.1      41.3    20  50708 South  \n 2        365   6315        1.5       69.3   11.3      66.7   152 566432 West   \n 3       2212   4530        1.8       70.6    7.8      58.1    15 113417 West   \n 4       2110   3378        1.9       70.7   10.1      39.9    65  51945 South  \n 5      21198   5114        1.1       71.7   10.3      62.6    20 156361 West   \n 6       2541   4884        0.7       72.1    6.8      63.9   166 103766 West   \n 7       3100   5348        1.1       72.5    3.1      56     139   4862 Northe…\n 8        579   4809        0.9       70.1    6.2      54.6   103   1982 South  \n 9       8277   4815        1.3       70.7   10.7      52.6    11  54090 South  \n10       4931   4091        2         68.5   13.9      40.6    60  58073 South  \n# ℹ 40 more rows\n# ℹ 5 more variables: name &lt;chr&gt;, code &lt;chr&gt;, center_x &lt;dbl&gt;, center_y &lt;dbl&gt;,\n#   Pop_Density &lt;dbl&gt;",
    "crumbs": [
      "Exploratory Data Analysis with GGplot"
    ]
  },
  {
    "objectID": "RIntro/EDAwithGGPlot.html#recoding-variables",
    "href": "RIntro/EDAwithGGPlot.html#recoding-variables",
    "title": "Exploratory Data Analysis with GGplot",
    "section": "Recoding Variables",
    "text": "Recoding Variables\nRecoding is important because sometimes the way the variable is stored in the data file is not the same as the way we want to analyze it.\n\nFactor variables can represent categories with integer values or string labels.\n\nOften there is a code book which maps integer category labels to string values. For example:\n\n\n\nFemale\nMale\n\nThe factor() function creates factor variables.\n\nfactor(c(1,1,1,2,2,2),levels=1:2,labels=c(\"Female\",\"Male\"))\n\n[1] Female Female Female Male   Male   Male  \nLevels: Female Male\n\nfactor(c(\"Male\",\"Male\",\"Male\",\"Female\",\"Female\",\"Female\"),levels=c(\"Male\",\"Female\"))\n\n[1] Male   Male   Male   Female Female Female\nLevels: Male Female\n\nordered(c(\"H\",\"H\",\"M\",\"M\",\"L\",\"L\"), levels=c(\"L\",\"M\",\"H\"))\n\n[1] H H M M L L\nLevels: L &lt; M &lt; H\n\n\n\nThe levels argument tells R how the data are coded (in the case of integer coding).\nThe labels argument gives the names for the levels (if omitted it is the same as levels).\n\n\nThe ordered() function produces an ordered variable as opposed to factor() which produces a nominal one. This only makes a difference in a few places. Probably the most important one is how they are used in an Analysis of Variance (ANOVA). That is covered in EDF 5402.\n\n\n\n\n\n\n\nWarning\n\n\n\nNote Bene! The read_csv() function which is part of the tidyverse will read factor variables as either character or integer variables, depending on how they are coded. So you will need to use mutate(x=factor(x)) to convert x into a factor.\n\n\nThe function parse_factor() is almost the same, but gives a warning if some of the levels aren’t recognized.\n\nfactor(c(\"Male\",\"Female\",\"Non-binary\"),levels=c(\"Male\",\"Female\"))\n\n[1] Male   Female &lt;NA&gt;  \nLevels: Male Female\n\nparse_factor(c(\"Male\",\"Female\",\"Non-binary\"),levels=c(\"Male\",\"Female\"))\n\nWarning: 1 parsing failure.\nrow col           expected     actual\n  3  -- value in level set Non-binary\n\n\n[1] Male   Female &lt;NA&gt;  \nattr(,\"problems\")\n# A tibble: 1 × 4\n    row   col expected           actual    \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;              &lt;chr&gt;     \n1     3    NA value in level set Non-binary\nLevels: Male Female\n\n\nAnother way to do the coding is to use * recode() (makes a character or numeric value) * recode_factor() (makes a factor variable)\nThe first argument is the vector to be recorded, the remaining arguments are the values to be replaced.\n\nrecode_factor(c(1,1,1,2,2,2),`1`=\"Male\",`2`=\"Female\")\n\n[1] Male   Male   Male   Female Female Female\nLevels: Male Female\n\nrecode_factor(c(1,1,1,2,2,2),\"Male\",\"Female\")\n\n[1] Male   Male   Male   Female Female Female\nLevels: Male Female\n\nrecode_factor(c(\"M\",\"M\",\"F\",\"F\"),M=\"Male\",F=\"Female\")\n\n[1] Male   Male   Female Female\nLevels: Male Female\n\nrecode_factor(c(\"White\",\"Black\",\"Latinx\",\"Other\"),White=\"White\",.default=\"Non-White\")\n\n[1] White     Non-White Non-White Non-White\nLevels: White Non-White\n\n\nNote how we used the last version to collapse several categories into one. This is often useful, particularly when the number of subjects in one category is small.",
    "crumbs": [
      "Exploratory Data Analysis with GGplot"
    ]
  },
  {
    "objectID": "RIntro/EDAwithGGPlot.html#recoding-nas",
    "href": "RIntro/EDAwithGGPlot.html#recoding-nas",
    "title": "Exploratory Data Analysis with GGplot",
    "section": "Recoding NAs",
    "text": "Recoding NAs\nA special case of recoding comes about with missing values.\nIn R, these are called NA (for Not Applicable).\n\nNAs are contagious: NA + anything is still NA.\n\n\nNA+5\n\n[1] NA\n\nmean(c(1,2,NA))\n\n[1] NA\n\nmean(c(1,2,NA),na.rm=TRUE)\n\n[1] 1.5\n\n\n\nNaN (not a number) is similar but it comes from nonsense arthimatic (taking log of negative number).\nNAs can be coded in many different ways in a data set:\n\nLeave the value blank.\nSpecial character, e.g., . or *\nSpecial String, e.g., NA\nNonsense numeric value, e.g., -9\n\n\nWhen using nonsense numeric values, it is important to pick a value that is not plausible, e.g., a large negative value. That way, if you accidently forget to convert, you can know that something is wrong.\nThe function na_if() can be used to replace a value with NAs.\n\nna_if(c(1:5,-9),-9)\n\n[1]  1  2  3  4  5 NA\n\nstarwars %&gt;% select(name,eye_color) %&gt;%\n  mutate(eye_color=na_if(eye_color,\"unknown\"))\n\n# A tibble: 87 × 2\n   name               eye_color\n   &lt;chr&gt;              &lt;chr&gt;    \n 1 Luke Skywalker     blue     \n 2 C-3PO              yellow   \n 3 R2-D2              red      \n 4 Darth Vader        yellow   \n 5 Leia Organa        brown    \n 6 Owen Lars          blue     \n 7 Beru Whitesun Lars blue     \n 8 R5-D4              red      \n 9 Biggs Darklighter  brown    \n10 Obi-Wan Kenobi     blue-gray\n# ℹ 77 more rows\n\n\nThe function replace_na() goes in the opposite direction.\nFor example, we might want to treat missing values as score of 0 on a test.\n\nreplace_na(c(1,1,0,0,NA),0)\n\n[1] 1 1 0 0 0",
    "crumbs": [
      "Exploratory Data Analysis with GGplot"
    ]
  },
  {
    "objectID": "RIntro/EDAwithGGPlot.html#logical-tests",
    "href": "RIntro/EDAwithGGPlot.html#logical-tests",
    "title": "Exploratory Data Analysis with GGplot",
    "section": "Logical Tests",
    "text": "Logical Tests\nThe function if_else() is also useful for splitting data sets up into groups.\nWe can see the form in:\n\nargs(if_else)\n\nfunction (condition, true, false, missing = NULL, ..., ptype = NULL, \n    size = NULL) \nNULL\n\n\nNote that condition is a logical expression which should yeild a true or false value for every row of the tibble. The variable true is the value to use if true, false the value to use if false, and missing the value to use if missing.\n\nint5 &lt;- -5:5\nif_else(int5&lt;0,\"-\",\"+\")\n\n [1] \"-\" \"-\" \"-\" \"-\" \"-\" \"+\" \"+\" \"+\" \"+\" \"+\" \"+\"\n\nif_else(int5&lt;0,-int5,int5) #Absolute value\n\n [1] 5 4 3 2 1 0 1 2 3 4 5\n\nna_if(int5,0)\n\n [1] -5 -4 -3 -2 -1 NA  1  2  3  4  5\n\nif_else(na_if(int5,0)&lt;0 ,\"-\",\"+\",\"0\")\n\n [1] \"-\" \"-\" \"-\" \"-\" \"-\" \"0\" \"+\" \"+\" \"+\" \"+\" \"+\"\n\n\nHere are the common logical tests:\n\n== – equals (don’t confuse this with = assignment.)\n!= – not equals\n&lt;, &lt;=, =&gt;, &gt; – less than, &c.\n! – Not (true if the rest of the expression is false)\nis.na() – True if the value is NA, false otherwise. (Also, !is.na())\n& – logical and (true when LHS and RHS are true)\n| – logical or (true if either LHS or RHS is true)\n%in% – True if value is in list.\n\n\ndrupes &lt;- c(\"Almond\",\"Cashew\",\"Walnut\")\nc(\"Peanut\",\"Almond\",\"Hazelnut\",\"Macademia\",\"Cashew\") %in% drupes\n\n[1] FALSE  TRUE FALSE FALSE  TRUE",
    "crumbs": [
      "Exploratory Data Analysis with GGplot"
    ]
  },
  {
    "objectID": "RIntro/EDAwithGGPlot.html#selecting-cases",
    "href": "RIntro/EDAwithGGPlot.html#selecting-cases",
    "title": "Exploratory Data Analysis with GGplot",
    "section": "Selecting Cases",
    "text": "Selecting Cases\nVery often instead of setting the value to NA, we just want to exclude that row from the data set.\nThe command filter() does this.\n\nstate77 %&gt;% filter(!(code %in% c(\"AK\",\"HI\")))\n\n# A tibble: 48 × 13\n   Population Income Illiteracy `Life Exp` Murder `HS Grad` Frost   Area region \n        &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;  \n 1       3615   3624        2.1       69.0   15.1      41.3    20  50708 South  \n 2       2212   4530        1.8       70.6    7.8      58.1    15 113417 West   \n 3       2110   3378        1.9       70.7   10.1      39.9    65  51945 South  \n 4      21198   5114        1.1       71.7   10.3      62.6    20 156361 West   \n 5       2541   4884        0.7       72.1    6.8      63.9   166 103766 West   \n 6       3100   5348        1.1       72.5    3.1      56     139   4862 Northe…\n 7        579   4809        0.9       70.1    6.2      54.6   103   1982 South  \n 8       8277   4815        1.3       70.7   10.7      52.6    11  54090 South  \n 9       4931   4091        2         68.5   13.9      40.6    60  58073 South  \n10        813   4119        0.6       71.9    5.3      59.5   126  82677 West   \n# ℹ 38 more rows\n# ℹ 4 more variables: name &lt;chr&gt;, code &lt;chr&gt;, center_x &lt;dbl&gt;, center_y &lt;dbl&gt;\n\n\nSometimes we want to temporarily remove the biggest values or the smallest values so we can see the details in a plot.\n\nstate77 %&gt;% select(name,Area) %&gt;% filter(Area &lt;200000)\n\n# A tibble: 48 × 2\n   name          Area\n   &lt;chr&gt;        &lt;dbl&gt;\n 1 Alabama      50708\n 2 Arizona     113417\n 3 Arkansas     51945\n 4 California  156361\n 5 Colorado    103766\n 6 Connecticut   4862\n 7 Delaware      1982\n 8 Florida      54090\n 9 Georgia      58073\n10 Hawaii        6425\n# ℹ 38 more rows\n\n\nSometimes we want to create subsets of the data that just have fewer cases.\nThe functions sample_frac() and sample_n() specify the size of the sample in fraction of the original data or absolute size.\nThe function slice() will select a contiguous range of cases, which is useful when looping through the data.",
    "crumbs": [
      "Exploratory Data Analysis with GGplot"
    ]
  },
  {
    "objectID": "RIntro/EDAwithGGPlot.html#calculating-summary-statistics",
    "href": "RIntro/EDAwithGGPlot.html#calculating-summary-statistics",
    "title": "Exploratory Data Analysis with GGplot",
    "section": "Calculating Summary Statistics",
    "text": "Calculating Summary Statistics\nPipe the output of the select and filter command into summarize():\n\nstate77 %&gt;% summarize(N=n(),Income=mean(Income),Population=mean(Population))\n\n# A tibble: 1 × 3\n      N Income Population\n  &lt;int&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1    50  4436.      4246.\n\n\nHere are some useful functions to use with summarize():\n\nn(), n_distinct(), sum(!is.na()) – Count, count of unique values, count of non-missing values.\nmean(), median() – Measures of center\nmin(), max(), quantile() – Position other than the center.\n\n\nstate77 %&gt;% select(Population) %&gt;% summarize(Min=min(Population),Q1=quantile(Population,.25),Q2=median(Population),Q3=quantile(Population,.75),Max=max(Population))\n\n# A tibble: 1 × 5\n    Min    Q1    Q2    Q3   Max\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   365 1080. 2838. 4968. 21198\n\n\n\nsd(), IQR(), mad() – measures of scale.\nsum(), prod() – Arithmetic\nsum(), any(), all() – Summarize logical expressions (count number true, true if all are true, true if any is true).\n\nAll of these functions have an optional argument na.rm. If there are NAs, you usually want to include na.rm=TRUE, as otherwise the value will be NA.",
    "crumbs": [
      "Exploratory Data Analysis with GGplot"
    ]
  },
  {
    "objectID": "RIntro/EDAwithGGPlot.html#summarizing-multiple-columns.",
    "href": "RIntro/EDAwithGGPlot.html#summarizing-multiple-columns.",
    "title": "Exploratory Data Analysis with GGplot",
    "section": "Summarizing Multiple columns.",
    "text": "Summarizing Multiple columns.\nOften, you want to do the same summary on several columns.\nThe function summarize_all() does that.\n\nstate77 %&gt;% select(Area,Population) %&gt;% summarize_all(mean,na.rm=TRUE)\n\n# A tibble: 1 × 2\n    Area Population\n   &lt;dbl&gt;      &lt;dbl&gt;\n1 70736.      4246.\n\n\nYou can use multiple statsitics by putting them in a list.\n\nstate77 %&gt;% select(Area,Population) %&gt;% summarize_all(list(mean=mean,sd=sd))\n\n# A tibble: 1 × 4\n  Area_mean Population_mean Area_sd Population_sd\n      &lt;dbl&gt;           &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;\n1    70736.           4246.  85327.         4464.\n\n\nThe function summarize_at() combines the select() and sumarize().\nThe function summarize_if() allows the selection of columns based on logical criteria.",
    "crumbs": [
      "Exploratory Data Analysis with GGplot"
    ]
  },
  {
    "objectID": "RIntro/EDAwithGGPlot.html#calculating-statistics-by-group",
    "href": "RIntro/EDAwithGGPlot.html#calculating-statistics-by-group",
    "title": "Exploratory Data Analysis with GGplot",
    "section": "Calculating Statistics by Group",
    "text": "Calculating Statistics by Group\nVery often we want to be to compare groups. We can use the function group_by() to split the data set by a factor variable.\n\nstate77 %&gt;% group_by(region) %&gt;% select(Area,Population) %&gt;% summarize_all(list(mean=mean,sd=sd))\n\nAdding missing grouping variables: `region`\n\n\n# A tibble: 4 × 5\n  region        Area_mean Population_mean Area_sd Population_sd\n  &lt;fct&gt;             &lt;dbl&gt;           &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;\n1 Northeast        18141            5495.  18076.         6080.\n2 South            54605.           4208.  57965.         2780.\n3 North Central    62652            4803   14967.         3703.\n4 West            134463            2915. 134982.         5579.\n\n\n\nstate77 %&gt;% group_by(region) %&gt;%\n  select(Area,Population) %&gt;%\n  summarise_all(list(Min=min,Q1=function(x){quantile(x,.25)},Q2=median,Q3=function(x){quantile(x,.75)},Max=max))\n\nAdding missing grouping variables: `region`\n\n\n# A tibble: 4 × 11\n  region     Area_Min Population_Min Area_Q1 Population_Q1 Area_Q2 Population_Q2\n  &lt;fct&gt;         &lt;dbl&gt;          &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;\n1 Northeast      1049            472   7521           931     9027         3100 \n2 South          1982            579  37294.         2622.   46113         3710.\n3 North Cen…    36097            637  55427          2096    62906         4255 \n4 West           6425            365  82677           746   103766         1144 \n# ℹ 4 more variables: Area_Q3 &lt;dbl&gt;, Population_Q3 &lt;dbl&gt;, Area_Max &lt;dbl&gt;,\n#   Population_Max &lt;dbl&gt;\n\n\n\nThe function(){} makes an anonymous function. This gets around the problem that quantile() needs two arguments, but summarize_all() expects a function of just one. In more recent versions of R, one can use \\(){} instead of function to create anonymous functions.\n\nThe cheat sheet.\nYou can find a handy list of dplyr and other tidyverse commands for manipulating data by selected “Help &gt; Cheat Sheets &gt; Data Mainpulation with dplyr” from the RStudio menu.\n\n\nGraphics\n\nMaking Histograms\n\nggplot(state77,aes(Population)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\n\nggplot(state77,aes(Population)) + geom_histogram(binwidth=500)\n\n\n\n\n\n\n\n\n\nggplot(state77,aes(Population)) + geom_histogram(bins=10)\n\n\n\n\n\n\n\n\n\nggplot(state77,aes(Population)) + geom_dotplot()\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n\n\n\nggplot(state77,aes(Population)) +geom_dotplot(binwidth=1000) +geom_density(aes(y=..scaled..))\n\nWarning: The dot-dot notation (`..scaled..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(scaled)` instead.\n\n\n\n\n\n\n\n\n\n\nggplot(state77,aes(Population)) +geom_histogram(binwidth=1000) +geom_density(aes(y=1000*..count..))\n\n\n\n\n\n\n\n\n\nggplot(state77,aes(Population)) +geom_histogram(binwidth=1000) +stat_function(fun= function(x) dnorm(x,mean=mean(state77$Population), sd=sd(state77$Population))*nrow(state77)*1000)\n\n\n\n\n\n\n\n\n\nbw &lt;- 1000\nggplot(state77,aes(Population)) + geom_histogram(aes(y=..density..),binwidth=bw) + \nstat_function(fun=dnorm, args=c(mean=mean(state77$Population), sd=sd(state77$Population))) +\nscale_y_continuous(\"Density\",sec.axis=sec_axis(trans = ~ . * bw * nrow(state77), name = \"Counts\"))\n\nWarning: The `trans` argument of `sec_axis()` is deprecated as of ggplot2 3.5.0.\nℹ Please use the `transform` argument instead.\n\n\n\n\n\n\n\n\n\n\n\nPanel Histograms by a Group\n\nggplot(state77,aes(Population)) + facet_grid(rows=vars(region)) + geom_dotplot()\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n\n\n\nggplot(state77,aes(Population)) + facet_grid(rows=vars(region)) + geom_dotplot(binwidth=750)+geom_density(aes(y=750*..count..))\n\n\n\n\n\n\n\n\n\n\nMaking Boxplots\n\nggplot(state77,aes(x=region,y=Population)) + geom_boxplot()\n\n\n\n\n\n\n\n\n\nggplot(state77,aes(x=region,y=Population)) + geom_violin()\n\n\n\n\n\n\n\n\n\nggplot(state77,aes(region,Population)) + geom_dotplot(binaxis=\"y\",stackdir=\"center\")\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\nSaving Your Work\n\nSaving Your Plots\n\nggsave(\"foo.png\")\n\nSaving 7 x 5 in image\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\nJust saved file.\n\n\n\n\nSaving Your Tables\n\nlibrary(xtable)\nprint(xtable(state77 %&gt;% group_by(region)%&gt;% select(Population,Area) %&gt;% summarize_all(list(mean=mean,sd=sd))),digits=3,type=\"html\",file=\"foo.html\")\n\nAdding missing grouping variables: `region`\n\n\nresult\n\n\nWorking in R Markdown",
    "crumbs": [
      "Exploratory Data Analysis with GGplot"
    ]
  },
  {
    "objectID": "RIntro/EDAwithGGPlot.html#the-cheat-sheet.",
    "href": "RIntro/EDAwithGGPlot.html#the-cheat-sheet.",
    "title": "Exploratory Data Analysis with GGplot",
    "section": "The cheat sheet.",
    "text": "The cheat sheet.\nYou can find a handy list of dplyr and other tidyverse commands for manipulating data by selected “Help &gt; Cheat Sheets &gt; Data Mainpulation with dplyr” from the RStudio menu.",
    "crumbs": [
      "Exploratory Data Analysis with GGplot"
    ]
  },
  {
    "objectID": "RIntro/EDAwithGGPlot.html#making-histograms",
    "href": "RIntro/EDAwithGGPlot.html#making-histograms",
    "title": "Exploratory Data Analysis with GGplot",
    "section": "Making Histograms",
    "text": "Making Histograms\n\nggplot(state77,aes(Population)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\n\nggplot(state77,aes(Population)) + geom_histogram(binwidth=500)\n\n\n\n\n\n\n\n\n\nggplot(state77,aes(Population)) + geom_histogram(bins=10)\n\n\n\n\n\n\n\n\n\nggplot(state77,aes(Population)) + geom_dotplot()\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n\n\n\nggplot(state77,aes(Population)) +geom_dotplot(binwidth=1000) +geom_density(aes(y=..scaled..))\n\nWarning: The dot-dot notation (`..scaled..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(scaled)` instead.\n\n\n\n\n\n\n\n\n\n\nggplot(state77,aes(Population)) +geom_histogram(binwidth=1000) +geom_density(aes(y=1000*..count..))\n\n\n\n\n\n\n\n\n\nggplot(state77,aes(Population)) +geom_histogram(binwidth=1000) +stat_function(fun= function(x) dnorm(x,mean=mean(state77$Population), sd=sd(state77$Population))*nrow(state77)*1000)\n\n\n\n\n\n\n\n\n\nbw &lt;- 1000\nggplot(state77,aes(Population)) + geom_histogram(aes(y=..density..),binwidth=bw) + \nstat_function(fun=dnorm, args=c(mean=mean(state77$Population), sd=sd(state77$Population))) +\nscale_y_continuous(\"Density\",sec.axis=sec_axis(trans = ~ . * bw * nrow(state77), name = \"Counts\"))\n\nWarning: The `trans` argument of `sec_axis()` is deprecated as of ggplot2 3.5.0.\nℹ Please use the `transform` argument instead.",
    "crumbs": [
      "Exploratory Data Analysis with GGplot"
    ]
  },
  {
    "objectID": "RIntro/EDAwithGGPlot.html#panel-histograms-by-a-group",
    "href": "RIntro/EDAwithGGPlot.html#panel-histograms-by-a-group",
    "title": "Exploratory Data Analysis with GGplot",
    "section": "Panel Histograms by a Group",
    "text": "Panel Histograms by a Group\n\nggplot(state77,aes(Population)) + facet_grid(rows=vars(region)) + geom_dotplot()\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n\n\n\nggplot(state77,aes(Population)) + facet_grid(rows=vars(region)) + geom_dotplot(binwidth=750)+geom_density(aes(y=750*..count..))",
    "crumbs": [
      "Exploratory Data Analysis with GGplot"
    ]
  },
  {
    "objectID": "RIntro/EDAwithGGPlot.html#making-boxplots",
    "href": "RIntro/EDAwithGGPlot.html#making-boxplots",
    "title": "Exploratory Data Analysis with GGplot",
    "section": "Making Boxplots",
    "text": "Making Boxplots\n\nggplot(state77,aes(x=region,y=Population)) + geom_boxplot()\n\n\n\n\n\n\n\n\n\nggplot(state77,aes(x=region,y=Population)) + geom_violin()\n\n\n\n\n\n\n\n\n\nggplot(state77,aes(region,Population)) + geom_dotplot(binaxis=\"y\",stackdir=\"center\")\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.",
    "crumbs": [
      "Exploratory Data Analysis with GGplot"
    ]
  },
  {
    "objectID": "RIntro/EDAwithGGPlot.html#saving-your-plots",
    "href": "RIntro/EDAwithGGPlot.html#saving-your-plots",
    "title": "Exploratory Data Analysis with GGplot",
    "section": "Saving Your Plots",
    "text": "Saving Your Plots\n\nggsave(\"foo.png\")\n\nSaving 7 x 5 in image\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\nJust saved file.",
    "crumbs": [
      "Exploratory Data Analysis with GGplot"
    ]
  },
  {
    "objectID": "RIntro/EDAwithGGPlot.html#saving-your-tables",
    "href": "RIntro/EDAwithGGPlot.html#saving-your-tables",
    "title": "Exploratory Data Analysis with GGplot",
    "section": "Saving Your Tables",
    "text": "Saving Your Tables\n\nlibrary(xtable)\nprint(xtable(state77 %&gt;% group_by(region)%&gt;% select(Population,Area) %&gt;% summarize_all(list(mean=mean,sd=sd))),digits=3,type=\"html\",file=\"foo.html\")\n\nAdding missing grouping variables: `region`\n\n\nresult",
    "crumbs": [
      "Exploratory Data Analysis with GGplot"
    ]
  },
  {
    "objectID": "RIntro/EDAwithGGPlot.html#working-in-r-markdown",
    "href": "RIntro/EDAwithGGPlot.html#working-in-r-markdown",
    "title": "Exploratory Data Analysis with GGplot",
    "section": "Working in R Markdown",
    "text": "Working in R Markdown",
    "crumbs": [
      "Exploratory Data Analysis with GGplot"
    ]
  },
  {
    "objectID": "RIntro/TidyStrings.html",
    "href": "RIntro/TidyStrings.html",
    "title": "String Hacks",
    "section": "",
    "text": "Tidyverse contains the stringr package.\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.2.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "String Hacks"
    ]
  },
  {
    "objectID": "RIntro/TidyStrings.html#making-file-names.",
    "href": "RIntro/TidyStrings.html#making-file-names.",
    "title": "String Hacks",
    "section": "Making file names.",
    "text": "Making file names.\n\nstr_c(\"data\",1:3,\".csv\")\n\n[1] \"data1.csv\" \"data2.csv\" \"data3.csv\"\n\nstr_c(\"dir\",\"subdir\",\"file.ext\",sep=\"/\")\n\n[1] \"dir/subdir/file.ext\"\n\nfile.path(\"dir\",\"subdir\",\"file.ext\")\n\n[1] \"dir/subdir/file.ext\"",
    "crumbs": [
      "String Hacks"
    ]
  },
  {
    "objectID": "RIntro/TidyStrings.html#regular-expressions.",
    "href": "RIntro/TidyStrings.html#regular-expressions.",
    "title": "String Hacks",
    "section": "Regular expressions.",
    "text": "Regular expressions.\n\nx &lt;- c(\"apple\", \"banana\", \"pear\")\nstr_view(x, \"an\")\n\n[2] │ b&lt;an&gt;&lt;an&gt;a\n\nstr_view(x, \"an\") -&gt; foo\nfoo\n\n[2] │ b&lt;an&gt;&lt;an&gt;a",
    "crumbs": [
      "String Hacks"
    ]
  },
  {
    "objectID": "RIntro/TidyStrings.html#match-strings-exactly-3-characters-long.",
    "href": "RIntro/TidyStrings.html#match-strings-exactly-3-characters-long.",
    "title": "String Hacks",
    "section": "Match strings exactly 3 characters long.",
    "text": "Match strings exactly 3 characters long.\n\nexp &lt;- c(\"a\",\"aa\",\"aaa\",\"aaaa\",\"aaaaa\")\nstr_view(exp,\"^...$\")\n\n[3] │ &lt;aaa&gt;",
    "crumbs": [
      "String Hacks"
    ]
  },
  {
    "objectID": "RIntro/TidyStrings.html#challenge-problem",
    "href": "RIntro/TidyStrings.html#challenge-problem",
    "title": "String Hacks",
    "section": "Challenge problem",
    "text": "Challenge problem\nThe answer to a stat question is \\(t = -.876\\). Suppose that the answer is to be counted correct:\n* No matter how many digits the person has. * No matter whether or not the person put a leading zero. * No matter whether or not the person included the minus sign. * Does not count rounding errors.\nWrite a regular expression that matches a character string (consisting of numbers) which “scores” this example.\n\nposex &lt;- c(\"-.876\",\"-.88\", \"-.9\",\"-0.9\",\"-0.876\",\n             \".876\",\".88\", \".9\",\"0.9\",\"0.876\",\n             \".87\",\".8\",\"- 0.9\")\nnegex &lt;- c(\"-.54\",\".33\",\".888\",\"-1.876\")\nstr_detect(posex,\".876?\")\n\n [1]  TRUE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE FALSE\n[13] FALSE\n\nstr_detect(negex,\".876?\")\n\n[1] FALSE FALSE FALSE  TRUE",
    "crumbs": [
      "String Hacks"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html",
    "href": "RIntro/RDataStructures.html",
    "title": "Basic R Data Structures",
    "section": "",
    "text": "S1 (qpe) – original version internal to Bell Labs\n\nFortran interface\nCopy on modify (preserve original data).\nData stored on disk.\n\nS2 (Becker, Chambers & Wilks, 1988; Blue Book).\n\nvector\nmatrix, array\nfunction\nplot\n\nS3 (Chambers & Hastie, 1992; White Book)\n\nlist\ndata.frame\nformula & lm\nclass() – Informal (S3) classes\nUseMethod – Informal Generic Functions\n\nSplus is commercial version of S\nR open source version of S\n\nInput from XLISP-STAT\nWorkspace, not disk storage.\n\nS4 (Chambers, 1998; Green Book)\n\nFormal Classes & Generic Function\nReally first implemented in R\n\nR6 (Chambers, 2016)\n\nReference classes (like c++, java, &c)",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#a-brief-history-of-s-splus-and-r",
    "href": "RIntro/RDataStructures.html#a-brief-history-of-s-splus-and-r",
    "title": "Basic R Data Structures",
    "section": "",
    "text": "S1 (qpe) – original version internal to Bell Labs\n\nFortran interface\nCopy on modify (preserve original data).\nData stored on disk.\n\nS2 (Becker, Chambers & Wilks, 1988; Blue Book).\n\nvector\nmatrix, array\nfunction\nplot\n\nS3 (Chambers & Hastie, 1992; White Book)\n\nlist\ndata.frame\nformula & lm\nclass() – Informal (S3) classes\nUseMethod – Informal Generic Functions\n\nSplus is commercial version of S\nR open source version of S\n\nInput from XLISP-STAT\nWorkspace, not disk storage.\n\nS4 (Chambers, 1998; Green Book)\n\nFormal Classes & Generic Function\nReally first implemented in R\n\nR6 (Chambers, 2016)\n\nReference classes (like c++, java, &c)",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#storage-modes",
    "href": "RIntro/RDataStructures.html#storage-modes",
    "title": "Basic R Data Structures",
    "section": "Storage modes",
    "text": "Storage modes",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#the-c-function",
    "href": "RIntro/RDataStructures.html#the-c-function",
    "title": "Basic R Data Structures",
    "section": "The c() function",
    "text": "The c() function",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#vectorized-operations",
    "href": "RIntro/RDataStructures.html#vectorized-operations",
    "title": "Basic R Data Structures",
    "section": "Vectorized operations",
    "text": "Vectorized operations",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#summary-operators",
    "href": "RIntro/RDataStructures.html#summary-operators",
    "title": "Basic R Data Structures",
    "section": "Summary operators",
    "text": "Summary operators",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#numerical-indexes",
    "href": "RIntro/RDataStructures.html#numerical-indexes",
    "title": "Basic R Data Structures",
    "section": "Numerical Indexes",
    "text": "Numerical Indexes\n\nAssignment & Indexes",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#negative-indexes",
    "href": "RIntro/RDataStructures.html#negative-indexes",
    "title": "Basic R Data Structures",
    "section": "Negative indexes",
    "text": "Negative indexes",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#logical-indexes",
    "href": "RIntro/RDataStructures.html#logical-indexes",
    "title": "Basic R Data Structures",
    "section": "Logical Indexes",
    "text": "Logical Indexes",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#the-if-statement",
    "href": "RIntro/RDataStructures.html#the-if-statement",
    "title": "Basic R Data Structures",
    "section": "The if statement",
    "text": "The if statement\n\nisTRUE()\n\n\nall and any",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#the-ifelse-function",
    "href": "RIntro/RDataStructures.html#the-ifelse-function",
    "title": "Basic R Data Structures",
    "section": "The ifelse function",
    "text": "The ifelse function",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#named-vectors",
    "href": "RIntro/RDataStructures.html#named-vectors",
    "title": "Basic R Data Structures",
    "section": "Named vectors",
    "text": "Named vectors",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#character-indexes",
    "href": "RIntro/RDataStructures.html#character-indexes",
    "title": "Basic R Data Structures",
    "section": "Character indexes",
    "text": "Character indexes",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#vector-exercises",
    "href": "RIntro/RDataStructures.html#vector-exercises",
    "title": "Basic R Data Structures",
    "section": "Vector exercises",
    "text": "Vector exercises",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#column-major-order",
    "href": "RIntro/RDataStructures.html#column-major-order",
    "title": "Basic R Data Structures",
    "section": "Column major order",
    "text": "Column major order",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#indexes",
    "href": "RIntro/RDataStructures.html#indexes",
    "title": "Basic R Data Structures",
    "section": "indexes",
    "text": "indexes",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#row-and-column-sums-apply-function",
    "href": "RIntro/RDataStructures.html#row-and-column-sums-apply-function",
    "title": "Basic R Data Structures",
    "section": "row and column sums, apply function",
    "text": "row and column sums, apply function",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#sweep-operator",
    "href": "RIntro/RDataStructures.html#sweep-operator",
    "title": "Basic R Data Structures",
    "section": "sweep operator",
    "text": "sweep operator",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#arrays",
    "href": "RIntro/RDataStructures.html#arrays",
    "title": "Basic R Data Structures",
    "section": "arrays",
    "text": "arrays",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#aperm",
    "href": "RIntro/RDataStructures.html#aperm",
    "title": "Basic R Data Structures",
    "section": "aperm",
    "text": "aperm",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#writing-functions",
    "href": "RIntro/RDataStructures.html#writing-functions",
    "title": "Basic R Data Structures",
    "section": "writing functions",
    "text": "writing functions",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#functions-as-arguments",
    "href": "RIntro/RDataStructures.html#functions-as-arguments",
    "title": "Basic R Data Structures",
    "section": "functions as arguments",
    "text": "functions as arguments",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#anonymous-functions",
    "href": "RIntro/RDataStructures.html#anonymous-functions",
    "title": "Basic R Data Structures",
    "section": "anonymous functions",
    "text": "anonymous functions",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#local-and-global-variables",
    "href": "RIntro/RDataStructures.html#local-and-global-variables",
    "title": "Basic R Data Structures",
    "section": "Local and global variables",
    "text": "Local and global variables",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#variable-scope",
    "href": "RIntro/RDataStructures.html#variable-scope",
    "title": "Basic R Data Structures",
    "section": "Variable Scope",
    "text": "Variable Scope",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#search-lists",
    "href": "RIntro/RDataStructures.html#search-lists",
    "title": "Basic R Data Structures",
    "section": "Search Lists",
    "text": "Search Lists",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#function-exercises",
    "href": "RIntro/RDataStructures.html#function-exercises",
    "title": "Basic R Data Structures",
    "section": "Function exercises",
    "text": "Function exercises",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#the-two-extraction-operators",
    "href": "RIntro/RDataStructures.html#the-two-extraction-operators",
    "title": "Basic R Data Structures",
    "section": "The two extraction operators",
    "text": "The two extraction operators",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#named-lists-dictionaries",
    "href": "RIntro/RDataStructures.html#named-lists-dictionaries",
    "title": "Basic R Data Structures",
    "section": "Named lists (dictionaries)",
    "text": "Named lists (dictionaries)",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#the-extractor",
    "href": "RIntro/RDataStructures.html#the-extractor",
    "title": "Basic R Data Structures",
    "section": "The $ extractor",
    "text": "The $ extractor",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/RDataStructures.html#the-for-loop",
    "href": "RIntro/RDataStructures.html#the-for-loop",
    "title": "Basic R Data Structures",
    "section": "The for loop",
    "text": "The for loop\n\nstate.x77\n\n               Population Income Illiteracy Life Exp Murder HS Grad Frost\nAlabama              3615   3624        2.1    69.05   15.1    41.3    20\nAlaska                365   6315        1.5    69.31   11.3    66.7   152\nArizona              2212   4530        1.8    70.55    7.8    58.1    15\nArkansas             2110   3378        1.9    70.66   10.1    39.9    65\nCalifornia          21198   5114        1.1    71.71   10.3    62.6    20\nColorado             2541   4884        0.7    72.06    6.8    63.9   166\nConnecticut          3100   5348        1.1    72.48    3.1    56.0   139\nDelaware              579   4809        0.9    70.06    6.2    54.6   103\nFlorida              8277   4815        1.3    70.66   10.7    52.6    11\nGeorgia              4931   4091        2.0    68.54   13.9    40.6    60\nHawaii                868   4963        1.9    73.60    6.2    61.9     0\nIdaho                 813   4119        0.6    71.87    5.3    59.5   126\nIllinois            11197   5107        0.9    70.14   10.3    52.6   127\nIndiana              5313   4458        0.7    70.88    7.1    52.9   122\nIowa                 2861   4628        0.5    72.56    2.3    59.0   140\nKansas               2280   4669        0.6    72.58    4.5    59.9   114\nKentucky             3387   3712        1.6    70.10   10.6    38.5    95\nLouisiana            3806   3545        2.8    68.76   13.2    42.2    12\nMaine                1058   3694        0.7    70.39    2.7    54.7   161\nMaryland             4122   5299        0.9    70.22    8.5    52.3   101\nMassachusetts        5814   4755        1.1    71.83    3.3    58.5   103\nMichigan             9111   4751        0.9    70.63   11.1    52.8   125\nMinnesota            3921   4675        0.6    72.96    2.3    57.6   160\nMississippi          2341   3098        2.4    68.09   12.5    41.0    50\nMissouri             4767   4254        0.8    70.69    9.3    48.8   108\nMontana               746   4347        0.6    70.56    5.0    59.2   155\nNebraska             1544   4508        0.6    72.60    2.9    59.3   139\nNevada                590   5149        0.5    69.03   11.5    65.2   188\nNew Hampshire         812   4281        0.7    71.23    3.3    57.6   174\nNew Jersey           7333   5237        1.1    70.93    5.2    52.5   115\nNew Mexico           1144   3601        2.2    70.32    9.7    55.2   120\nNew York            18076   4903        1.4    70.55   10.9    52.7    82\nNorth Carolina       5441   3875        1.8    69.21   11.1    38.5    80\nNorth Dakota          637   5087        0.8    72.78    1.4    50.3   186\nOhio                10735   4561        0.8    70.82    7.4    53.2   124\nOklahoma             2715   3983        1.1    71.42    6.4    51.6    82\nOregon               2284   4660        0.6    72.13    4.2    60.0    44\nPennsylvania        11860   4449        1.0    70.43    6.1    50.2   126\nRhode Island          931   4558        1.3    71.90    2.4    46.4   127\nSouth Carolina       2816   3635        2.3    67.96   11.6    37.8    65\nSouth Dakota          681   4167        0.5    72.08    1.7    53.3   172\nTennessee            4173   3821        1.7    70.11   11.0    41.8    70\nTexas               12237   4188        2.2    70.90   12.2    47.4    35\nUtah                 1203   4022        0.6    72.90    4.5    67.3   137\nVermont               472   3907        0.6    71.64    5.5    57.1   168\nVirginia             4981   4701        1.4    70.08    9.5    47.8    85\nWashington           3559   4864        0.6    71.72    4.3    63.5    32\nWest Virginia        1799   3617        1.4    69.48    6.7    41.6   100\nWisconsin            4589   4468        0.7    72.48    3.0    54.5   149\nWyoming               376   4566        0.6    70.29    6.9    62.9   173\n                 Area\nAlabama         50708\nAlaska         566432\nArizona        113417\nArkansas        51945\nCalifornia     156361\nColorado       103766\nConnecticut      4862\nDelaware         1982\nFlorida         54090\nGeorgia         58073\nHawaii           6425\nIdaho           82677\nIllinois        55748\nIndiana         36097\nIowa            55941\nKansas          81787\nKentucky        39650\nLouisiana       44930\nMaine           30920\nMaryland         9891\nMassachusetts    7826\nMichigan        56817\nMinnesota       79289\nMississippi     47296\nMissouri        68995\nMontana        145587\nNebraska        76483\nNevada         109889\nNew Hampshire    9027\nNew Jersey       7521\nNew Mexico     121412\nNew York        47831\nNorth Carolina  48798\nNorth Dakota    69273\nOhio            40975\nOklahoma        68782\nOregon          96184\nPennsylvania    44966\nRhode Island     1049\nSouth Carolina  30225\nSouth Dakota    75955\nTennessee       41328\nTexas          262134\nUtah            82096\nVermont          9267\nVirginia        39780\nWashington      66570\nWest Virginia   24070\nWisconsin       54464\nWyoming         97203",
    "crumbs": [
      "Basic R Data Structures"
    ]
  },
  {
    "objectID": "RIntro/WorkingWithRData.html",
    "href": "RIntro/WorkingWithRData.html",
    "title": "Working With R Data",
    "section": "",
    "text": "At the end of this lesson you should be able to\n\nMake vectors in R\nAccess parts of the vector using the [] operator.\n\nNumeric Indexes\nNegative Indexes\nLogical Indexes\nCharacter Indexes\n\nCheck types of object using is and mode functions.\nConvert types of object using as functions.\nAccess names elements of lists using $.\nAccess elements, row and columns of matrixes using [,]\nConvert between data frames and matrixes\nRead and write data frames using read.csv and write.csv.\n\nThis lesson covers the traditional R way of doing things. The next lesson will show tidyverse alternatives.",
    "crumbs": [
      "Working With R Data"
    ]
  },
  {
    "objectID": "RIntro/WorkingWithRData.html#basic-r-container-objects",
    "href": "RIntro/WorkingWithRData.html#basic-r-container-objects",
    "title": "Working With R Data",
    "section": "Basic R Container objects",
    "text": "Basic R Container objects\n\nVector – ordered collection of objects of the same storage mode ([ extract)\n\nNamed Vector – adds a names attribute (Can use names in subscripts)\nMatrix, Array – adds a dim and dimnames attribute\n\nList – ordered collection of objects of any type or mode ([[ extract)\n\nNamed List – add names attribute (Can use $ to extract elements)\nS3 Class – adds a class attribute\ndata.frame – a list of columns in a spreadsheet. Uses ([ or $ to extract).\ntibble – The tidyverse extension of a data frame.\n\nS4 Class – formal class mechanism. Uses @ instead of $.",
    "crumbs": [
      "Working With R Data"
    ]
  },
  {
    "objectID": "RIntro/WorkingWithRData.html#storage-modes.",
    "href": "RIntro/WorkingWithRData.html#storage-modes.",
    "title": "Working With R Data",
    "section": "Storage modes.",
    "text": "Storage modes.\nThe mode function in R refers to storage modes, not the mode of a distribution.\n\nmode(123)\n\n[1] \"numeric\"\n\nmode(123L)\n\n[1] \"numeric\"\n\nmode(TRUE)\n\n[1] \"logical\"\n\nmode(\"True\")\n\n[1] \"character\"\n\nmode(3.14)\n\n[1] \"numeric\"\n\nmode(t)\n\n[1] \"function\"\n\n?mode\n\n\nThe is.XXX functions can be used to check the type (mode or class) of an object.\nThe as.XXX functions can be used to convert between different types.\n\n\nis.integer(3)\n\n[1] FALSE\n\nis.integer(3L)\n\n[1] TRUE\n\nas.integer(3)\n\n[1] 3\n\nis.integer(as.integer(3))\n\n[1] TRUE\n\nas.integer(\"three\")\n\nWarning: NAs introduced by coercion\n\n\n[1] NA\n\nas.character(3)\n\n[1] \"3\"\n\nas.logical(3)\n\n[1] TRUE\n\n\nThe most commonly seen modes are:\n\nNumeric\n\nReal or double (the default)\nInteger (Putting an L after a number tells R that this should be an integer.)\nLogical (TRUE/T or FALSE/F)\n\nCharacter – Each element of a character vector is a string.\nAny – A vector of anything is a list; thus, almost all R objects are in fact vectors.",
    "crumbs": [
      "Working With R Data"
    ]
  },
  {
    "objectID": "RIntro/WorkingWithRData.html#factors",
    "href": "RIntro/WorkingWithRData.html#factors",
    "title": "Working With R Data",
    "section": "Factors",
    "text": "Factors\n\nThe factor and ordered classes also behave a lot like storage modes.\nAtucally, they are R classes where the data values are integers and there is a special property which gives the names of the levels.\nThe built-in data value state.region is a factor.\n\n*( The function head() lists the first 6 data points instead of all of them.)\n\nhead(state.region)\n\n[1] South West  West  South West  West \nLevels: Northeast South North Central West\n\nlevels(state.region)\n\n[1] \"Northeast\"     \"South\"         \"North Central\" \"West\"         \n\nhead(as.integer(state.region))\n\n[1] 2 4 4 2 4 4\n\nhead(as.character(state.region))\n\n[1] \"South\" \"West\"  \"West\"  \"South\" \"West\"  \"West\" \n\nunclass(state.region)\n\n [1] 2 4 4 2 4 4 1 2 2 2 4 4 3 3 3 3 2 2 1 2 1 3 3 2 3 4 3 4 1 1 4 1 2 3 3 2 4 1\n[39] 1 2 3 2 2 4 1 2 4 2 3 4\nattr(,\"levels\")\n[1] \"Northeast\"     \"South\"         \"North Central\" \"West\"         \n\n\n\nThe values of a factor variable are just labels,\n\nNumeric labels\n\nas.integer()\n\nString labels\n\nas.character()\n\n\nThe function as.factor() will force a numeric or character vector into a factor.\n\nR will just pick and arbitrary order (usually alphabetical) for labels.\nAlphabetical ordering doesn’t always work with as.ordered().\n\nHigh,Low,Medium\nUse the function ordered() with more control over the levels.\n\n\n\n\nhelp(ordered)\nofact &lt;- ordered(c(\"H\",\"M\",\"H\",\"L\",\"M\",\"H\"),levels=c(\"L\",\"M\",\"H\"))\nofact\n\n[1] H M H L M H\nLevels: L &lt; M &lt; H",
    "crumbs": [
      "Working With R Data"
    ]
  },
  {
    "objectID": "RIntro/WorkingWithRData.html#making-vectors",
    "href": "RIntro/WorkingWithRData.html#making-vectors",
    "title": "Working With R Data",
    "section": "Making vectors",
    "text": "Making vectors\nThe : operator produces sequences (of integers) between first and second argument. (The function seq() allows step sizes of other than one.)\n\n1:3\n\n[1] 1 2 3\n\n3:1\n\n[1] 3 2 1\n\n-1:1\n\n[1] -1  0  1\n\n-3:-1\n\n[1] -3 -2 -1\n\n\nThe c() function can be used to glue vectors together. (c stands for combine)\n\nc(1:3, 10:12)\n\n[1]  1  2  3 10 11 12\n\nc(\"Hansel\", \"Gretel\", \"Tedd\",\"Alice\")\n\n[1] \"Hansel\" \"Gretel\" \"Tedd\"   \"Alice\" \n\n\n\nImplicit Looping\nR implicitly loops over all the elements of a vector. Such implicit loops are faster than explicit for loops.\n\n1:11\n\n [1]  1  2  3  4  5  6  7  8  9 10 11\n\n(1:11)/2\n\n [1] 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5\n\nmean(1:11)\n\n[1] 6\n\nz &lt;- ((1:11) - mean(1:11))/sd(1:11)\nz\n\n [1] -1.5075567 -1.2060454 -0.9045340 -0.6030227 -0.3015113  0.0000000\n [7]  0.3015113  0.6030227  0.9045340  1.2060454  1.5075567\n\nmean (z)\n\n[1] 0\n\nsd(z)\n\n[1] 1\n\n\n\n\nRandom vectors\n\nR has a number of built in random number generators to generate random numbers.\nThe most commonly used are runif, rnorm and sample.\n\nSample has a replace option to do sampling with or without replacement.\n\nThere are also many others, with names that look like rXXX (try substituting chisq, t, beta, gamma, &c for XXX).\n\n\nrunif(5)\n\n[1] 0.87245658 0.54736140 0.60264401 0.05685799 0.32416803\n\nrnorm(10)\n\n [1] -0.008947004  0.332443648  2.882269315  0.040916437 -0.766231657\n [6]  0.127704650  0.102152957  0.427843943 -0.025908830 -0.192708404\n\nsample.int(5,5,replace=TRUE)\n\n[1] 4 3 2 1 3\n\n\n\n\nExercises\n\nGenerate 100 random numbers with mean 50 and standard deviation 25.\n\n1a. Use the result of the previous question to generate a random sample of size 101 with one outlier of 200.\n\nGenerate random integers between 0 and 100\n\n\nThe variable state.area contains the areas of the 50 US states (in alphabetical area). Create a random sample of size 10 of the state areas.",
    "crumbs": [
      "Working With R Data"
    ]
  },
  {
    "objectID": "RIntro/WorkingWithRData.html#three-ways-of-subscripting-a-vector",
    "href": "RIntro/WorkingWithRData.html#three-ways-of-subscripting-a-vector",
    "title": "Working With R Data",
    "section": "Three ways of subscripting a vector",
    "text": "Three ways of subscripting a vector\n\nThe [] operator is used to subscript vectors.\nThere are three different things you can put inside of the brackets:\n\nnumbers,\n\nnegative numbers (exclude values)\n\nlogical expressions\nnames (character values).\n\n\n\nNumeric Indexes\n\nNumbers are the most straightforward way to do indexing.\nR starts the indexes at 1 and it goes up to the length of the vector.\nThe function length() is useful in writing indexes.\nGiving multiple indexes with return a sub-vector (remember, there are no scalars in R, just vectors of length 1).\n\n\nint10 &lt;- 1:10\nint10[3]\n\n[1] 3\n\nint10[c(5:7,9)]\n\n[1] 5 6 7 9\n\nstate.area[c(1,length(state.area))]\n\n[1] 51609 97914\n\n\nAnother useful trick is to use negative indexes. These leave the numbered variables out.\n\nint10[-2]\n\n[1]  1  3  4  5  6  7  8  9 10\n\nint10[-(3:8)]\n\n[1]  1  2  9 10\n\n\nIndexing expressions can also be used on the LHS of assignment operators, to allow to assignment to just certain values.\n\nint10[3] &lt;- -3\nint10\n\n [1]  1  2 -3  4  5  6  7  8  9 10\n\n\n\n\nLogical Indexes\nThe second option for indexing is to use a logical vector the same length as the vector you are indexing.\n\nint10&lt;0\n\n [1] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\nint10[int10&lt;0]\n\n[1] -3\n\nint10[int10&lt;0] &lt;- abs (int10[int10&lt;0])\nint10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nBe careful with NAs.\n\nint55 &lt;- -5:5\nsqrt(int55) &lt; 1.2\n\nWarning in sqrt(int55): NaNs produced\n\n\n [1]    NA    NA    NA    NA    NA  TRUE  TRUE FALSE FALSE FALSE FALSE\n\nint55[sqrt(int55) &lt; 1.2]\n\nWarning in sqrt(int55): NaNs produced\n\n\n[1] NA NA NA NA NA  0  1\n\n\nThe real power of logical indexes comes when we have two vectors of the same length.\nFor example, state.abb gives the two letter postal codes of the states. Suppose we wanted to see all of the states that are bigger than average:\n\nstate.abb[state.area&gt;median(state.area)]\n\n [1] \"AK\" \"AZ\" \"CA\" \"CO\" \"FL\" \"GA\" \"ID\" \"IL\" \"IA\" \"KS\" \"MI\" \"MN\" \"MO\" \"MT\" \"NE\"\n[16] \"NV\" \"NM\" \"ND\" \"OK\" \"OR\" \"SD\" \"TX\" \"UT\" \"WA\" \"WY\"\n\n\n\n\nAside: ifelse and if\nThe built in language primitive if is not vectorized. It is expecting a single value. The code below will not do what you think it will.\n\ntry({\n  if (int55 &lt; 0) {\n    cat(\"Negative.\\n\")\n  } else { \n    cat(\"Non-negative.\\n\")\n  }\n})\n\nError in if (int55 &lt; 0) { : the condition has length &gt; 1\n\n\nThe functions any(), all() and isTRUE() are often useful here.\n\nif (all(int55 &gt;0)) {\n  cat(\"Positive.\\n\")\n} else { \n  cat(\"Not all positive.\\n\")\n}\n\nNot all positive.\n\n\nThe function ifelse() can be used to loop over if-else expressions.\n\nThere are two differences from if.\n\nFirst the condition is a logical vector.\nSecond, both the if-true and if-false argument are always evaluated, so they better not generate an error!\n\n\n\nifelse(int55&lt;0, \"-\",\"+\")\n\n [1] \"-\" \"-\" \"-\" \"-\" \"-\" \"+\" \"+\" \"+\" \"+\" \"+\" \"+\"\n\n\n\nintA &lt;- 1:10\nintA[3] &lt;- -3\nA&lt;- sqrt(intA)\n\nWarning in sqrt(intA): NaNs produced\n\nA\n\n [1] 1.000000 1.414214      NaN 2.000000 2.236068 2.449490 2.645751 2.828427\n [9] 3.000000 3.162278\n\nmean(A)\n\n[1] NaN\n\nmean(A,na.rm=TRUE)\n\n[1] 2.304025\n\n\n\n\nNames and character indexes\nIt would be really convenient if we could access the state data by name.\nFlorida is the 9 state alphabetically, but I can’t remember that.\nWhat we can do is add names to a vector. Then we can select by name.\n\nnames(state.area) &lt;- state.abb\nhead(state.area)\n\n    AL     AK     AZ     AR     CA     CO \n 51609 589757 113909  53104 158693 104247 \n\nhead(names(state.area))\n\n[1] \"AL\" \"AK\" \"AZ\" \"AR\" \"CA\" \"CO\"\n\nstate.area[\"FL\"]\n\n   FL \n58560 \n\nstate.area[c(\"NY\",\"CA\")]\n\n    NY     CA \n 49576 158693 \n\n\nSometimes we need to make up names.\nThe paste() command is handy for that.\nIt is vectorized, so you can put a bunch of numbers in.\n\npaste(\"Student\",1:5,sep=\"_\")\n\n[1] \"Student_1\" \"Student_2\" \"Student_3\" \"Student_4\" \"Student_5\"\n\n\n\n\nExercises\n\nWrite an expression that removes the outlier from the data you generated for 1b.\n\n\nSuppose the data you generated for problem 1 was suppose to have a minimum score of 0 and a maximum score of 100. Fix, the data set so that all of the values are between 0 and 100.\n\n\nFix my positive/negative test, so that it has a 0 as well\n\n\nFind all of the states that are bigger than Florida.\n\n\nGenerate a bunch of random integers between -10 and 10. Then turn all negative integers into NA.",
    "crumbs": [
      "Working With R Data"
    ]
  },
  {
    "objectID": "RIntro/WorkingWithRData.html#matrixes-and-arrays",
    "href": "RIntro/WorkingWithRData.html#matrixes-and-arrays",
    "title": "Working With R Data",
    "section": "Matrixes and Arrays",
    "text": "Matrixes and Arrays\n\nA matrix is an object with rows and columns.\nAn array can have any number of dimensions.\nBut they all the entries need to be the same type (mode).\nThere is a dim() attribute which shows the dimensions of the matrix.\n\n\ndim(state.x77)\n\n[1] 50  8\n\nhead(state.x77)\n\n           Population Income Illiteracy Life Exp Murder HS Grad Frost   Area\nAlabama          3615   3624        2.1    69.05   15.1    41.3    20  50708\nAlaska            365   6315        1.5    69.31   11.3    66.7   152 566432\nArizona          2212   4530        1.8    70.55    7.8    58.1    15 113417\nArkansas         2110   3378        1.9    70.66   10.1    39.9    65  51945\nCalifornia      21198   5114        1.1    71.71   10.3    62.6    20 156361\nColorado         2541   4884        0.7    72.06    6.8    63.9   166 103766\n\n\n\nGetting and setting dims\n\nThe dim() function is used to access the number of rows and columns.\ndim()[1] gets the number of rows\ndim()[2] gets the number of columns.\nFor matrixes, the functions nrow() and ncol() are easier to remember.\n\nSetting dim() will reshape a vector into a matrix or array.\n\nnrow(state.x77)\n\n[1] 50\n\nncol(state.x77)\n\n[1] 8\n\nint12 &lt;- 1:12\ndim(int12) &lt;- c(3,4)\nint12\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\n\n\n\nmatrix() and array() functions\n\nSetting the dim() attribute directly is not recommended (makes for hard to read code).\nInstead use matrix() or array()\nR stores matrixes in row major order (like FORTRAN, not like c).\n\nUse byrow=TRUE to reverse this in matrix or array\n\n\n\nmatrix(1:12,3,4)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\nmatrix(1:12,3,4,byrow=TRUE)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n\narray(1:24,c(2,3,4))\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]    7    9   11\n[2,]    8   10   12\n\n, , 3\n\n     [,1] [,2] [,3]\n[1,]   13   15   17\n[2,]   14   16   18\n\n, , 4\n\n     [,1] [,2] [,3]\n[1,]   19   21   23\n[2,]   20   22   24\n\n\n\n\nNumeric and logical indexes\nFor matrixes and arrays, the [] operator does something a little bit different. In particular, x[i,j] picks out row \\(i\\) and column \\(j\\).\nEither the row or column selector could be\n\nA number or vector of numbers (pick those rows or columns)\nA negative number of vector of negative numbers (excluded those rows or columns)\nA logical vector of size nrow(x) or ncol(x) (select the rows/columns corresponding to true).\nA character vector (select rows or columns by name, see below).\nLeft blank, in which case all rows/columns are selected.\n\nIf a single row or column is selected, then it turns into a vector.\n\nstate.x77[1:5,1:5]\n\n           Population Income Illiteracy Life Exp Murder\nAlabama          3615   3624        2.1    69.05   15.1\nAlaska            365   6315        1.5    69.31   11.3\nArizona          2212   4530        1.8    70.55    7.8\nArkansas         2110   3378        1.9    70.66   10.1\nCalifornia      21198   5114        1.1    71.71   10.3\n\nstate.x77[1:5,]\n\n           Population Income Illiteracy Life Exp Murder HS Grad Frost   Area\nAlabama          3615   3624        2.1    69.05   15.1    41.3    20  50708\nAlaska            365   6315        1.5    69.31   11.3    66.7   152 566432\nArizona          2212   4530        1.8    70.55    7.8    58.1    15 113417\nArkansas         2110   3378        1.9    70.66   10.1    39.9    65  51945\nCalifornia      21198   5114        1.1    71.71   10.3    62.6    20 156361\n\nstate.x77[9,]\n\nPopulation     Income Illiteracy   Life Exp     Murder    HS Grad      Frost \n   8277.00    4815.00       1.30      70.66      10.70      52.60      11.00 \n      Area \n  54090.00 \n\ndim(state.x77[9,])\n\nNULL\n\nhead(state.x77[,3])\n\n   Alabama     Alaska    Arizona   Arkansas California   Colorado \n       2.1        1.5        1.8        1.9        1.1        0.7 \n\nstate.x77[9,,drop=FALSE]\n\n        Population Income Illiteracy Life Exp Murder HS Grad Frost  Area\nFlorida       8277   4815        1.3    70.66   10.7    52.6    11 54090\n\ndim(state.x77[9,,drop=FALSE])\n\n[1] 1 8\n\n\n\n\ndimnames and character indexes\nTo use character indexes with matrixes, we need to set the rownames() and colnames() of the matrix.\nWe can also use the dimnames() (although this will produce a list).\n\nrownames(state.x77)\n\n [1] \"Alabama\"        \"Alaska\"         \"Arizona\"        \"Arkansas\"      \n [5] \"California\"     \"Colorado\"       \"Connecticut\"    \"Delaware\"      \n [9] \"Florida\"        \"Georgia\"        \"Hawaii\"         \"Idaho\"         \n[13] \"Illinois\"       \"Indiana\"        \"Iowa\"           \"Kansas\"        \n[17] \"Kentucky\"       \"Louisiana\"      \"Maine\"          \"Maryland\"      \n[21] \"Massachusetts\"  \"Michigan\"       \"Minnesota\"      \"Mississippi\"   \n[25] \"Missouri\"       \"Montana\"        \"Nebraska\"       \"Nevada\"        \n[29] \"New Hampshire\"  \"New Jersey\"     \"New Mexico\"     \"New York\"      \n[33] \"North Carolina\" \"North Dakota\"   \"Ohio\"           \"Oklahoma\"      \n[37] \"Oregon\"         \"Pennsylvania\"   \"Rhode Island\"   \"South Carolina\"\n[41] \"South Dakota\"   \"Tennessee\"      \"Texas\"          \"Utah\"          \n[45] \"Vermont\"        \"Virginia\"       \"Washington\"     \"West Virginia\" \n[49] \"Wisconsin\"      \"Wyoming\"       \n\ncolnames(state.x77)\n\n[1] \"Population\" \"Income\"     \"Illiteracy\" \"Life Exp\"   \"Murder\"    \n[6] \"HS Grad\"    \"Frost\"      \"Area\"      \n\ndimnames(state.x77)\n\n[[1]]\n [1] \"Alabama\"        \"Alaska\"         \"Arizona\"        \"Arkansas\"      \n [5] \"California\"     \"Colorado\"       \"Connecticut\"    \"Delaware\"      \n [9] \"Florida\"        \"Georgia\"        \"Hawaii\"         \"Idaho\"         \n[13] \"Illinois\"       \"Indiana\"        \"Iowa\"           \"Kansas\"        \n[17] \"Kentucky\"       \"Louisiana\"      \"Maine\"          \"Maryland\"      \n[21] \"Massachusetts\"  \"Michigan\"       \"Minnesota\"      \"Mississippi\"   \n[25] \"Missouri\"       \"Montana\"        \"Nebraska\"       \"Nevada\"        \n[29] \"New Hampshire\"  \"New Jersey\"     \"New Mexico\"     \"New York\"      \n[33] \"North Carolina\" \"North Dakota\"   \"Ohio\"           \"Oklahoma\"      \n[37] \"Oregon\"         \"Pennsylvania\"   \"Rhode Island\"   \"South Carolina\"\n[41] \"South Dakota\"   \"Tennessee\"      \"Texas\"          \"Utah\"          \n[45] \"Vermont\"        \"Virginia\"       \"Washington\"     \"West Virginia\" \n[49] \"Wisconsin\"      \"Wyoming\"       \n\n[[2]]\n[1] \"Population\" \"Income\"     \"Illiteracy\" \"Life Exp\"   \"Murder\"    \n[6] \"HS Grad\"    \"Frost\"      \"Area\"      \n\nrownames(state.x77) &lt;- state.abb\nhead(state.x77)\n\n   Population Income Illiteracy Life Exp Murder HS Grad Frost   Area\nAL       3615   3624        2.1    69.05   15.1    41.3    20  50708\nAK        365   6315        1.5    69.31   11.3    66.7   152 566432\nAZ       2212   4530        1.8    70.55    7.8    58.1    15 113417\nAR       2110   3378        1.9    70.66   10.1    39.9    65  51945\nCA      21198   5114        1.1    71.71   10.3    62.6    20 156361\nCO       2541   4884        0.7    72.06    6.8    63.9   166 103766\n\n\n\n\nRow and column sums and averages\nRemember that a matrix is just a vector with a dim attribute. Consequently, mean and other summary functions don’t do what we want:\n\nmean(state.x77)\n\n[1] 9956.887\n\nsd(state.x77)\n\n[1] 37801.78\n\nvar(state.x77)\n\n              Population        Income   Illiteracy      Life Exp       Murder\nPopulation 19931683.7588   571229.7796  292.8679592 -4.078425e+02  5663.523714\nIncome       571229.7796   377573.3061 -163.7020408  2.806632e+02  -521.894286\nIlliteracy      292.8680     -163.7020    0.3715306 -4.815122e-01     1.581776\nLife Exp       -407.8425      280.6632   -0.4815122  1.802020e+00    -3.869480\nMurder         5663.5237     -521.8943    1.5817755 -3.869480e+00    13.627465\nHS Grad       -3551.5096     3076.7690   -3.2354694  6.312685e+00   -14.549616\nFrost        -77081.9727     7227.6041  -21.2900000  1.828678e+01  -103.406000\nArea        8587916.9494 19049013.7510 4018.3371429 -1.229410e+04 71940.429959\n                 HS Grad        Frost          Area\nPopulation  -3551.509551 -77081.97265  8.587917e+06\nIncome       3076.768980   7227.60408  1.904901e+07\nIlliteracy     -3.235469    -21.29000  4.018337e+03\nLife Exp        6.312685     18.28678 -1.229410e+04\nMurder        -14.549616   -103.40600  7.194043e+04\nHS Grad        65.237894    153.99216  2.298732e+05\nFrost         153.992163   2702.00857  2.627039e+05\nArea       229873.192816 262703.89306  7.280748e+09\n\ncor(state.x77)\n\n            Population     Income  Illiteracy    Life Exp     Murder\nPopulation  1.00000000  0.2082276  0.10762237 -0.06805195  0.3436428\nIncome      0.20822756  1.0000000 -0.43707519  0.34025534 -0.2300776\nIlliteracy  0.10762237 -0.4370752  1.00000000 -0.58847793  0.7029752\nLife Exp   -0.06805195  0.3402553 -0.58847793  1.00000000 -0.7808458\nMurder      0.34364275 -0.2300776  0.70297520 -0.78084575  1.0000000\nHS Grad    -0.09848975  0.6199323 -0.65718861  0.58221620 -0.4879710\nFrost      -0.33215245  0.2262822 -0.67194697  0.26206801 -0.5388834\nArea        0.02254384  0.3633154  0.07726113 -0.10733194  0.2283902\n               HS Grad      Frost        Area\nPopulation -0.09848975 -0.3321525  0.02254384\nIncome      0.61993232  0.2262822  0.36331544\nIlliteracy -0.65718861 -0.6719470  0.07726113\nLife Exp    0.58221620  0.2620680 -0.10733194\nMurder     -0.48797102 -0.5388834  0.22839021\nHS Grad     1.00000000  0.3667797  0.33354187\nFrost       0.36677970  1.0000000  0.05922910\nArea        0.33354187  0.0592291  1.00000000\n\n\nTaking row and column sums are such a frequent operation, that there is a shortcut for them: rowSums(), colSums(), rowMeans(), colMeans()\n\ncolMeans(state.x77)\n\nPopulation     Income Illiteracy   Life Exp     Murder    HS Grad      Frost \n 4246.4200  4435.8000     1.1700    70.8786     7.3780    53.1080   104.4600 \n      Area \n70735.8800 \n\n\nThe summary function in the tidyverse package is another way to do this.\n\n\nExercises:\n\nFind the population for all states whose area is bigger than Florida’s.\n\n\nCalculate the population density (population per area) for each state\n\n\nTurn the state.x77 data into z-scores by subtracting the column means and dividing by the column standard deviations.\n\n\nScale the state.x77 data from 0 (minimum in the column) to 1 (maximum in the column).",
    "crumbs": [
      "Working With R Data"
    ]
  },
  {
    "objectID": "RIntro/WorkingWithRData.html#lists",
    "href": "RIntro/WorkingWithRData.html#lists",
    "title": "Working With R Data",
    "section": "Lists",
    "text": "Lists\n\nA list in R is a special vector whose elements can be anything, even other lists.\nIt is possible to build up quite complex objects from lists (Old S3 class system.)\n\nUse the `list() constructor to make lists\n\nlist(1,2:3,\"four\",quote(2+3))\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2 3\n\n[[3]]\n[1] \"four\"\n\n[[4]]\n2 + 3\n\n\nNotice that the second element is a vector and the last element is an R expression (this is what quote does). R lists are quite flexible.\n\nNotice that the list is show with a double square bracket [[ instead of a single one [. This is because with lists the extraction operators behave a little bit differently. The single bracket refers to a sublist, and the double bracket to the element. Fortunately, this doesn’t come up a lot at the beginning because, most people use the $ extractors instead.",
    "crumbs": [
      "Working With R Data"
    ]
  },
  {
    "objectID": "RIntro/WorkingWithRData.html#named-lists-and-extraction",
    "href": "RIntro/WorkingWithRData.html#named-lists-and-extraction",
    "title": "Working With R Data",
    "section": "Named Lists and $ extraction",
    "text": "Named Lists and $ extraction\nNamed lists have a special role in R. They are similar to environments in that they allow the analyst to associate names and values. If x is a list then x[[name]] or x$name will retrieve (or set if used with &lt;-) that element.\n\nalist &lt;- list(one=1, two=2:3, three=\"three\", four=quote(2+2))\nalist\n\n$one\n[1] 1\n\n$two\n[1] 2 3\n\n$three\n[1] \"three\"\n\n$four\n2 + 2\n\nalist$two\n\n[1] 2 3\n\nalist$two &lt;- 2\nalist\n\n$one\n[1] 1\n\n$two\n[1] 2\n\n$three\n[1] \"three\"\n\n$four\n2 + 2",
    "crumbs": [
      "Working With R Data"
    ]
  },
  {
    "objectID": "RIntro/WorkingWithRData.html#lists-and-classes",
    "href": "RIntro/WorkingWithRData.html#lists-and-classes",
    "title": "Working With R Data",
    "section": "Lists and Classes",
    "text": "Lists and Classes\nThis ability to associate names and values is very hand. The older S3 (informal) class system just uses lists with appropriate values as classes. To get components, just use the $ operator.\nFor example, the function lm() does a regression and returns an object of class lm. The $ operator can be used to access its components.\n\nfit1 &lt;- lm(dist~speed,data=cars)\nfit1$coefficients\n\n(Intercept)       speed \n -17.579095    3.932409",
    "crumbs": [
      "Working With R Data"
    ]
  },
  {
    "objectID": "RIntro/WorkingWithRData.html#data-frame",
    "href": "RIntro/WorkingWithRData.html#data-frame",
    "title": "Working With R Data",
    "section": "Data frame",
    "text": "Data frame\n\nA data frame is a list that behaves like a matrix.\n\nA data frame is a list of columns with a class of data.frame.\n\nDifferent columns can have different classes or storage modes.\n\nMatrixes and arrays all must be the same kind of value.\n\nUsing the single square bracket [i,j] can reference row i and column j, like a matrix.\nUsing the $ operator can reference columns.\n\n\n?mtcars\nnames(mtcars)  # Get the variable names\n\n [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n[11] \"carb\"\n\nrownames(mtcars) # Get the car names\n\n [1] \"Mazda RX4\"           \"Mazda RX4 Wag\"       \"Datsun 710\"         \n [4] \"Hornet 4 Drive\"      \"Hornet Sportabout\"   \"Valiant\"            \n [7] \"Duster 360\"          \"Merc 240D\"           \"Merc 230\"           \n[10] \"Merc 280\"            \"Merc 280C\"           \"Merc 450SE\"         \n[13] \"Merc 450SL\"          \"Merc 450SLC\"         \"Cadillac Fleetwood\" \n[16] \"Lincoln Continental\" \"Chrysler Imperial\"   \"Fiat 128\"           \n[19] \"Honda Civic\"         \"Toyota Corolla\"      \"Toyota Corona\"      \n[22] \"Dodge Challenger\"    \"AMC Javelin\"         \"Camaro Z28\"         \n[25] \"Pontiac Firebird\"    \"Fiat X1-9\"           \"Porsche 914-2\"      \n[28] \"Lotus Europa\"        \"Ford Pantera L\"      \"Ferrari Dino\"       \n[31] \"Maserati Bora\"       \"Volvo 142E\"         \n\nmtcars[1:5,] # First five rows\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n\nmtcars[\"Honda Civic\", ]  # Just one car\n\n             mpg cyl disp hp drat    wt  qsec vs am gear carb\nHonda Civic 30.4   4 75.7 52 4.93 1.615 18.52  1  1    4    2\n\nmtcars[,\"mpg\"] # Just MPG variable\n\n [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4\n[16] 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7\n[31] 15.0 21.4\n\nmtcars$disp  # Just DISP variable\n\n [1] 160.0 160.0 108.0 258.0 360.0 225.0 360.0 146.7 140.8 167.6 167.6 275.8\n[13] 275.8 275.8 472.0 460.0 440.0  78.7  75.7  71.1 120.1 318.0 304.0 350.0\n[25] 400.0  79.0 120.3  95.1 351.0 145.0 301.0 121.0",
    "crumbs": [
      "Working With R Data"
    ]
  },
  {
    "objectID": "RIntro/WorkingWithRData.html#data.frame-as.matrix-and-as.data.frame",
    "href": "RIntro/WorkingWithRData.html#data.frame-as.matrix-and-as.data.frame",
    "title": "Working With R Data",
    "section": "data.frame(), as.matrix and as.data.frame",
    "text": "data.frame(), as.matrix and as.data.frame\nThe function data.frame() will put a data frame together column by column. (If one of the arguments is a matrix each column in the matrix will become a column in the data frame.)\n\nstateX77 &lt;- data.frame(state.x77,region=state.region,row.names=state.abb)\nstateX77\n\n   Population Income Illiteracy Life.Exp Murder HS.Grad Frost   Area\nAL       3615   3624        2.1    69.05   15.1    41.3    20  50708\nAK        365   6315        1.5    69.31   11.3    66.7   152 566432\nAZ       2212   4530        1.8    70.55    7.8    58.1    15 113417\nAR       2110   3378        1.9    70.66   10.1    39.9    65  51945\nCA      21198   5114        1.1    71.71   10.3    62.6    20 156361\nCO       2541   4884        0.7    72.06    6.8    63.9   166 103766\nCT       3100   5348        1.1    72.48    3.1    56.0   139   4862\nDE        579   4809        0.9    70.06    6.2    54.6   103   1982\nFL       8277   4815        1.3    70.66   10.7    52.6    11  54090\nGA       4931   4091        2.0    68.54   13.9    40.6    60  58073\nHI        868   4963        1.9    73.60    6.2    61.9     0   6425\nID        813   4119        0.6    71.87    5.3    59.5   126  82677\nIL      11197   5107        0.9    70.14   10.3    52.6   127  55748\nIN       5313   4458        0.7    70.88    7.1    52.9   122  36097\nIA       2861   4628        0.5    72.56    2.3    59.0   140  55941\nKS       2280   4669        0.6    72.58    4.5    59.9   114  81787\nKY       3387   3712        1.6    70.10   10.6    38.5    95  39650\nLA       3806   3545        2.8    68.76   13.2    42.2    12  44930\nME       1058   3694        0.7    70.39    2.7    54.7   161  30920\nMD       4122   5299        0.9    70.22    8.5    52.3   101   9891\nMA       5814   4755        1.1    71.83    3.3    58.5   103   7826\nMI       9111   4751        0.9    70.63   11.1    52.8   125  56817\nMN       3921   4675        0.6    72.96    2.3    57.6   160  79289\nMS       2341   3098        2.4    68.09   12.5    41.0    50  47296\nMO       4767   4254        0.8    70.69    9.3    48.8   108  68995\nMT        746   4347        0.6    70.56    5.0    59.2   155 145587\nNE       1544   4508        0.6    72.60    2.9    59.3   139  76483\nNV        590   5149        0.5    69.03   11.5    65.2   188 109889\nNH        812   4281        0.7    71.23    3.3    57.6   174   9027\nNJ       7333   5237        1.1    70.93    5.2    52.5   115   7521\nNM       1144   3601        2.2    70.32    9.7    55.2   120 121412\nNY      18076   4903        1.4    70.55   10.9    52.7    82  47831\nNC       5441   3875        1.8    69.21   11.1    38.5    80  48798\nND        637   5087        0.8    72.78    1.4    50.3   186  69273\nOH      10735   4561        0.8    70.82    7.4    53.2   124  40975\nOK       2715   3983        1.1    71.42    6.4    51.6    82  68782\nOR       2284   4660        0.6    72.13    4.2    60.0    44  96184\nPA      11860   4449        1.0    70.43    6.1    50.2   126  44966\nRI        931   4558        1.3    71.90    2.4    46.4   127   1049\nSC       2816   3635        2.3    67.96   11.6    37.8    65  30225\nSD        681   4167        0.5    72.08    1.7    53.3   172  75955\nTN       4173   3821        1.7    70.11   11.0    41.8    70  41328\nTX      12237   4188        2.2    70.90   12.2    47.4    35 262134\nUT       1203   4022        0.6    72.90    4.5    67.3   137  82096\nVT        472   3907        0.6    71.64    5.5    57.1   168   9267\nVA       4981   4701        1.4    70.08    9.5    47.8    85  39780\nWA       3559   4864        0.6    71.72    4.3    63.5    32  66570\nWV       1799   3617        1.4    69.48    6.7    41.6   100  24070\nWI       4589   4468        0.7    72.48    3.0    54.5   149  54464\nWY        376   4566        0.6    70.29    6.9    62.9   173  97203\n          region\nAL         South\nAK          West\nAZ          West\nAR         South\nCA          West\nCO          West\nCT     Northeast\nDE         South\nFL         South\nGA         South\nHI          West\nID          West\nIL North Central\nIN North Central\nIA North Central\nKS North Central\nKY         South\nLA         South\nME     Northeast\nMD         South\nMA     Northeast\nMI North Central\nMN North Central\nMS         South\nMO North Central\nMT          West\nNE North Central\nNV          West\nNH     Northeast\nNJ     Northeast\nNM          West\nNY     Northeast\nNC         South\nND North Central\nOH North Central\nOK         South\nOR          West\nPA     Northeast\nRI     Northeast\nSC         South\nSD North Central\nTN         South\nTX         South\nUT          West\nVT     Northeast\nVA         South\nWA          West\nWV         South\nWI North Central\nWY          West\n\nstateX77$Income\n\n [1] 3624 6315 4530 3378 5114 4884 5348 4809 4815 4091 4963 4119 5107 4458 4628\n[16] 4669 3712 3545 3694 5299 4755 4751 4675 3098 4254 4347 4508 5149 4281 5237\n[31] 3601 4903 3875 5087 4561 3983 4660 4449 4558 3635 4167 3821 4188 4022 3907\n[46] 4701 4864 3617 4468 4566\n\n\nThe functions as.data.frame() and as.matrix() can be used to go back and forth between the two different representations.\n\nAll matrixes can be converted to data frames, but data frames can only be converted to matrixes if all of the variables are the same type.\nThere are certain mathematical operators (like taking the inverse) which only work on matrixes.\n\nFor most of what I do in R, the data frame is the most convenient representation for data.\nThe tidyverse package uses the tibble instead of the data.frame. A tibble is a new class for data frames which has slightly more intelligence printing and more consistent subseting behavior.",
    "crumbs": [
      "Working With R Data"
    ]
  },
  {
    "objectID": "RIntro/WorkingWithRData.html#read.table-and-read.csv",
    "href": "RIntro/WorkingWithRData.html#read.table-and-read.csv",
    "title": "Working With R Data",
    "section": "read.table and read.csv",
    "text": "read.table and read.csv\nMost common format for storing data is tab separated value (.dat) and comma separated value (.csv).\n\nCases are rows\nVariables are separated by tab or comma\nOften a header row giving variable names\nSometimes there are row names.\nSometimes quotes are used for strings\n\nThe functions read.table() and read.csv() read these data files and produce data frames. * Really the same function with different options. * Many options, look at the help!!\n\nhelp(read.table)\n\nThese functions automatically convert strings to factors. The as.is optional argument suppresses that. Often factors, dates and missing values need to be cleaned up after reading in the data. (More about this in the next lesson).\nWindows Only. Usually both .dat and .csv files are mapped to open in Excel when you double click on them. If the file is open in Excel, then Windows will lock the file and not let another program read it. You may need to close the file in Excel before you can read it into R.\nThe functions write.table() and write.csv() go in the opposite directions.\nThe tidyverse alternative is read_csv(). It might be somewhat easier to use, but it produces tibbles instead of data frames. More about this in the next session.",
    "crumbs": [
      "Working With R Data"
    ]
  },
  {
    "objectID": "RIntro/WorkingWithRData.html#foreign-interfaces",
    "href": "RIntro/WorkingWithRData.html#foreign-interfaces",
    "title": "Working With R Data",
    "section": "Foreign interfaces",
    "text": "Foreign interfaces\nR can read data from an other packages, but you need to load the foreign package first.\n\nlibrary(foreign) (Part of the base R distribution)\n\nread.spss (SPSS)\nread.dta (Stata)\nread.ssd (SAS)\nread.systat (Systat)\n\n\nExcel workbooks are another common format. The easiest way to work with Excel data is to save it in .csv format from Excel. You could also try the xlsx package (need to install it first).\n\nlibrary(xlsx) (Need to install from CRAN)\n\nread.xlsx (Excel)\n\n\nThe book R for Data Science (Grolemund and Wickham, 2017) recommends the haven and readxl packages. Also, the DBI package allows importing data directly from databases (an advanced R trick).\n\nExercises\nUse the function write.csv() to write out the stateX77 data we made. Read it into Excel (or another spreadsheet) make some changes. Now read the modified version back into R.",
    "crumbs": [
      "Working With R Data"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "These are a series of small shiny apps I developed for teaching. Please feel free to use them in your courses or for your personal learning.\nAs learning from my R code is part of the learning, the source for these pages can be found on my github account: github:ralmond/TeachingDemos.\nIf you see a problem feel free to raise an issue on github (or better yet, send me a pull request with a fix for the issue.)\nAll pages are Copyright 2026 Russell G. Almond. Permission to reuse and remix granted under the basis of the CC-BY 4.0 license.",
    "crumbs": [
      "Home",
      "About"
    ]
  },
  {
    "objectID": "Bayesian/index.html",
    "href": "Bayesian/index.html",
    "title": "Index of R Demonstrations for Bayesian Data Analysis",
    "section": "",
    "text": "Florida State University\nThese are demonstrations which were written for EDF 5404, which is a Bayesian Data Analysis class taught at the graduate level.",
    "crumbs": [
      "Home",
      "Bayesian"
    ]
  },
  {
    "objectID": "Bayesian/index.html#index-of-r-demonstrations",
    "href": "Bayesian/index.html#index-of-r-demonstrations",
    "title": "Index of R Demonstrations for Bayesian Data Analysis",
    "section": "Index of R Demonstrations",
    "text": "Index of R Demonstrations\n\n\n\n\n\n\n\n\nCC-BY\n\n\nThese are licensed under the creative commons CC BY license. You many distribute, remix, adapt, and build upon the material in any medium or format, so long as attribution is given to the creator.\nFor more information contact Russell Almond.\nThe Source files for these demonstrations can be found at https://pluto.coe.fsu.edu/svn/common/rgroup-shiny/Bayesian",
    "crumbs": [
      "Home",
      "Bayesian"
    ]
  },
  {
    "objectID": "Bayesian/Schools8Stan.html",
    "href": "Bayesian/Schools8Stan.html",
    "title": "8 Schools Stan",
    "section": "",
    "text": "This is the classic eight schools example from Rubin(1981)1. (It is also found in Chapter 5 of Gelman et al., 2014 2.) The story is that 8 different schools experimented with an SAT coaching experiment. The performance gains of the coached students were compared to students on a weight list control. Separate estimates were obtained for each school, but because the size of the schools differed, the standard errors differed as well.\nHere are the data:\n\nSchools &lt;- data.frame(row.names=c(\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\"),\n                      effect = c(28.39,7.94,-2.75,6.82,-.64,.63,18.01,12.16),\n                      see = c(14.9, 10.2, 16.3, 11.0, 9.4, 11.4, 10.4, 17.6))\n\nSchools\n\n  effect  see\nA  28.39 14.9\nB   7.94 10.2\nC  -2.75 16.3\nD   6.82 11.0\nE  -0.64  9.4\nF   0.63 11.4\nG  18.01 10.4\nH  12.16 17.6\n\n\nLets start by calculating a weighted average effect. I’ll weight each case by the precision (one over the square of the see).\n\nSchools$w &lt;- 1/Schools$see^2\nSchools.mean &lt;- sum(Schools$w*Schools$effect)/sum(Schools$w)\nSchools.mean\n\n[1] 7.870546\n\n\nHere is a plot of the data.\n\nord &lt;- order(Schools$effect)\nplot(Schools$effect[ord[c(1,8)]]+c(-2,2)*Schools$see[ord[c(1,8)]],\n     c(nrow(Schools),1),main = \"8 Schools data.\",type=\"n\",yaxt=\"n\",\n     xlab=\"Effect Size\",ylab=\"School\")\npoints(Schools$effect[ord],nrow(Schools):1,pch=rownames(Schools)[ord])\nsegments(Schools$effect[ord]-2*Schools$see[ord],nrow(Schools):1,\n         Schools$effect[ord]+2*Schools$see[ord],nrow(Schools):1)\nabline(v=Schools.mean,col=\"blue\")\n\n\n\n\n\n\n\n\n\n\nFirst we need to load the packages. The rstan package runs stan from R. The shinystan package gives us a browser for the results.\n\nlibrary(rstan)\n\nLoading required package: StanHeaders\n\n\n\nrstan version 2.32.7 (Stan version 2.32.2)\n\n\nFor execution on a local, multicore CPU with excess RAM we recommend calling\noptions(mc.cores = parallel::detectCores()).\nTo avoid recompilation of unchanged Stan programs, we recommend calling\nrstan_options(auto_write = TRUE)\nFor within-chain threading using `reduce_sum()` or `map_rect()` Stan functions,\nchange `threads_per_chain` option:\nrstan_options(threads_per_chain = 1)\n\nlibrary(shinystan)\n\nLoading required package: shiny\n\n\n\nThis is shinystan version 2.6.0\n\nlibrary(parallel) # For using multiple chains\n\nFirst we set up the Stan model, and put it into a variable called school8 (note the output.var=\"school8\" in stan block tag)\n\ndata {\n  int&lt;lower=0&gt; J; // number of schools\n  real y[J]; // estimated treatement effects\n  real&lt;lower=0&gt; sigma[J]; //s.e. of effects.\n}\nparameters {\n  real mu;\n  real&lt;lower=0&gt; tau;\n  vector[J] theta;\n} \nmodel {\n  theta ~ normal(mu,tau);\n  //target += normal_lpdf(theta|mu,tau);\n  y ~ normal(theta,sigma);\n  //target += normal_lpdf(effect|theta,see);\n}\n\n\n\nAlmost all models will have a data, parameters and model block. They could have others as well (common ones are transformed data to do pre-calculations and transformed parameters to recast the model).\n\n\nIn a stan model, data refers to values that don’t change over the course of the MCMC loop. This tends to be one of three things: * The observed data values. * Fixed hyperparameters for prior distributions. * Structural hyperparamters (e.g., sample size, number of groups).\nIn stan (like C++) all variables must be declared. They are generally either int or real (here vector is shorthand for a vector of real values). The modifiers &lt;lower=XXX&gt; and &lt;upper=XXX&gt; can be used to constrain the inputs. In the data field this is just used to to type checking.\n\n\n\nThe parameters are the values that stan will try to estimate. These include both latent variables and ordinary parameters and hyperparameters.\nIn stan, parameters should all be real. Non-continuous parameters don’t work with the Hamiltonian Monte Carlo. (There are tricks for dealing with common cases like mixture models in the stan examples.) This is called lp__ (log pdf) in the output.\nNote carefully the use of the lower and upper bounds. It is important for stan to know when a parameter is restricted to say positive values (i.e., a scale parameter) as it needs to constrain the space for the sampler.\n\n\n\nThis section gives the distribution for all of the parameters. First I give the BUGS-like way of doing this. This is to use the ~ operator to give the distribution. The names are slightly different in stan and R, so the rstan function lookup can help you find the stan function corresponding to the R function.\n\nlookup(dnorm)\n\n          StanFunction\n415 normal_id_glm_lpdf\n418         normal_log\n419        normal_lpdf\n553    std_normal_lpdf\n                                                                       Arguments\n415 (real, matrix, real, vector, T);(vector, row_vector, vector, vector, vector)\n418                                     (real, real, T);(vector, vector, vector)\n419                                     (real, real, T);(vector, vector, vector)\n553                                                                 (T);(vector)\n    ReturnType\n415     T;real\n418     T;real\n419     T;real\n553     T;real\n\nlookup(dt)\n\n      StanFunction                                              Arguments\n562  student_t_log (real, real, real, T);(vector, vector, vector, vector)\n563 student_t_lpdf (real, real, real, T);(vector, vector, vector, vector)\n    ReturnType\n562     T;real\n563     T;real\n\n\nNote that what stan is actually doing in the model block is calculating the log p.d.f. Thus, the commented out expressions are an alternative to the ~ notation.\nFinally, note that this is an incomplete Bayesian model as there are no distributions for mu or tau. In stan this means we are implicitly putting a uniform prior on mu and log(tau); the latter is transformed so that it will always be positive. These are improper priors, but stan will be fine as long as the posterior is proper.\n\n\n\n\n\nBuild a list which contains the data as elements using the names in the data section of the model.\n\nschool8.dat &lt;- list(\n  J = nrow(Schools),\n  y = Schools$effect,\n  sigma=Schools$see)\n\n\n\n\nThere are two funcitons to start the sampling in stan. The first one is stan(file=XXX,data=YYY,...). This assumes that the stan model is in a file. However, with R markdown, we already saved the model in an object so we can use sampling(model,data=YYY,...). By the way, this same function can be used to make additional samples after we have sampled for a while.\nThe mc.cores should work well for Unix (Linux and Mac OS), I’m not so sure about Windows.\n\n#options(mc.cores = parallel::detectCores()-1)\nschool8.fit1 &lt;- sampling(\n  school8.stan,       # The model\n  data = school8.dat, # The data\n  chains = 5,\n  warmup = 1000,\n  iter = 2000,\n  refresh=100        # Show progress\n)\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.1e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  100 / 2000 [  5%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  300 / 2000 [ 15%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  500 / 2000 [ 25%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  700 / 2000 [ 35%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration:  900 / 2000 [ 45%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1100 / 2000 [ 55%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1300 / 2000 [ 65%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1500 / 2000 [ 75%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1700 / 2000 [ 85%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 1900 / 2000 [ 95%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.038 seconds (Warm-up)\nChain 1:                0.019 seconds (Sampling)\nChain 1:                0.057 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 5e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  100 / 2000 [  5%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  300 / 2000 [ 15%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  500 / 2000 [ 25%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  700 / 2000 [ 35%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration:  900 / 2000 [ 45%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1100 / 2000 [ 55%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1300 / 2000 [ 65%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1500 / 2000 [ 75%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1700 / 2000 [ 85%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 1900 / 2000 [ 95%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.038 seconds (Warm-up)\nChain 2:                0.135 seconds (Sampling)\nChain 2:                0.173 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 2e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  100 / 2000 [  5%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  300 / 2000 [ 15%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  500 / 2000 [ 25%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  700 / 2000 [ 35%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration:  900 / 2000 [ 45%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1100 / 2000 [ 55%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1300 / 2000 [ 65%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1500 / 2000 [ 75%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1700 / 2000 [ 85%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 1900 / 2000 [ 95%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.034 seconds (Warm-up)\nChain 3:                0.029 seconds (Sampling)\nChain 3:                0.063 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 3e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  100 / 2000 [  5%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  300 / 2000 [ 15%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  500 / 2000 [ 25%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  700 / 2000 [ 35%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration:  900 / 2000 [ 45%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1100 / 2000 [ 55%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1300 / 2000 [ 65%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1500 / 2000 [ 75%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1700 / 2000 [ 85%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 1900 / 2000 [ 95%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.038 seconds (Warm-up)\nChain 4:                0.052 seconds (Sampling)\nChain 4:                0.09 seconds (Total)\nChain 4: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 5).\nChain 5: \nChain 5: Gradient evaluation took 2e-06 seconds\nChain 5: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 5: Adjust your expectations accordingly!\nChain 5: \nChain 5: \nChain 5: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 5: Iteration:  100 / 2000 [  5%]  (Warmup)\nChain 5: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 5: Iteration:  300 / 2000 [ 15%]  (Warmup)\nChain 5: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 5: Iteration:  500 / 2000 [ 25%]  (Warmup)\nChain 5: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 5: Iteration:  700 / 2000 [ 35%]  (Warmup)\nChain 5: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 5: Iteration:  900 / 2000 [ 45%]  (Warmup)\nChain 5: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 5: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 5: Iteration: 1100 / 2000 [ 55%]  (Sampling)\nChain 5: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 5: Iteration: 1300 / 2000 [ 65%]  (Sampling)\nChain 5: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 5: Iteration: 1500 / 2000 [ 75%]  (Sampling)\nChain 5: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 5: Iteration: 1700 / 2000 [ 85%]  (Sampling)\nChain 5: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 5: Iteration: 1900 / 2000 [ 95%]  (Sampling)\nChain 5: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 5: \nChain 5:  Elapsed Time: 0.033 seconds (Warm-up)\nChain 5:                0.018 seconds (Sampling)\nChain 5:                0.051 seconds (Total)\nChain 5: \n\n\nWarning: There were 154 divergent transitions after warmup. See\nhttps://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\nto find out why this is a problem and how to eliminate them.\n\n\nWarning: There were 2 chains where the estimated Bayesian Fraction of Missing Information was low. See\nhttps://mc-stan.org/misc/warnings.html#bfmi-low\n\n\nWarning: Examine the pairs() plot to diagnose sampling problems\n\n\nWarning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#bulk-ess\n\n\nWarning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#tail-ess\n\nsummary(school8.fit1)\n\n$summary\n               mean   se_mean       sd        2.5%        25%        50%\nmu         8.063823 0.1932575 5.160332  -1.3259767   4.601827   7.900742\ntau        6.396979 0.3103570 5.357866   0.4788392   2.417204   4.982668\ntheta[1]  11.396494 0.3077070 8.177041  -1.7407249   5.985315  10.427094\ntheta[2]   8.076589 0.1791163 6.276306  -4.7949497   4.448369   7.924210\ntheta[3]   6.467250 0.2058307 7.714801 -10.7781777   2.440348   6.665312\ntheta[4]   7.678566 0.2107533 6.602538  -5.2816080   3.408931   7.511128\ntheta[5]   5.557861 0.2124874 6.388271  -8.5144291   1.776550   5.703977\ntheta[6]   6.279659 0.1934404 6.677172  -8.1196585   2.484585   6.355409\ntheta[7]  10.461287 0.2482535 6.725375  -1.1724790   5.741956   9.916992\ntheta[8]   8.528932 0.1899462 7.858490  -6.8748362   4.055652   8.252270\nlp__     -16.646031 0.5919228 6.527616 -27.3633646 -21.265203 -17.547663\n                75%     97.5%     n_eff     Rhat\nmu        11.403054 18.725417  712.9888 1.012893\ntau        8.804120 19.987979  298.0305 1.008668\ntheta[1]  15.712431 30.924250  706.1836 1.008570\ntheta[2]  12.042022 20.771631 1227.8296 1.006468\ntheta[3]  11.142922 20.551331 1404.8478 1.005145\ntheta[4]  11.668921 21.325728  981.4607 1.008483\ntheta[5]   9.933139 17.371930  903.8584 1.006890\ntheta[6]  10.577980 18.948792 1191.4906 1.005716\ntheta[7]  14.339119 25.374286  733.9088 1.007432\ntheta[8]  12.854723 25.330627 1711.6578 1.005591\nlp__     -12.644816 -0.520155  121.6127 1.030203\n\n$c_summary\n, , chains = chain:1\n\n          stats\nparameter        mean       sd       2.5%         25%        50%        75%\n  mu         7.282719 5.229804  -1.311686   3.7261895   6.840869  10.826337\n  tau        7.012299 5.240803   1.180464   3.2438068   5.827881   9.484641\n  theta[1]  11.365646 8.815098  -2.324844   5.5531473   9.913712  16.701125\n  theta[2]   7.574866 6.379560  -5.422789   3.6186932   7.166054  12.011718\n  theta[3]   5.586158 8.068504 -11.854400   0.9859646   6.099303  10.596346\n  theta[4]   6.747930 6.723885  -5.599142   2.3843981   6.195913  11.055850\n  theta[5]   4.556820 6.539177  -8.931886   0.4538806   4.836312   8.490538\n  theta[6]   5.478413 6.931856  -8.348210   1.2218709   5.353378   9.800898\n  theta[7]  10.242826 7.008009  -1.230049   5.0152979   9.875153  14.706052\n  theta[8]   8.081118 8.564069  -9.257853   2.9034094   7.196510  13.092288\n  lp__     -17.911787 5.651639 -27.866352 -21.6967855 -18.643771 -14.710572\n          stats\nparameter      97.5%\n  mu       17.483807\n  tau      19.772057\n  theta[1] 32.246409\n  theta[2] 20.088141\n  theta[3] 20.271999\n  theta[4] 20.990033\n  theta[5] 16.970465\n  theta[6] 19.104984\n  theta[7] 24.886528\n  theta[8] 25.610862\n  lp__     -5.803353\n\n, , chains = chain:2\n\n          stats\nparameter        mean       sd        2.5%        25%        50%        75%\n  mu         8.369482 5.338541  -2.5357105   5.088740   8.232416  11.715376\n  tau        5.592496 5.622217   0.2886967   1.596150   4.072735   7.869791\n  theta[1]  11.153933 7.698555  -0.9600623   6.123882  10.120523  15.460358\n  theta[2]   8.410520 6.341443  -4.7047783   4.795606   8.221050  12.301352\n  theta[3]   7.217754 6.995832  -7.4607485   3.363496   7.445287  11.539353\n  theta[4]   7.923417 6.674757  -5.4545991   4.044690   7.798009  12.101184\n  theta[5]   6.234488 6.292806  -7.1991456   2.608826   6.692219  10.039194\n  theta[6]   6.943496 6.388402  -7.8766807   3.247776   7.349545  10.923576\n  theta[7]  10.344362 6.629248  -1.6803007   5.802965   9.562906  14.176368\n  theta[8]   8.651360 7.363847  -6.6853946   4.314188   8.314774  12.904664\n  lp__     -14.846212 7.720905 -27.0440241 -20.424496 -16.130766 -10.195984\n          stats\nparameter      97.5%\n  mu       19.003712\n  tau      19.491261\n  theta[1] 29.417658\n  theta[2] 20.972061\n  theta[3] 20.097535\n  theta[4] 20.772555\n  theta[5] 18.200035\n  theta[6] 18.948792\n  theta[7] 25.130246\n  theta[8] 23.990219\n  lp__      2.716849\n\n, , chains = chain:3\n\n          stats\nparameter        mean       sd        2.5%        25%        50%        75%\n  mu         8.873016 4.929843  -1.1605205   5.383816   9.157392  12.231116\n  tau        6.638429 4.765563   1.1317447   3.067705   5.380039   8.987833\n  theta[1]  12.176237 7.648900  -1.2833153   7.051719  11.444270  16.569960\n  theta[2]   8.719311 6.337301  -3.7189326   4.903266   8.645062  12.621204\n  theta[3]   7.257961 7.534582  -9.8020499   3.314416   7.618341  12.054933\n  theta[4]   8.653424 6.395249  -4.7370312   4.541216   9.095262  12.791922\n  theta[5]   6.163134 6.483306  -8.4919474   2.047499   6.208978  10.726697\n  theta[6]   6.891969 6.805793  -8.1060971   2.851827   7.392187  11.463391\n  theta[7]  11.466872 6.482320  -0.4147575   7.012496  11.083998  15.214810\n  theta[8]   9.374413 8.176910  -6.3289148   4.401083   9.612659  13.933705\n  lp__     -17.686766 4.997223 -26.7455477 -21.203577 -18.017035 -14.396998\n          stats\nparameter      97.5%\n  mu       19.027584\n  tau      18.687089\n  theta[1] 29.221303\n  theta[2] 21.794100\n  theta[3] 21.234875\n  theta[4] 20.328448\n  theta[5] 18.114117\n  theta[6] 19.226178\n  theta[7] 25.742193\n  theta[8] 26.009646\n  lp__     -7.063764\n\n, , chains = chain:4\n\n          stats\nparameter        mean       sd        2.5%        25%        50%        75%\n  mu         8.091812 5.076398  -0.8425344   4.412178   7.870849  11.402970\n  tau        6.330311 6.015725   0.4320454   1.858901   4.574109   8.930366\n  theta[1]  11.372386 8.336178  -2.0383630   6.005397  10.937299  14.975714\n  theta[2]   8.010314 6.383543  -4.4864252   4.068065   7.965424  11.680475\n  theta[3]   5.921364 8.504707 -14.4897496   1.485791   6.627561  11.068909\n  theta[4]   7.999607 6.718544  -5.1728198   3.705512   8.110325  11.692054\n  theta[5]   5.664339 6.447360  -8.8408712   1.672669   6.431875  10.719279\n  theta[6]   6.193572 6.740621  -7.8841257   2.321322   6.403065  10.888760\n  theta[7]  10.471510 6.794573  -1.3823923   5.975281  10.346433  13.956443\n  theta[8]   8.466771 7.828049  -6.9143009   3.876160   8.218038  12.188508\n  lp__     -15.756553 7.649664 -27.6581132 -21.690867 -16.823297 -10.926338\n          stats\nparameter      97.5%\n  mu       18.841303\n  tau      21.641405\n  theta[1] 32.376435\n  theta[2] 21.246654\n  theta[3] 20.201117\n  theta[4] 22.117877\n  theta[5] 16.107905\n  theta[6] 18.587446\n  theta[7] 25.473601\n  theta[8] 26.138659\n  lp__      1.215802\n\n, , chains = chain:5\n\n          stats\nparameter        mean       sd        2.5%        25%        50%        75%\n  mu         7.702088 5.082935  -1.9775700   4.460717   7.422270  10.722675\n  tau        6.411360 4.958211   1.5011570   2.788502   4.909364   8.736781\n  theta[1]  10.914266 8.289431  -1.4719501   5.201660   9.289994  15.110132\n  theta[2]   7.667933 5.860499  -5.3121089   5.002163   7.183776  10.913805\n  theta[3]   6.353011 7.239378 -10.9098719   3.207669   5.890261  10.245648\n  theta[4]   7.068452 6.323527  -5.8728490   3.076808   6.643771  10.607220\n  theta[5]   5.170524 6.021785  -8.8548368   2.061720   4.960641   9.011760\n  theta[6]   5.890842 6.392712  -8.0107842   2.795503   5.045903   9.741530\n  theta[7]   9.780866 6.599349  -0.9045583   4.954774   8.465719  13.288097\n  theta[8]   8.070998 7.223002  -6.5099588   4.623918   7.907669  11.869418\n  lp__     -17.028837 5.595290 -27.1709433 -21.224552 -17.361854 -12.756322\n          stats\nparameter      97.5%\n  mu       18.427456\n  tau      18.990219\n  theta[1] 31.590988\n  theta[2] 19.761773\n  theta[3] 21.215358\n  theta[4] 21.934060\n  theta[5] 16.170050\n  theta[6] 18.942435\n  theta[7] 25.125479\n  theta[8] 22.804569\n  lp__     -7.525026\n\n\nAlternate style using external file.\n\noptions(mc.cores = parallel::detectCores()-1)\nschool8.fit1 &lt;- stan(\n  file=\"school8.stan\",       # The model\n  data = school8.dat, # The data\n  chains = 5,\n  warmup = 1000,\n  iter = 2000,\n  refresh=100        # Show progress\n)\nsummary(school8.fit1)\n\n\n\n\nPrinting gives summaries of the posterior for the specified parameters. Use the pars argument to select what to print.\n\nprint(school8.fit1,pars=c(\"theta\",\"mu\",\"tau\",\"lp__\"), probs=c(.1,.5,.9))\n\nInference for Stan model: anon_model.\n5 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=5000.\n\n           mean se_mean   sd    10%    50%   90% n_eff Rhat\ntheta[1]  11.40    0.31 8.18   2.44  10.43 21.88   706 1.01\ntheta[2]   8.08    0.18 6.28   0.50   7.92 15.65  1228 1.01\ntheta[3]   6.47    0.21 7.71  -2.77   6.67 15.35  1405 1.01\ntheta[4]   7.68    0.21 6.60  -0.57   7.51 15.81   981 1.01\ntheta[5]   5.56    0.21 6.39  -2.49   5.70 13.25   904 1.01\ntheta[6]   6.28    0.19 6.68  -1.86   6.36 14.34  1191 1.01\ntheta[7]  10.46    0.25 6.73   2.71   9.92 19.24   734 1.01\ntheta[8]   8.53    0.19 7.86  -0.45   8.25 18.26  1712 1.01\nmu         8.06    0.19 5.16   1.75   7.90 14.54   713 1.01\ntau        6.40    0.31 5.36   1.36   4.98 13.47   298 1.01\nlp__     -16.65    0.59 6.53 -24.27 -17.55 -7.65   122 1.03\n\nSamples were drawn using NUTS(diag_e) at Sun Jan  4 20:14:06 2026.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\nThe plot function gives 50% and 95% intervals.\n\nplot(school8.fit1)\n\nci_level: 0.8 (80% intervals)\n\n\nouter_level: 0.95 (95% intervals)\n\n\n\n\n\n\n\n\n\nTraceplot shows convergence (note stan and coda have slightly different traceplot functions).\n\nrstan::traceplot(school8.fit1,pars=c(\"mu\",\"tau\",\"lp__\"), inc_warmup=TRUE, nrow=3)\n\n\n\n\n\n\n\n\nWhat we are looking for here is (a) white-noise like, and all chains plotting over top of each other.\nTypically variance and scale parameters will be positively skewed, and log-posterior is often not as well behaved as others.\nPair plot sometimes uncover problems with models.\n\npairs(school8.fit1,pars=c(\"mu\",\"theta[1]\",\"tau\"))\n\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\n\n\n\n\n\n\n\n\n\n\n\n\nShinystan opens a new shiny workspace which allows interactive browsing of the results.\n\nschool8.fit1s &lt;- launch_shinystan(school8.fit1)",
    "crumbs": [
      "8 Schools Stan"
    ]
  },
  {
    "objectID": "Bayesian/Schools8Stan.html#model-setup-in-stan.",
    "href": "Bayesian/Schools8Stan.html#model-setup-in-stan.",
    "title": "8 Schools Stan",
    "section": "",
    "text": "First we need to load the packages. The rstan package runs stan from R. The shinystan package gives us a browser for the results.\n\nlibrary(rstan)\n\nLoading required package: StanHeaders\n\n\n\nrstan version 2.32.7 (Stan version 2.32.2)\n\n\nFor execution on a local, multicore CPU with excess RAM we recommend calling\noptions(mc.cores = parallel::detectCores()).\nTo avoid recompilation of unchanged Stan programs, we recommend calling\nrstan_options(auto_write = TRUE)\nFor within-chain threading using `reduce_sum()` or `map_rect()` Stan functions,\nchange `threads_per_chain` option:\nrstan_options(threads_per_chain = 1)\n\nlibrary(shinystan)\n\nLoading required package: shiny\n\n\n\nThis is shinystan version 2.6.0\n\nlibrary(parallel) # For using multiple chains\n\nFirst we set up the Stan model, and put it into a variable called school8 (note the output.var=\"school8\" in stan block tag)\n\ndata {\n  int&lt;lower=0&gt; J; // number of schools\n  real y[J]; // estimated treatement effects\n  real&lt;lower=0&gt; sigma[J]; //s.e. of effects.\n}\nparameters {\n  real mu;\n  real&lt;lower=0&gt; tau;\n  vector[J] theta;\n} \nmodel {\n  theta ~ normal(mu,tau);\n  //target += normal_lpdf(theta|mu,tau);\n  y ~ normal(theta,sigma);\n  //target += normal_lpdf(effect|theta,see);\n}\n\n\n\nAlmost all models will have a data, parameters and model block. They could have others as well (common ones are transformed data to do pre-calculations and transformed parameters to recast the model).\n\n\nIn a stan model, data refers to values that don’t change over the course of the MCMC loop. This tends to be one of three things: * The observed data values. * Fixed hyperparameters for prior distributions. * Structural hyperparamters (e.g., sample size, number of groups).\nIn stan (like C++) all variables must be declared. They are generally either int or real (here vector is shorthand for a vector of real values). The modifiers &lt;lower=XXX&gt; and &lt;upper=XXX&gt; can be used to constrain the inputs. In the data field this is just used to to type checking.\n\n\n\nThe parameters are the values that stan will try to estimate. These include both latent variables and ordinary parameters and hyperparameters.\nIn stan, parameters should all be real. Non-continuous parameters don’t work with the Hamiltonian Monte Carlo. (There are tricks for dealing with common cases like mixture models in the stan examples.) This is called lp__ (log pdf) in the output.\nNote carefully the use of the lower and upper bounds. It is important for stan to know when a parameter is restricted to say positive values (i.e., a scale parameter) as it needs to constrain the space for the sampler.\n\n\n\nThis section gives the distribution for all of the parameters. First I give the BUGS-like way of doing this. This is to use the ~ operator to give the distribution. The names are slightly different in stan and R, so the rstan function lookup can help you find the stan function corresponding to the R function.\n\nlookup(dnorm)\n\n          StanFunction\n415 normal_id_glm_lpdf\n418         normal_log\n419        normal_lpdf\n553    std_normal_lpdf\n                                                                       Arguments\n415 (real, matrix, real, vector, T);(vector, row_vector, vector, vector, vector)\n418                                     (real, real, T);(vector, vector, vector)\n419                                     (real, real, T);(vector, vector, vector)\n553                                                                 (T);(vector)\n    ReturnType\n415     T;real\n418     T;real\n419     T;real\n553     T;real\n\nlookup(dt)\n\n      StanFunction                                              Arguments\n562  student_t_log (real, real, real, T);(vector, vector, vector, vector)\n563 student_t_lpdf (real, real, real, T);(vector, vector, vector, vector)\n    ReturnType\n562     T;real\n563     T;real\n\n\nNote that what stan is actually doing in the model block is calculating the log p.d.f. Thus, the commented out expressions are an alternative to the ~ notation.\nFinally, note that this is an incomplete Bayesian model as there are no distributions for mu or tau. In stan this means we are implicitly putting a uniform prior on mu and log(tau); the latter is transformed so that it will always be positive. These are improper priors, but stan will be fine as long as the posterior is proper.",
    "crumbs": [
      "8 Schools Stan"
    ]
  },
  {
    "objectID": "Bayesian/Schools8Stan.html#data-preparation",
    "href": "Bayesian/Schools8Stan.html#data-preparation",
    "title": "8 Schools Stan",
    "section": "",
    "text": "Build a list which contains the data as elements using the names in the data section of the model.\n\nschool8.dat &lt;- list(\n  J = nrow(Schools),\n  y = Schools$effect,\n  sigma=Schools$see)",
    "crumbs": [
      "8 Schools Stan"
    ]
  },
  {
    "objectID": "Bayesian/Schools8Stan.html#running-stan",
    "href": "Bayesian/Schools8Stan.html#running-stan",
    "title": "8 Schools Stan",
    "section": "",
    "text": "There are two funcitons to start the sampling in stan. The first one is stan(file=XXX,data=YYY,...). This assumes that the stan model is in a file. However, with R markdown, we already saved the model in an object so we can use sampling(model,data=YYY,...). By the way, this same function can be used to make additional samples after we have sampled for a while.\nThe mc.cores should work well for Unix (Linux and Mac OS), I’m not so sure about Windows.\n\n#options(mc.cores = parallel::detectCores()-1)\nschool8.fit1 &lt;- sampling(\n  school8.stan,       # The model\n  data = school8.dat, # The data\n  chains = 5,\n  warmup = 1000,\n  iter = 2000,\n  refresh=100        # Show progress\n)\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.1e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  100 / 2000 [  5%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  300 / 2000 [ 15%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  500 / 2000 [ 25%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  700 / 2000 [ 35%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration:  900 / 2000 [ 45%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1100 / 2000 [ 55%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1300 / 2000 [ 65%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1500 / 2000 [ 75%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1700 / 2000 [ 85%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 1900 / 2000 [ 95%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.038 seconds (Warm-up)\nChain 1:                0.019 seconds (Sampling)\nChain 1:                0.057 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 5e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  100 / 2000 [  5%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  300 / 2000 [ 15%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  500 / 2000 [ 25%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  700 / 2000 [ 35%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration:  900 / 2000 [ 45%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1100 / 2000 [ 55%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1300 / 2000 [ 65%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1500 / 2000 [ 75%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1700 / 2000 [ 85%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 1900 / 2000 [ 95%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.038 seconds (Warm-up)\nChain 2:                0.135 seconds (Sampling)\nChain 2:                0.173 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 2e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  100 / 2000 [  5%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  300 / 2000 [ 15%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  500 / 2000 [ 25%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  700 / 2000 [ 35%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration:  900 / 2000 [ 45%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1100 / 2000 [ 55%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1300 / 2000 [ 65%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1500 / 2000 [ 75%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1700 / 2000 [ 85%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 1900 / 2000 [ 95%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.034 seconds (Warm-up)\nChain 3:                0.029 seconds (Sampling)\nChain 3:                0.063 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 3e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  100 / 2000 [  5%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  300 / 2000 [ 15%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  500 / 2000 [ 25%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  700 / 2000 [ 35%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration:  900 / 2000 [ 45%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1100 / 2000 [ 55%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1300 / 2000 [ 65%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1500 / 2000 [ 75%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1700 / 2000 [ 85%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 1900 / 2000 [ 95%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.038 seconds (Warm-up)\nChain 4:                0.052 seconds (Sampling)\nChain 4:                0.09 seconds (Total)\nChain 4: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 5).\nChain 5: \nChain 5: Gradient evaluation took 2e-06 seconds\nChain 5: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 5: Adjust your expectations accordingly!\nChain 5: \nChain 5: \nChain 5: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 5: Iteration:  100 / 2000 [  5%]  (Warmup)\nChain 5: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 5: Iteration:  300 / 2000 [ 15%]  (Warmup)\nChain 5: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 5: Iteration:  500 / 2000 [ 25%]  (Warmup)\nChain 5: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 5: Iteration:  700 / 2000 [ 35%]  (Warmup)\nChain 5: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 5: Iteration:  900 / 2000 [ 45%]  (Warmup)\nChain 5: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 5: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 5: Iteration: 1100 / 2000 [ 55%]  (Sampling)\nChain 5: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 5: Iteration: 1300 / 2000 [ 65%]  (Sampling)\nChain 5: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 5: Iteration: 1500 / 2000 [ 75%]  (Sampling)\nChain 5: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 5: Iteration: 1700 / 2000 [ 85%]  (Sampling)\nChain 5: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 5: Iteration: 1900 / 2000 [ 95%]  (Sampling)\nChain 5: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 5: \nChain 5:  Elapsed Time: 0.033 seconds (Warm-up)\nChain 5:                0.018 seconds (Sampling)\nChain 5:                0.051 seconds (Total)\nChain 5: \n\n\nWarning: There were 154 divergent transitions after warmup. See\nhttps://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\nto find out why this is a problem and how to eliminate them.\n\n\nWarning: There were 2 chains where the estimated Bayesian Fraction of Missing Information was low. See\nhttps://mc-stan.org/misc/warnings.html#bfmi-low\n\n\nWarning: Examine the pairs() plot to diagnose sampling problems\n\n\nWarning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#bulk-ess\n\n\nWarning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#tail-ess\n\nsummary(school8.fit1)\n\n$summary\n               mean   se_mean       sd        2.5%        25%        50%\nmu         8.063823 0.1932575 5.160332  -1.3259767   4.601827   7.900742\ntau        6.396979 0.3103570 5.357866   0.4788392   2.417204   4.982668\ntheta[1]  11.396494 0.3077070 8.177041  -1.7407249   5.985315  10.427094\ntheta[2]   8.076589 0.1791163 6.276306  -4.7949497   4.448369   7.924210\ntheta[3]   6.467250 0.2058307 7.714801 -10.7781777   2.440348   6.665312\ntheta[4]   7.678566 0.2107533 6.602538  -5.2816080   3.408931   7.511128\ntheta[5]   5.557861 0.2124874 6.388271  -8.5144291   1.776550   5.703977\ntheta[6]   6.279659 0.1934404 6.677172  -8.1196585   2.484585   6.355409\ntheta[7]  10.461287 0.2482535 6.725375  -1.1724790   5.741956   9.916992\ntheta[8]   8.528932 0.1899462 7.858490  -6.8748362   4.055652   8.252270\nlp__     -16.646031 0.5919228 6.527616 -27.3633646 -21.265203 -17.547663\n                75%     97.5%     n_eff     Rhat\nmu        11.403054 18.725417  712.9888 1.012893\ntau        8.804120 19.987979  298.0305 1.008668\ntheta[1]  15.712431 30.924250  706.1836 1.008570\ntheta[2]  12.042022 20.771631 1227.8296 1.006468\ntheta[3]  11.142922 20.551331 1404.8478 1.005145\ntheta[4]  11.668921 21.325728  981.4607 1.008483\ntheta[5]   9.933139 17.371930  903.8584 1.006890\ntheta[6]  10.577980 18.948792 1191.4906 1.005716\ntheta[7]  14.339119 25.374286  733.9088 1.007432\ntheta[8]  12.854723 25.330627 1711.6578 1.005591\nlp__     -12.644816 -0.520155  121.6127 1.030203\n\n$c_summary\n, , chains = chain:1\n\n          stats\nparameter        mean       sd       2.5%         25%        50%        75%\n  mu         7.282719 5.229804  -1.311686   3.7261895   6.840869  10.826337\n  tau        7.012299 5.240803   1.180464   3.2438068   5.827881   9.484641\n  theta[1]  11.365646 8.815098  -2.324844   5.5531473   9.913712  16.701125\n  theta[2]   7.574866 6.379560  -5.422789   3.6186932   7.166054  12.011718\n  theta[3]   5.586158 8.068504 -11.854400   0.9859646   6.099303  10.596346\n  theta[4]   6.747930 6.723885  -5.599142   2.3843981   6.195913  11.055850\n  theta[5]   4.556820 6.539177  -8.931886   0.4538806   4.836312   8.490538\n  theta[6]   5.478413 6.931856  -8.348210   1.2218709   5.353378   9.800898\n  theta[7]  10.242826 7.008009  -1.230049   5.0152979   9.875153  14.706052\n  theta[8]   8.081118 8.564069  -9.257853   2.9034094   7.196510  13.092288\n  lp__     -17.911787 5.651639 -27.866352 -21.6967855 -18.643771 -14.710572\n          stats\nparameter      97.5%\n  mu       17.483807\n  tau      19.772057\n  theta[1] 32.246409\n  theta[2] 20.088141\n  theta[3] 20.271999\n  theta[4] 20.990033\n  theta[5] 16.970465\n  theta[6] 19.104984\n  theta[7] 24.886528\n  theta[8] 25.610862\n  lp__     -5.803353\n\n, , chains = chain:2\n\n          stats\nparameter        mean       sd        2.5%        25%        50%        75%\n  mu         8.369482 5.338541  -2.5357105   5.088740   8.232416  11.715376\n  tau        5.592496 5.622217   0.2886967   1.596150   4.072735   7.869791\n  theta[1]  11.153933 7.698555  -0.9600623   6.123882  10.120523  15.460358\n  theta[2]   8.410520 6.341443  -4.7047783   4.795606   8.221050  12.301352\n  theta[3]   7.217754 6.995832  -7.4607485   3.363496   7.445287  11.539353\n  theta[4]   7.923417 6.674757  -5.4545991   4.044690   7.798009  12.101184\n  theta[5]   6.234488 6.292806  -7.1991456   2.608826   6.692219  10.039194\n  theta[6]   6.943496 6.388402  -7.8766807   3.247776   7.349545  10.923576\n  theta[7]  10.344362 6.629248  -1.6803007   5.802965   9.562906  14.176368\n  theta[8]   8.651360 7.363847  -6.6853946   4.314188   8.314774  12.904664\n  lp__     -14.846212 7.720905 -27.0440241 -20.424496 -16.130766 -10.195984\n          stats\nparameter      97.5%\n  mu       19.003712\n  tau      19.491261\n  theta[1] 29.417658\n  theta[2] 20.972061\n  theta[3] 20.097535\n  theta[4] 20.772555\n  theta[5] 18.200035\n  theta[6] 18.948792\n  theta[7] 25.130246\n  theta[8] 23.990219\n  lp__      2.716849\n\n, , chains = chain:3\n\n          stats\nparameter        mean       sd        2.5%        25%        50%        75%\n  mu         8.873016 4.929843  -1.1605205   5.383816   9.157392  12.231116\n  tau        6.638429 4.765563   1.1317447   3.067705   5.380039   8.987833\n  theta[1]  12.176237 7.648900  -1.2833153   7.051719  11.444270  16.569960\n  theta[2]   8.719311 6.337301  -3.7189326   4.903266   8.645062  12.621204\n  theta[3]   7.257961 7.534582  -9.8020499   3.314416   7.618341  12.054933\n  theta[4]   8.653424 6.395249  -4.7370312   4.541216   9.095262  12.791922\n  theta[5]   6.163134 6.483306  -8.4919474   2.047499   6.208978  10.726697\n  theta[6]   6.891969 6.805793  -8.1060971   2.851827   7.392187  11.463391\n  theta[7]  11.466872 6.482320  -0.4147575   7.012496  11.083998  15.214810\n  theta[8]   9.374413 8.176910  -6.3289148   4.401083   9.612659  13.933705\n  lp__     -17.686766 4.997223 -26.7455477 -21.203577 -18.017035 -14.396998\n          stats\nparameter      97.5%\n  mu       19.027584\n  tau      18.687089\n  theta[1] 29.221303\n  theta[2] 21.794100\n  theta[3] 21.234875\n  theta[4] 20.328448\n  theta[5] 18.114117\n  theta[6] 19.226178\n  theta[7] 25.742193\n  theta[8] 26.009646\n  lp__     -7.063764\n\n, , chains = chain:4\n\n          stats\nparameter        mean       sd        2.5%        25%        50%        75%\n  mu         8.091812 5.076398  -0.8425344   4.412178   7.870849  11.402970\n  tau        6.330311 6.015725   0.4320454   1.858901   4.574109   8.930366\n  theta[1]  11.372386 8.336178  -2.0383630   6.005397  10.937299  14.975714\n  theta[2]   8.010314 6.383543  -4.4864252   4.068065   7.965424  11.680475\n  theta[3]   5.921364 8.504707 -14.4897496   1.485791   6.627561  11.068909\n  theta[4]   7.999607 6.718544  -5.1728198   3.705512   8.110325  11.692054\n  theta[5]   5.664339 6.447360  -8.8408712   1.672669   6.431875  10.719279\n  theta[6]   6.193572 6.740621  -7.8841257   2.321322   6.403065  10.888760\n  theta[7]  10.471510 6.794573  -1.3823923   5.975281  10.346433  13.956443\n  theta[8]   8.466771 7.828049  -6.9143009   3.876160   8.218038  12.188508\n  lp__     -15.756553 7.649664 -27.6581132 -21.690867 -16.823297 -10.926338\n          stats\nparameter      97.5%\n  mu       18.841303\n  tau      21.641405\n  theta[1] 32.376435\n  theta[2] 21.246654\n  theta[3] 20.201117\n  theta[4] 22.117877\n  theta[5] 16.107905\n  theta[6] 18.587446\n  theta[7] 25.473601\n  theta[8] 26.138659\n  lp__      1.215802\n\n, , chains = chain:5\n\n          stats\nparameter        mean       sd        2.5%        25%        50%        75%\n  mu         7.702088 5.082935  -1.9775700   4.460717   7.422270  10.722675\n  tau        6.411360 4.958211   1.5011570   2.788502   4.909364   8.736781\n  theta[1]  10.914266 8.289431  -1.4719501   5.201660   9.289994  15.110132\n  theta[2]   7.667933 5.860499  -5.3121089   5.002163   7.183776  10.913805\n  theta[3]   6.353011 7.239378 -10.9098719   3.207669   5.890261  10.245648\n  theta[4]   7.068452 6.323527  -5.8728490   3.076808   6.643771  10.607220\n  theta[5]   5.170524 6.021785  -8.8548368   2.061720   4.960641   9.011760\n  theta[6]   5.890842 6.392712  -8.0107842   2.795503   5.045903   9.741530\n  theta[7]   9.780866 6.599349  -0.9045583   4.954774   8.465719  13.288097\n  theta[8]   8.070998 7.223002  -6.5099588   4.623918   7.907669  11.869418\n  lp__     -17.028837 5.595290 -27.1709433 -21.224552 -17.361854 -12.756322\n          stats\nparameter      97.5%\n  mu       18.427456\n  tau      18.990219\n  theta[1] 31.590988\n  theta[2] 19.761773\n  theta[3] 21.215358\n  theta[4] 21.934060\n  theta[5] 16.170050\n  theta[6] 18.942435\n  theta[7] 25.125479\n  theta[8] 22.804569\n  lp__     -7.525026\n\n\nAlternate style using external file.\n\noptions(mc.cores = parallel::detectCores()-1)\nschool8.fit1 &lt;- stan(\n  file=\"school8.stan\",       # The model\n  data = school8.dat, # The data\n  chains = 5,\n  warmup = 1000,\n  iter = 2000,\n  refresh=100        # Show progress\n)\nsummary(school8.fit1)",
    "crumbs": [
      "8 Schools Stan"
    ]
  },
  {
    "objectID": "Bayesian/Schools8Stan.html#summaries-using-the-base-stan-functions",
    "href": "Bayesian/Schools8Stan.html#summaries-using-the-base-stan-functions",
    "title": "8 Schools Stan",
    "section": "",
    "text": "Printing gives summaries of the posterior for the specified parameters. Use the pars argument to select what to print.\n\nprint(school8.fit1,pars=c(\"theta\",\"mu\",\"tau\",\"lp__\"), probs=c(.1,.5,.9))\n\nInference for Stan model: anon_model.\n5 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=5000.\n\n           mean se_mean   sd    10%    50%   90% n_eff Rhat\ntheta[1]  11.40    0.31 8.18   2.44  10.43 21.88   706 1.01\ntheta[2]   8.08    0.18 6.28   0.50   7.92 15.65  1228 1.01\ntheta[3]   6.47    0.21 7.71  -2.77   6.67 15.35  1405 1.01\ntheta[4]   7.68    0.21 6.60  -0.57   7.51 15.81   981 1.01\ntheta[5]   5.56    0.21 6.39  -2.49   5.70 13.25   904 1.01\ntheta[6]   6.28    0.19 6.68  -1.86   6.36 14.34  1191 1.01\ntheta[7]  10.46    0.25 6.73   2.71   9.92 19.24   734 1.01\ntheta[8]   8.53    0.19 7.86  -0.45   8.25 18.26  1712 1.01\nmu         8.06    0.19 5.16   1.75   7.90 14.54   713 1.01\ntau        6.40    0.31 5.36   1.36   4.98 13.47   298 1.01\nlp__     -16.65    0.59 6.53 -24.27 -17.55 -7.65   122 1.03\n\nSamples were drawn using NUTS(diag_e) at Sun Jan  4 20:14:06 2026.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\nThe plot function gives 50% and 95% intervals.\n\nplot(school8.fit1)\n\nci_level: 0.8 (80% intervals)\n\n\nouter_level: 0.95 (95% intervals)\n\n\n\n\n\n\n\n\n\nTraceplot shows convergence (note stan and coda have slightly different traceplot functions).\n\nrstan::traceplot(school8.fit1,pars=c(\"mu\",\"tau\",\"lp__\"), inc_warmup=TRUE, nrow=3)\n\n\n\n\n\n\n\n\nWhat we are looking for here is (a) white-noise like, and all chains plotting over top of each other.\nTypically variance and scale parameters will be positively skewed, and log-posterior is often not as well behaved as others.\nPair plot sometimes uncover problems with models.\n\npairs(school8.fit1,pars=c(\"mu\",\"theta[1]\",\"tau\"))\n\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter",
    "crumbs": [
      "8 Schools Stan"
    ]
  },
  {
    "objectID": "Bayesian/Schools8Stan.html#shinystan",
    "href": "Bayesian/Schools8Stan.html#shinystan",
    "title": "8 Schools Stan",
    "section": "",
    "text": "Shinystan opens a new shiny workspace which allows interactive browsing of the results.\n\nschool8.fit1s &lt;- launch_shinystan(school8.fit1)",
    "crumbs": [
      "8 Schools Stan"
    ]
  },
  {
    "objectID": "Bayesian/Schools8Stan.html#footnotes",
    "href": "Bayesian/Schools8Stan.html#footnotes",
    "title": "8 Schools Stan",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRubin, D. B. (1981). Estimation in Parallel randomized experiments. Journal of Educational Statistics, 6, 377-401.↩︎\nGelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A. and Rubin, D. B. (2014). Bayesian Data Analysis: Third Edition. CRC Press. (ISBN: 978-1-4398-4095-5)↩︎",
    "crumbs": [
      "8 Schools Stan"
    ]
  },
  {
    "objectID": "Bayesian/RatersStan.html",
    "href": "Bayesian/RatersStan.html",
    "title": "Multiple Raters in Stan",
    "section": "",
    "text": "Models for adjusting individual ratings: a committee of 10 persons is evaluating 100 job applicants. Each person on the committee reads 30 applications (structured so that each application is read by 3 people) and gives a numerical rating between 1 and 10.\n(a) It would be natural to rate the applications based on their combined scores; however, there is a worry that different raters use different standards (severity) and we would like to correct for this. Set up a model for the ratings (with parameters for the applicants and the raters).\nI &lt;- 100L # applicants\nJ &lt;- 10L  # raters\nW &lt;- 3L   # ratings per applicant\n\n#assignments &lt;- read.csv(\"assignment.csv\")\nratings &lt;- read.csv(\"ratings.csv\")\ndata {\n  int&lt;lower=0&gt; I; //Number of applicants\n  int&lt;lower=0&gt; J; //Number of raters\n  int&lt;lower=0&gt; W; //Ratings per applicant\n  \n  int&lt;lower=0,upper=I&gt; applicant[I*W];\n  int&lt;lower=0,upper=J&gt; rater[I*W];\n  real&lt;lower=1,upper=10&gt; rating[I*W];\n\n}\nparameters {\n\n  real&lt;lower=1,upper=10&gt; ability[I];\n  real severity[J];\n  \n  //Variance parameters\n  real&lt;lower=0&gt; sigma; //rating level SD\n  real&lt;lower=0&gt; tau; //rater level SD\n}\ntransformed parameters {\n  real expRating[I*W];\n  for (i in 1:(I*W)) \n    expRating[i] = ability[applicant[i]] + severity[rater[i]];\n    \n}\nmodel {\n  // Ability and severity\n  ability ~ uniform(1,10);\n  severity ~ normal(0,tau);\n  \n  // Data probability (likelihood)\n  rating ~ normal(expRating,sigma);\n  \n  // Priors for sigma and tau\n  \n\n}",
    "crumbs": [
      "Multiple Raters in Stan"
    ]
  },
  {
    "objectID": "Bayesian/RatersStan.html#next-step-is-to-prep-the-data",
    "href": "Bayesian/RatersStan.html#next-step-is-to-prep-the-data",
    "title": "Multiple Raters in Stan",
    "section": "Next step is to prep the data",
    "text": "Next step is to prep the data\nPut it all into one big list. Make sure the names match the names in the data block.\n\nratings.data &lt;- list(\n  I = I, J=J, W=W,\n  applicant = ratings$applicant,\n  rater=ratings$rater,\n  rating=ratings$rating\n)",
    "crumbs": [
      "Multiple Raters in Stan"
    ]
  },
  {
    "objectID": "Bayesian/RatersStan.html#now-run-stan",
    "href": "Bayesian/RatersStan.html#now-run-stan",
    "title": "Multiple Raters in Stan",
    "section": "Now Run Stan",
    "text": "Now Run Stan\n\noptions(mc.cores = 5)\nratings.fit1 &lt;- sampling(\n  ratings.stan,       # The model\n  data = ratings.data, # The data\n  chains = 5,\n  warmup = 1000,\n  iter = 2000,\n  refresh=1000        # Show progress\n)\n\nWarning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#bulk-ess\n\nsummary(ratings.fit1)\n\n872  0.58242189    1.50963239\nability[28]       2.15142743 0.0113834004  0.54145576    1.17992499\nability[29]       5.07646476 0.0139145640  0.59979546    3.90277948\nability[30]       8.37901066 0.0149700298  0.58404216    7.25219934\nability[31]       8.79413183 0.0127106223  0.55816571    7.66159242\nability[32]       2.36942136 0.0127878932  0.56169152    1.31565629\nability[33]       9.03240739 0.0120497301  0.53671453    7.90708769\nability[34]       7.31549256 0.0127746801  0.59082074    6.14529293\nability[35]       5.50396292 0.0124982663  0.58559931    4.37085124\nability[36]       5.88965230 0.0124653864  0.59789604    4.71284970\nability[37]       4.00379246 0.0125803989  0.59005427    2.83653660\nability[38]       2.03461675 0.0121666682  0.54476375    1.10219421\nability[39]       2.26547435 0.0125693663  0.56303669    1.21385467\nability[40]       1.94600061 0.0100684566  0.50238902    1.09874368\nability[41]       7.76272815 0.0127333730  0.57846383    6.61810684\nability[42]       7.12627284 0.0133234143  0.56925111    6.00896497\nability[43]       8.54935777 0.0122689032  0.55804012    7.44006717\nability[44]       7.70195512 0.0136499301  0.60181088    6.51217563\nability[45]       5.91440809 0.0130858290  0.57749836    4.78082166\nability[46]       2.76030554 0.0140565683  0.59254868    1.62294383\nability[47]       9.19918933 0.0103378493  0.48776629    8.13934151\nability[48]       7.91116915 0.0144878012  0.58455242    6.74843433\nability[49]       2.89650834 0.0134988048  0.57359181    1.75126078\nability[50]       5.65308205 0.0127980529  0.59594693    4.48437629\nability[51]       1.64364482 0.0075431197  0.42457373    1.03986164\nability[52]       2.62339353 0.0137315883  0.57515710    1.50615389\nability[53]       6.62643679 0.0132064068  0.58907310    5.47157504\nability[54]       3.22207094 0.0118992415  0.57677152    2.09658385\nability[55]       3.39609412 0.0135191724  0.59345652    2.24636909\nability[56]       2.28125905 0.0118226096  0.56749722    1.23392169\nability[57]       4.90538859 0.0142162788  0.59081454    3.73398765\nability[58]       8.81414779 0.0139778556  0.54568435    7.70112806\nability[59]       4.29019564 0.0140266463  0.60100559    3.12713088\nability[60]       8.19062476 0.0143763766  0.58341940    7.04354524\nability[61]       2.45833538 0.0131571434  0.57256833    1.34207325\nability[62]       9.15986512 0.0098828352  0.49072950    8.10335225\nability[63]       9.05372377 0.0127315466  0.50483738    7.98949337\nability[64]       2.09319173 0.0129452754  0.54072012    1.14836924\nability[65]       6.59940689 0.0119944217  0.58191886    5.45944409\nability[66]       3.52408981 0.0138709399  0.59276097    2.37137253\nability[67]       2.49271757 0.0143883062  0.57816674    1.38840504\nability[68]       2.98993606 0.0131246591  0.58808571    1.81751584\nability[69]       9.27639044 0.0089042603  0.45592176    8.26035117\nability[70]       5.49563420 0.0135358061  0.59789885    4.35692508\nability[71]       4.74026518 0.0126661657  0.58530950    3.59380985\nability[72]       6.66817257 0.0140935162  0.59903840    5.48265522\nability[73]       3.45351199 0.0138414127  0.58283314    2.31791490\nability[74]       7.62415198 0.0135575726  0.59689335    6.47741311\nability[75]       8.71104186 0.0116753357  0.55257046    7.61556997\nability[76]       5.02685422 0.0126410116  0.59332186    3.84787436\nability[77]       3.59123960 0.0137062862  0.61622039    2.38701544\nability[78]       4.36241155 0.0123464981  0.61582992    3.15249594\nability[79]       6.78662407 0.0135968420  0.59191885    5.63885077\nability[80]       4.35703377 0.0123523800  0.59659258    3.16745286\nability[81]       6.43082497 0.0130899660  0.57283509    5.30252636\nability[82]       3.07777581 0.0130202817  0.58125400    1.98640136\nability[83]       5.29166682 0.0131552263  0.59755398    4.14990012\nability[84]       1.63249863 0.0085040608  0.42183841    1.02956718\nability[85]       5.67038580 0.0126687570  0.59366048    4.51170646\nability[86]       8.20066160 0.0131397793  0.57708216    7.05282700\nability[87]       2.34710718 0.0127910985  0.56230229    1.29986528\nability[88]       8.52065540 0.0126057211  0.57335365    7.32819247\nability[89]       6.60430319 0.0138252866  0.58789532    5.44139059\nability[90]       8.44169461 0.0137664539  0.56295270    7.32703140\nability[91]       3.16330399 0.0125919166  0.58574763    2.03982278\nability[92]       3.37503485 0.0120659388  0.59998869    2.19762813\nability[93]       4.01611753 0.0117872169  0.58214029    2.87197607\nability[94]       2.84683930 0.0132738718  0.58702129    1.66838926\nability[95]       7.75614424 0.0128061134  0.57682376    6.63659331\nability[96]       4.33501492 0.0127552955  0.57586244    3.18935915\nability[97]       5.49710411 0.0135026036  0.59292472    4.33488200\nability[98]       6.50479499 0.0121332377  0.58569710    5.32637722\nability[99]       4.42439555 0.0133156617  0.59334507    3.24279907\nability[100]      5.96322004 0.0125322826  0.58931203    4.80453415\nseverity[1]       0.30908805 0.0121889830  0.27735414   -0.23452467\nseverity[2]       0.74061737 0.0110793158  0.26774449    0.22188778\nseverity[3]      -1.70955711 0.0121850181  0.27800913   -2.25959618\nseverity[4]       1.59113107 0.0118869541  0.28501004    1.04888058\nseverity[5]      -0.70542157 0.0129257835  0.28276638   -1.26298941\nseverity[6]      -0.65294528 0.0129316265  0.28650421   -1.21190789\nseverity[7]      -2.56761960 0.0122281550  0.28765062   -3.12893051\nseverity[8]       0.44659361 0.0114129261  0.27146350   -0.08444225\nseverity[9]       1.47679195 0.0125347472  0.28157094    0.91486741\nseverity[10]      0.08875006 0.0123477744  0.28260846   -0.45278103\nsigma             0.93595535 0.0008482415  0.04890269    0.84725357\ntau               1.49291584 0.0057178107  0.42677435    0.91943001\nexpRating[1]      3.90567977 0.0061430851  0.56853599    2.77171093\nexpRating[2]      7.20636795 0.0061814008  0.56234209    6.06352092\nexpRating[3]      4.96229160 0.0060834498  0.56410642    3.86184764\nexpRating[4]      8.31334516 0.0074172303  0.56699361    7.20445486\nexpRating[5]      9.59538817 0.0072563167  0.56373404    8.50778568\nexpRating[6]      7.35131183 0.0075028950  0.56510095    6.23351026\nexpRating[7]      6.15374234 0.0060372679  0.56076803    5.03910243\nexpRating[8]      7.25328123 0.0059752672  0.55341876    6.17864620\nexpRating[9]      6.89543768 0.0060144297  0.55959296    5.80655073\nexpRating[10]     7.83198608 0.0059440992  0.57250853    6.72815030\nexpRating[11]     6.43842343 0.0055412720  0.56959618    5.30277827\nexpRating[12]     4.52374911 0.0057939550  0.56731041    3.42842515\nexpRating[13]     3.84453880 0.0063365230  0.55397649    2.74879633\nexpRating[14]     4.69505249 0.0064149180  0.55842218    3.59025243\nexpRating[15]     2.39849986 0.0063591782  0.55395115    1.30272194\nexpRating[16]     6.15112698 0.0056836466  0.56189194    5.05860525\nexpRating[17]     3.85457434 0.0057776477  0.56076878    2.78799697\nexpRating[18]     4.64874597 0.0055992682  0.56062933    3.56042985\nexpRating[19]     9.24137459 0.0071386708  0.52895653    8.13702207\nexpRating[20]     7.22272942 0.0072577307  0.52811946    6.11515969\nexpRating[21]     9.37888015 0.0068045393  0.52432326    8.28647495\nexpRating[22]     2.51168372 0.0056601216  0.55009759    1.42584967\nexpRating[23]     3.56829556 0.0058990955  0.55536990    2.47107979\nexpRating[24]     5.69803279 0.0058310840  0.55246543    4.63124167\nexpRating[25]     3.16072442 0.0066111112  0.56261175    2.07121769\nexpRating[26]     1.14207925 0.0067542802  0.56483062    0.06617715\nexpRating[27]     0.28401677 0.0065225288  0.55746500   -0.77480058\nexpRating[28]    10.39243385 0.0070532427  0.54683117    9.32468748\nexpRating[29]     9.24789639 0.0070968132  0.54505673    8.14722814\nexpRating[30]    10.27809474 0.0072366732  0.54751376    9.17746334\nexpRating[31]     7.27175037 0.0058961573  0.57092543    6.12332662\nexpRating[32]     7.15741126 0.0058468624  0.56415445    6.03695911\nexpRating[33]     5.76936937 0.0056753575  0.56796329    4.63950303\nexpRating[34]     7.40310162 0.0061318307  0.58257146    6.23951208\nexpRating[35]     8.25361532 0.0061337954  0.57685891    7.10896298\nexpRating[36]     5.95706268 0.0064478317  0.58015980    4.81145912\nexpRating[37]     2.41790558 0.0068714538  0.45385249    1.64147743\nexpRating[38]    -0.03226890 0.0072396286  0.45870442   -0.80816210\nexpRating[39]     2.12388182 0.0070092867  0.45812039    1.34017033\nexpRating[40]     9.62262210 0.0064536454  0.44827079    8.65966323\nexpRating[41]     8.66058876 0.0067706285  0.45299340    7.69401526\nexpRating[42]     9.76012765 0.0066069128  0.44834412    8.79470875\nexpRating[43]     6.12539406 0.0058980478  0.57176454    5.02509645\nexpRating[44]     3.24868641 0.0060465546  0.56805576    2.16092406\nexpRating[45]     7.29309796 0.0060338996  0.57100001    6.18589455\nexpRating[46]     8.25246696 0.0059996717  0.56799762    7.15140801\nexpRating[47]     6.00839061 0.0062212332  0.55590000    4.93549969\nexpRating[48]     4.09371629 0.0059853061  0.56356606    2.99361098\nexpRating[49]     5.95249540 0.0060228102  0.56671932    4.84076725\nexpRating[50]     2.64425843 0.0057844391  0.56398774    1.51858765\nexpRating[51]     5.30062808 0.0056873068  0.55884115    4.19795113\nexpRating[52]     1.24262985 0.0068573548  0.50919126    0.33043836\nexpRating[53]     3.42484337 0.0071856189  0.50850238    2.50486796\nexpRating[54]     2.03680148 0.0071305781  0.50954177    1.11355829\nexpRating[55]     4.23045322 0.0067227296  0.55271920    3.16023170\nexpRating[56]     1.98637687 0.0065185248  0.55414311    0.90217862\nexpRating[57]     0.07170255 0.0065614714  0.55811776   -1.02810545\nexpRating[58]     4.13972932 0.0060602852  0.56215180    3.02574591\nexpRating[59]     4.19220561 0.0058990824  0.56867898    3.07147299\nexpRating[60]     6.32194285 0.0059004538  0.56675034    5.22741394\nexpRating[61]     7.60334971 0.0058473687  0.56810789    6.46607992\nexpRating[62]     6.20978706 0.0060206233  0.56629067    5.08417249\nexpRating[63]     7.30932595 0.0057941897  0.57303420    6.13772831\nexpRating[64]     7.94429362 0.0062921175  0.56795499    6.80505761\nexpRating[65]     9.22633663 0.0063376271  0.57208159    8.09618199\nexpRating[66]     5.06758596 0.0062672191  0.56584798    3.95581955\nexpRating[67]     2.07636033 0.0065611199  0.56890066    0.98634047\nexpRating[68]     5.37704850 0.0060510022  0.56824977    4.28294378\nexpRating[69]     4.23251105 0.0069528624  0.56696176    3.14695527\nexpRating[70]     9.02292526 0.0074389866  0.55154214    7.92833228\nexpRating[71]     7.00428009 0.0075269833  0.54810288    5.90500084\nexpRating[72]    10.19062916 0.0074344488  0.54764922    9.06866201\nexpRating[73]     2.87117558 0.0074525389  0.52731766    1.90162207\nexpRating[74]     2.57715181 0.0073478299  0.52974421    1.57119339\nexpRating[75]     3.60735016 0.0075261571  0.53663472    2.60421385\nexpRating[76]     7.53772647 0.0055581236  0.55205222    6.46126209\nexpRating[77]     5.29365012 0.0056536985  0.55517748    4.21112446\nexpRating[78]     6.39318901 0.0056190054  0.55213555    5.34240714\nexpRating[79]     0.91673228 0.0066971678  0.56148841   -0.16161367\nexpRating[80]     4.21742046 0.0067599017  0.56245469    3.13651490\nexpRating[81]     4.10308135 0.0069141674  0.56300265    3.00159297\nexpRating[82]     2.89204480 0.0072530198  0.52977010    1.91102173\nexpRating[83]     2.59802104 0.0071281866  0.53330921    1.60021994\nexpRating[84]     2.24017749 0.0074248946  0.53254074    1.23478050\nexpRating[85]     4.37104319 0.0059041329  0.57618888    3.22902511\nexpRating[86]     6.55325672 0.0062387859  0.57232892    5.41090089\nexpRating[87]     5.16521482 0.0059489420  0.57381682    4.04998673\nexpRating[88]     8.68809871 0.0079860672  0.55867539    7.62205935\nexpRating[89]     7.72606538 0.0080299837  0.55782691    6.65035034\nexpRating[90]     9.85580261 0.0081662698  0.56617371    8.75035714\nexpRating[91]    10.38526289 0.0071923984  0.54990657    9.29142545\nexpRating[92]    10.27092378 0.0070697209  0.53747329    9.19503230\nexpRating[93]     8.88288189 0.0069666830  0.53949480    7.79356559\nexpRating[94]     0.65986425 0.0070189745  0.54096839   -0.36012800\nexpRating[95]     1.71647608 0.0071673439  0.53721460    0.66995691\nexpRating[96]     2.45817142 0.0070682260  0.53744914    1.45612498\nexpRating[97]    10.62353846 0.0073206118  0.53247804    9.53790068\nexpRating[98]     6.46478779 0.0076794626  0.52527594    5.39053836\nexpRating[99]    10.50919935 0.0074406309  0.52216269    9.41776422\nexpRating[100]    8.05610994 0.0062610820  0.56769717    6.93371972\nexpRating[101]    7.76208617 0.0059611099  0.55937727    6.68243195\nexpRating[102]    7.40424262 0.0061634676  0.57092936    6.30581190\nexpRating[103]    4.79854135 0.0059216114  0.55717897    3.71981960\nexpRating[104]    6.98075488 0.0061273596  0.56683724    5.85899243\nexpRating[105]    5.59271298 0.0061930395  0.56267517    4.50121006\nexpRating[106]    6.63026967 0.0058713433  0.57186652    5.52183897\nexpRating[107]    5.23670702 0.0061375153  0.57900093    4.09926270\nexpRating[108]    7.36644425 0.0064760187  0.57847742    6.23863671\nexpRating[109]    4.31288051 0.0057595510  0.56936826    3.17967064\nexpRating[110]    1.43617286 0.0059533527  0.56738152    0.35036523\nexpRating[111]    5.48058441 0.0059678191  0.56802525    4.38495291\nexpRating[112]    2.34370480 0.0073767571  0.53037088    1.38686135\nexpRating[113]    1.32919518 0.0074636478  0.53667111    0.35611519\nexpRating[114]   -0.53300285 0.0072839677  0.53353519   -1.49032999\nexpRating[115]    3.85660542 0.0075227813  0.55215233    2.81542542\nexpRating[116]    1.56005278 0.0075052199  0.54530216    0.52603937\nexpRating[117]    3.74226630 0.0078412805  0.54500803    2.71073629\nexpRating[118]    2.68661798 0.0063990980  0.50388324    1.78056298\nexpRating[119]    0.23644350 0.0066449191  0.50598772   -0.64706545\nexpRating[120]    3.42279256 0.0066603554  0.50963453    2.49608868\nexpRating[121]    8.50334552 0.0063936055  0.56074573    7.41125539\nexpRating[122]    6.05317104 0.0064847327  0.55495926    4.96388377\nexpRating[123]    7.85147821 0.0064379543  0.55510160    6.77086347\nexpRating[124]    7.43536089 0.0064518865  0.54589114    6.37184694\nexpRating[125]    6.42085126 0.0062563315  0.54187006    5.32969736\nexpRating[126]    8.60306479 0.0064913144  0.55106998    7.51052679\nexpRating[127]    6.83980065 0.0066365946  0.54259854    5.75986564\nexpRating[128]    8.99595138 0.0066280463  0.54205126    7.92146036\nexpRating[129]    8.63810783 0.0065771780  0.53709939    7.54913267\nexpRating[130]    5.99239801 0.0064310206  0.57565420    4.85382394\nexpRating[131]    9.17874707 0.0063812540  0.57429110    8.03400349\nexpRating[132]    7.79070518 0.0064532435  0.57649591    6.65947188\nexpRating[133]    7.50553916 0.0054144389  0.55469563    6.43142119\nexpRating[134]    5.20898652 0.0056226535  0.55686691    4.11674470\nexpRating[135]    6.36100170 0.0055871642  0.55882288    5.25911734\nexpRating[136]    2.05488397 0.0064373739  0.56097126    0.99904589\nexpRating[137]    2.10736026 0.0063307591  0.56571024    1.01984831\nexpRating[138]    0.19268594 0.0064026961  0.56519884   -0.92330987\nexpRating[139]    9.50827738 0.0070645646  0.48753202    8.50412658\nexpRating[140]    8.49376776 0.0073838426  0.49496195    7.47249048\nexpRating[141]    9.64578294 0.0069611904  0.49291156    8.60983173\nexpRating[142]    7.20574758 0.0060636452  0.55760183    6.08190717\nexpRating[143]    9.38796110 0.0061516405  0.55861863    8.26603983\nexpRating[144]    7.99991921 0.0060023037  0.55637559    6.90457571\nexpRating[145]    4.48763940 0.0067966564  0.55006498    3.38518396\nexpRating[146]    0.32888874 0.0068925240  0.54753768   -0.75934243\nexpRating[147]    2.98525839 0.0066841612  0.54110087    1.91216089\nexpRating[148]    4.94766048 0.0055935241  0.56623861    3.84558963\nexpRating[149]    3.08546245 0.0055675627  0.57157677    1.96248107\nexpRating[150]    7.12987401 0.0053818682  0.56607086    5.99817990\nexpRating[151]    1.95273288 0.0082086754  0.45687783    1.15079454\nexpRating[152]    2.38426220 0.0073609777  0.45594761    1.58223261\nexpRating[153]    3.12043678 0.0088721474  0.45565925    2.30285423\nexpRating[154]    0.91383642 0.0063975429  0.54514847   -0.14910751\nexpRating[155]    1.97044825 0.0065250875  0.55542207    0.89855733\nexpRating[156]    4.10018548 0.0068017182  0.55203257    3.02913293\nexpRating[157]    5.92101522 0.0060412865  0.56593346    4.79231893\nexpRating[158]    4.05881720 0.0063087714  0.56541697    2.94976224\nexpRating[159]    8.10322875 0.0059948307  0.56765162    6.98930467\nexpRating[160]    3.96268831 0.0064877130  0.55414590    2.84236037\nexpRating[161]    1.51251382 0.0064258392  0.55395159    0.42288598\nexpRating[162]    2.51664936 0.0065732661  0.55791023    1.41141940\nexpRating[163]    3.70518217 0.0066680906  0.57305206    2.57849266\nexpRating[164]    2.69067254 0.0063596932  0.56789618    1.58150064\nexpRating[165]    4.87288607 0.0067546585  0.57717701    3.74587152\nexpRating[166]    3.87239012 0.0069613040  0.55174367    2.85005153\nexpRating[167]   -0.28636055 0.0075591038  0.55761184   -1.34613950\nexpRating[168]    2.72785266 0.0072946267  0.55007726    1.68050466\nexpRating[169]    3.19583147 0.0057514172  0.56170226    2.09515502\nexpRating[170]    2.33776899 0.0058293116  0.56155898    1.25570544\nexpRating[171]    4.99413864 0.0057031232  0.55936492    3.89426587\nexpRating[172]    7.10459067 0.0074653624  0.52866155    6.06786249\nexpRating[173]    8.10872622 0.0075145307  0.53614528    7.00185820\nexpRating[174]    8.90289785 0.0074363549  0.53077181    7.84584172\nexpRating[175]    3.63725036 0.0058756304  0.57042529    2.52411070\nexpRating[176]    1.72257604 0.0061769915  0.57491807    0.61085742\nexpRating[177]    4.37894570 0.0060331853  0.56851454    3.28338256\nexpRating[178]    6.48106765 0.0076101669  0.55440924    5.41004864\nexpRating[179]    9.78175583 0.0077443035  0.56313581    8.65897132\nexpRating[180]    9.66741672 0.0079951691  0.56647925    8.55844697\nexpRating[181]    2.76742343 0.0086061854  0.55196196    1.70369073\nexpRating[182]    1.80539010 0.0080348524  0.54859287    0.72298590\nexpRating[183]    2.90492899 0.0081733627  0.55055259    1.82424794\nexpRating[184]    9.46895317 0.0067292110  0.49228843    8.44476305\nexpRating[185]    9.90048249 0.0066614737  0.49706518    8.87425439\nexpRating[186]    9.60645873 0.0065754074  0.49419147    8.58023690\nexpRating[187]   10.64485483 0.0070104744  0.49810674    9.61306252\nexpRating[188]    8.34830219 0.0070152858  0.49891669    7.30748509\nexpRating[189]   10.53051572 0.0070167744  0.50008450    9.49972213\nexpRating[190]    2.40227978 0.0071475155  0.52910837    1.41421655\nexpRating[191]    1.44024645 0.0069386616  0.53478581    0.45965748\nexpRating[192]    3.56998368 0.0067952207  0.52552964    2.59106188\nexpRating[193]    6.90849495 0.0058106744  0.56378360    5.80300100\nexpRating[194]    7.34002426 0.0058300193  0.56290517    6.24729707\nexpRating[195]    8.19053796 0.0058383632  0.57062217    7.08178481\nexpRating[196]    3.83317787 0.0065729303  0.56837687    2.72338678\nexpRating[197]    2.81866824 0.0063213232  0.56836380    1.68579159\nexpRating[198]    0.95647021 0.0068232559  0.56317302   -0.16513424\nexpRating[199]    0.78316046 0.0070103045  0.55457829   -0.27501662\nexpRating[200]    4.08384864 0.0073821820  0.55848058    3.02157513\nexpRating[201]   -0.07490203 0.0071971591  0.55473491   -1.13932139\nexpRating[202]    3.73055344 0.0076180875  0.56509374    2.61018712\nexpRating[203]    4.58106713 0.0074315047  0.56797217    3.45914591\nexpRating[204]    0.42231646 0.0077079851  0.56837291   -0.69811588\nexpRating[205]    9.58547849 0.0059910401  0.46799523    8.59126316\nexpRating[206]   10.01700781 0.0058811675  0.46090593    9.04683647\nexpRating[207]    9.72298405 0.0058765444  0.46388726    8.75944957\nexpRating[208]    7.08676527 0.0055862277  0.58029435    5.99499728\nexpRating[209]    4.84268892 0.0059655490  0.56921279    3.75681218\nexpRating[210]    5.58438426 0.0057168068  0.56945868    4.49980182\nexpRating[211]    5.04935324 0.0057609061  0.56353296    3.95565266\nexpRating[212]    5.48088256 0.0059971115  0.56583930    4.38012535\nexpRating[213]    3.03070807 0.0062793602  0.57135512    1.92761110\nexpRating[214]    4.95861545 0.0057092831  0.56975420    3.83719835\nexpRating[215]    5.96275099 0.0058012743  0.57273031    4.81427883\nexpRating[216]    6.75692263 0.0055624744  0.57007217    5.62943412\nexpRating[217]    5.04464305 0.0066857071  0.55569697    3.95595653\nexpRating[218]    2.74809042 0.0065587595  0.56100378    1.63772840\nexpRating[219]    3.54226205 0.0064919611  0.56287438    2.41868650\nexpRating[220]    8.36476935 0.0074159663  0.57184783    7.26774045\nexpRating[221]    5.05653238 0.0070030861  0.57485689    3.93964462\nexpRating[222]    7.71290204 0.0069191079  0.57656978    6.61215701\nexpRating[223]    9.45165924 0.0074574668  0.54285478    8.38363170\nexpRating[224]    7.00148475 0.0074528852  0.54131228    5.92239243\nexpRating[225]    9.15763548 0.0074803000  0.53340396    8.10568249\nexpRating[226]    5.33594227 0.0060182171  0.57256507    4.23963438\nexpRating[227]    3.31729711 0.0059724517  0.57217239    2.16662276\nexpRating[228]    4.32143265 0.0059570835  0.57353599    3.20379292\nexpRating[229]    1.88168249 0.0075781744  0.59864623    0.71967090\nexpRating[230]    2.88581803 0.0077166753  0.59064285    1.74251279\nexpRating[231]    1.02362000 0.0074612754  0.59341291   -0.10440487\nexpRating[232]    4.67149960 0.0063523869  0.60028794    3.47011534\nexpRating[233]    5.95354261 0.0060805912  0.58836905    4.78461573\nexpRating[234]    1.79479195 0.0060438119  0.59426062    0.61138129\nexpRating[235]    7.09571212 0.0058491954  0.56995711    5.96652081\nexpRating[236]    5.07706696 0.0058974141  0.57009516    3.97792399\nexpRating[237]    6.08120250 0.0057540104  0.57417238    4.95144780\nexpRating[238]    2.64747666 0.0061431599  0.57755354    1.51759331\nexpRating[239]    4.80362738 0.0058845139  0.57451002    3.67018339\nexpRating[240]    4.44578383 0.0057822506  0.57258250    3.30108730\nexpRating[241]    4.72126785 0.0058601796  0.55516257    3.60886965\nexpRating[242]    3.86320537 0.0059730372  0.55591591    2.76676672\nexpRating[243]    6.87741858 0.0059296590  0.54815638    5.78411173\nexpRating[244]    3.38686386 0.0064870448  0.56066451    2.31825645\nexpRating[245]    2.37235424 0.0061431063  0.56608437    1.30266364\nexpRating[246]    3.16652587 0.0060668812  0.56340838    2.10424745\nexpRating[247]    5.60075487 0.0059289132  0.57303630    4.47338388\nexpRating[248]    4.63872154 0.0058243972  0.57012702    3.51696599\nexpRating[249]    5.38041688 0.0057079447  0.56597506    4.26168205\nexpRating[250]    1.94158668 0.0066559610  0.44567172    1.15303013\nexpRating[251]    2.37311600 0.0063808562  0.44300516    1.61853382\nexpRating[252]   -0.07705848 0.0071402286  0.44559533   -0.86177577\nexpRating[253]    3.10276620 0.0057249522  0.57532932    1.93798867\nexpRating[254]    6.11697941 0.0058423447  0.57399281    4.98311720\nexpRating[255]    5.75913586 0.0060692142  0.57166746    4.65139611\nexpRating[256]    6.49110449 0.0065950964  0.55581352    5.38835837\nexpRating[257]    5.63304200 0.0066732460  0.55230976    4.55301923\nexpRating[258]    8.28941166 0.0067741646  0.55409407    7.19586593\nexpRating[259]    2.65619523 0.0076119490  0.54228597    1.63458935\nexpRating[260]   -0.22051242 0.0076772293  0.54064358   -1.24630938\nexpRating[261]    2.43585724 0.0075804453  0.54669600    1.40985180\nexpRating[262]    7.81523382 0.0070252985  0.55003540    6.70363251\nexpRating[263]    7.86771012 0.0069648690  0.55189831    6.74962939\nexpRating[264]    5.95303580 0.0068911471  0.54921595    4.84942340\nexpRating[265]    5.95135792 0.0055678536  0.56381392    4.83182637\nexpRating[266]    4.03668360 0.0057332432  0.56376719    2.92015098\nexpRating[267]    6.69305325 0.0055766314  0.56622348    5.55996295\nexpRating[268]    7.73627303 0.0064245596  0.54116857    6.68263681\nexpRating[269]    5.87407501 0.0064477256  0.54304308    4.79050453\nexpRating[270]    8.88828822 0.0062178280  0.54126929    7.80940277\nexpRating[271]    3.90392136 0.0064697158  0.56563992    2.80266277\nexpRating[272]    2.45788242 0.0065859374  0.57030021    1.33882137\nexpRating[273]    2.51035871 0.0063383159  0.56853248    1.40247222\nexpRating[274]    3.68412290 0.0062828118  0.58435880    2.54771789\nexpRating[275]    4.11565222 0.0062039199  0.57812918    2.99101026\nexpRating[276]    3.82162846 0.0063880264  0.58231164    2.66379378\nexpRating[277]    4.75673490 0.0053517391  0.56001034    3.66647625\nexpRating[278]    5.60724860 0.0053656222  0.56793983    4.50635117\nexpRating[279]    4.46271114 0.0054819999  0.56013401    3.36786553\nexpRating[280]    3.15592735 0.0074648483  0.56614062    2.05345076\nexpRating[281]    4.43797037 0.0074097476  0.56224793    3.35524401\nexpRating[282]    2.19389402 0.0075154576  0.55312531    1.11657535\nexpRating[283]    8.49676161 0.0066535151  0.54903275    7.43129173\nexpRating[284]    9.34727531 0.0067316524  0.55906312    8.26450908\nexpRating[285]    8.20273785 0.0067228570  0.55855370    7.08148454\nexpRating[286]    5.07563229 0.0057496407  0.55253962    3.99928529\nexpRating[287]    3.68206964 0.0059982492  0.55687711    2.57849815\nexpRating[288]    4.78160853 0.0058310652  0.54971055    3.72647016\nexpRating[289]    6.23772149 0.0062180796  0.56275335    5.13934761\nexpRating[290]    4.84415883 0.0056388329  0.57017847    3.74925170\nexpRating[291]    5.94369772 0.0062858769  0.57298041    4.84870387\nexpRating[292]    7.24541236 0.0062318164  0.56901528    6.11374451\nexpRating[293]    5.85184971 0.0060036443  0.56389403    4.73802328\nexpRating[294]    6.95138860 0.0059134175  0.56292193    5.82664595\nexpRating[295]    5.16501292 0.0061029508  0.56717756    4.03306209\nexpRating[296]    3.77145027 0.0059921689  0.56147788    2.67876539\nexpRating[297]    4.87098916 0.0062754530  0.56355380    3.75062660\nexpRating[298]    4.25366293 0.0058390711  0.57452817    3.14425466\nexpRating[299]    5.31027476 0.0056936910  0.56197695    4.19615186\nexpRating[300]    7.44001200 0.0058727637  0.57176604    6.31357894\nlp__           -101.96765723 0.2973973912 10.10816939 -123.31741481\n                         25%           50%          75%       97.5%      n_eff\nability[1]        5.21241217    5.60889534   6.00524787   6.7916230  2070.2619\nability[2]        7.60564176    8.00872269   8.39386769   9.1716225  1813.5552\nability[3]        6.42438614    6.80062884   7.18601181   7.9699684  2024.6995\nability[4]        6.69618307    7.10632100   7.48450222   8.2541269  2077.7351\nability[5]        2.71408626    3.10075057   3.48890676   4.2427265  2105.0715\nability[6]        4.15806370    4.56611658   4.95467876   5.7258472  1919.6246\nability[7]        8.57240225    8.97070062   9.33052526   9.8682609  1984.6870\nability[8]        3.83716796    4.20805068   4.60895685   5.3504502  1683.3962\nability[9]        2.43668248    2.84511398   3.25916843   4.0374575  1868.0454\nability[10]       8.42384395    8.82409702   9.20201450   9.8070742  2090.2177\nability[11]       5.29166731    5.67500789   6.06609137   6.8399706  2124.9797\nability[12]       6.24852880    6.66663969   7.06740994   7.8500266  2027.3795\nability[13]       1.32826028    1.61454835   1.96533368   2.6701021  2942.3288\nability[14]       9.03653638    9.37296120   9.65572960   9.9491088  2761.0087\nability[15]       5.41231140    5.81375164   6.21158301   6.9935053  2253.2527\nability[16]       6.26805166    6.66322684   7.06359004   7.8056560  2263.2348\nability[17]       4.83958424    5.21375720   5.59575492   6.3470377  2022.9380\nability[18]       1.55151614    1.91325382   2.29490184   3.0162637  2485.7569\nability[19]       2.23808956    2.63559735   3.03349848   3.7979460  2114.7823\nability[20]       4.45337281    4.84295146   5.24098302   6.0012937  1716.8701\nability[21]       6.47418343    6.86665893   7.25459321   8.0328482  2355.5648\nability[22]       7.23229768    7.64905872   8.03794366   8.8034043  1839.8409\nability[23]       3.39236036    3.79680902   4.18387703   4.9174884  2327.0556\nability[24]       8.35011894    8.72761123   9.11230553   9.7491403  2052.4077\nability[25]       1.73822144    2.10795873   2.50527331   3.2224026  1915.1915\nability[26]       5.54962776    5.94186034   6.34539839   7.0424062  2283.5670\nability[27]       2.22204211    2.61721656   3.02970497   3.7603958  1802.8956\nability[28]       1.75937809    2.13354869   2.51660473   3.2508707  2262.4653\nability[29]       4.67510176    5.08212146   5.48745968   6.2604283  1858.0917\nability[30]       7.97740694    8.37421026   8.77372832   9.5301227  1522.0996\nability[31]       8.41908305    8.80853057   9.19553608   9.8108966  1928.3802\nability[32]       1.97298816    2.36017870   2.76251097   3.4909486  1929.2910\nability[33]       8.66015928    9.06844984   9.44824012   9.9239350  1983.9561\nability[34]       6.92285035    7.31704249   7.71758159   8.4335241  2139.0030\nability[35]       5.10383012    5.51126505   5.89287113   6.6316702  2195.3389\nability[36]       5.49603391    5.88872231   6.27842367   7.0647472  2300.5934\nability[37]       3.61363485    4.01130928   4.40077267   5.1509298  2199.8603\nability[38]       1.62645034    2.01064913   2.39286612   3.1932620  2004.8093\nability[39]       1.86894641    2.25706834   2.64573986   3.3807655  2006.5345\nability[40]       1.56984678    1.91381512   2.28377757   3.0096055  2489.7427\nability[41]       7.36968600    7.76300165   8.16012856   8.8842644  2063.7899\nability[42]       6.73874301    7.12239848   7.50751591   8.2353925  1825.4784\nability[43]       8.17741152    8.54801890   8.93412670   9.6107224  2068.8041\nability[44]       7.29533354    7.70310949   8.10843743   8.8452398  1943.8342\nability[45]       5.52739271    5.91860177   6.30208899   7.0196671  1947.5967\nability[46]       2.34716854    2.75978850   3.18293054   3.9308569  1777.0083\nability[47]       8.87832598    9.24542327   9.57400476   9.9509056  2226.1949\nability[48]       7.51919615    7.92624646   8.31326607   9.0236525  1627.9534\nability[49]       2.52351623    2.90271650   3.28621589   4.0126366  1805.5739\nability[50]       5.25394686    5.65803373   6.05360045   6.8224693  2168.3399\nability[51]       1.32163747    1.57613527   1.90718884   2.6047363  3168.1391\nability[52]       2.22241517    2.62753260   3.00682677   3.7682595  1754.4113\nability[53]       6.22929130    6.62601582   7.02204215   7.7622097  1989.6150\nability[54]       2.83487163    3.22364471   3.60011756   4.3583988  2349.4654\nability[55]       2.99581067    3.39805815   3.79335496   4.5361045  1926.9817\nability[56]       1.88812631    2.26475849   2.65839099   3.4401508  2304.0971\nability[57]       4.51838766    4.90619136   5.30918721   6.0528161  1727.1517\nability[58]       8.44349743    8.84136393   9.20928257   9.8339837  1524.0594\nability[59]       3.87325246    4.28912497   4.68730688   5.4974401  1835.9013\nability[60]       7.79674111    8.18091066   8.57970580   9.3480982  1646.8834\nability[61]       2.05953885    2.46393756   2.82712744   3.6265726  1893.7883\nability[62]       8.82986897    9.20393762   9.52860068   9.9422758  2465.5921\nability[63]       8.71825259    9.08891466   9.44098543   9.8926347  1572.3191\nability[64]       1.69663922    2.07509059   2.45815251   3.1945450  1744.7068\nability[65]       6.20211718    6.59576251   7.00477639   7.7504085  2353.7820\nability[66]       3.12295580    3.51194746   3.92197046   4.7033751  1826.1961\nability[67]       2.09332296    2.48568800   2.88356263   3.6394931  1614.6815\nability[68]       2.60754579    2.99322519   3.39051186   4.0953321  2007.7295\nability[69]       8.97453763    9.33127032   9.63327433   9.9551011  2621.7118\nability[70]       5.08578555    5.49414202   5.89141792   6.6918657  1951.1344\nability[71]       4.33658662    4.73637602   5.13886076   5.8801754  2135.4077\nability[72]       6.26218189    6.66219248   7.06600498   7.8617287  1806.6358\nability[73]       3.05745749    3.45227268   3.84087768   4.6289707  1773.0772\nability[74]       7.21811869    7.63025705   8.01884109   8.8179475  1938.3384\nability[75]       8.33233795    8.72055494   9.09990978   9.7699262  2239.9410\nability[76]       4.62992875    5.02861407   5.42073471   6.1688616  2203.0129\nability[77]       3.16924328    3.60095482   4.00216627   4.8105976  2021.3082\nability[78]       3.94555819    4.36130820   4.78081075   5.5752612  2487.9061\nability[79]       6.39119728    6.77833771   7.18685596   7.9698210  1895.1703\nability[80]       3.96226025    4.36141666   4.74766142   5.5446625  2332.6759\nability[81]       6.03969043    6.42807957   6.82081998   7.5684578  1915.0592\nability[82]       2.66669430    3.07196651   3.46954476   4.2275215  1992.9259\nability[83]       4.87441031    5.29036174   5.70091449   6.4533737  2063.2775\nability[84]       1.30272646    1.57631022   1.89664237   2.5933816  2460.5916\nability[85]       5.27054355    5.67927939   6.06436526   6.8410802  2195.8783\nability[86]       7.82408874    8.20654210   8.58749667   9.3284167  1928.8531\nability[87]       1.94813606    2.33770733   2.72165088   3.4829917  1932.5201\nability[88]       8.14852924    8.52922943   8.92653250   9.5703069  2068.7584\nability[89]       6.21672850    6.60028276   6.99568574   7.7805907  1808.2219\nability[90]       8.06052648    8.44206243   8.82293332   9.5507654  1672.2439\nability[91]       2.77152613    3.15667244   3.55592018   4.3214444  2163.9010\nability[92]       2.97391035    3.37220128   3.78436543   4.5347754  2472.6571\nability[93]       3.62528315    4.01600000   4.41244487   5.1731374  2439.1177\nability[94]       2.45687520    2.85737534   3.24133119   3.9626388  1955.7461\nability[95]       7.37349707    7.74978186   8.14113278   8.9041141  2028.8578\nability[96]       3.94318555    4.34404460   4.72929266   5.4563621  2038.2453\nability[97]       5.10658926    5.48884251   5.89231714   6.6841575  1928.2533\nability[98]       6.11425808    6.50981842   6.90378019   7.6384898  2330.1975\nability[99]       4.02696013    4.42488404   4.83191413   5.5989754  1985.5882\nability[100]      5.55977222    5.96079691   6.35184504   7.1180390  2211.2114\nseverity[1]       0.11712065    0.30899832   0.49838391   0.8537972   517.7670\nseverity[2]       0.56214293    0.73534761   0.91535482   1.2828469   584.0032\nseverity[3]      -1.89554419   -1.70811168  -1.52438601  -1.1570710   520.5540\nseverity[4]       1.39730965    1.58987977   1.78782183   2.1663954   574.8826\nseverity[5]      -0.89485460   -0.70801482  -0.50913860  -0.1494895   478.5660\nseverity[6]      -0.84539144   -0.65164979  -0.46262842  -0.0935654   490.8578\nseverity[7]      -2.76402417   -2.56863850  -2.37226500  -1.9962851   553.3613\nseverity[8]       0.26266415    0.45071407   0.62631615   0.9798780   565.7556\nseverity[9]       1.28585735    1.47949070   1.66784795   2.0272494   504.5968\nseverity[10]     -0.10386697    0.09200163   0.28011863   0.6394883   523.8331\nsigma             0.90201981    0.93442656   0.96763175   1.0383131  3323.7345\ntau               1.20546595    1.40466186   1.68936310   2.5523466  5571.0503\nexpRating[1]      3.51648470    3.90343214   4.29508998   5.0270015  8565.3058\nexpRating[2]      6.83175713    7.20653344   7.58449739   8.3235224  8276.1314\nexpRating[3]      4.58653266    4.96801345   5.33540150   6.0912726  8598.4906\nexpRating[4]      7.92878123    8.30808409   8.69644797   9.4155161  5843.4967\nexpRating[5]      9.21240346    9.58791872   9.96367177  10.7369213  6035.5389\nexpRating[6]      6.97128528    7.34914483   7.72461834   8.4797712  5672.7591\nexpRating[7]      5.78085129    6.15613662   6.53015280   7.2630995  8627.5126\nexpRating[8]      6.89317691    7.24879065   7.62427267   8.3399917  8578.1393\nexpRating[9]      6.52693559    6.89441652   7.27618180   8.0017850  8656.7640\nexpRating[10]     7.44924347    7.83588810   8.21796709   8.9545063  9276.6640\nexpRating[11]     6.05415098    6.44326824   6.81425464   7.5411575 10566.1117\nexpRating[12]     4.14111743    4.52412444   4.90710407   5.6224449  9587.1886\nexpRating[13]     3.47653426    3.85736566   4.20759145   4.9529253  7643.2951\nexpRating[14]     4.32997669    4.69897040   5.05482385   5.8141015  7577.7994\nexpRating[15]     2.01997437    2.40931261   2.77272796   3.4964587  7588.2378\nexpRating[16]     5.76860929    6.15070958   6.53215090   7.2461874  9773.5307\nexpRating[17]     3.47558762    3.84232687   4.24139125   4.9487496  9420.3175\nexpRating[18]     4.26012628    4.64888152   5.02962691   5.7227055 10025.1088\nexpRating[19]     8.88917309    9.27041913   9.60933222  10.2122748  5490.4161\nexpRating[20]     6.87220742    7.25142653   7.59212396   8.1936895  5294.9595\nexpRating[21]     9.02512189    9.40126676   9.75617326  10.3298001  5937.4614\nexpRating[22]     2.14402551    2.51331860   2.87688583   3.5718950  9445.5654\nexpRating[23]     3.19905160    3.55817392   3.95138128   4.6689550  8863.2665\nexpRating[24]     5.32673103    5.69172547   6.06741561   6.7576334  8976.5949\nexpRating[25]     2.76162875    3.16111092   3.54343056   4.2675533  7242.1691\nexpRating[26]     0.74710277    1.13997805   1.53072251   2.2453941  6993.2382\nexpRating[27]    -0.10878296    0.28722601   0.66779825   1.3743810  7304.7137\nexpRating[28]    10.02331351   10.41097291  10.78547012  11.3967535  6010.7527\nexpRating[29]     8.87944059    9.25502405   9.63915955  10.2549484  5898.7049\nexpRating[30]     9.91770448   10.29268280  10.66638431  11.2924616  5724.1652\nexpRating[31]     6.88896291    7.27073269   7.63863852   8.3992209  9376.0658\nexpRating[32]     6.78427692    7.15150441   7.52924068   8.2660660  9310.0129\nexpRating[33]     5.39248920    5.77013003   6.14708834   6.8859898 10015.0721\nexpRating[34]     7.01696218    7.40046659   7.79611832   8.5591704  9026.4733\nexpRating[35]     7.86409752    8.25406493   8.64332797   9.3655166  8844.6498\nexpRating[36]     5.56680752    5.96121335   6.35066411   7.0767720  8095.9513\nexpRating[37]     2.08871763    2.38288637   2.70428271   3.4053052  4362.4673\nexpRating[38]    -0.36005943   -0.06739656   0.26247608   0.9325350  4014.5167\nexpRating[39]     1.79594449    2.08304381   2.41946049   3.1071539  4271.8067\nexpRating[40]     9.33758299    9.64859117   9.93992717  10.4287105  4824.7040\nexpRating[41]     8.37586741    8.68682452   8.98323927   9.4697286  4476.3679\nexpRating[42]     9.48048926    9.78330159  10.07913816  10.5569473  4604.9596\nexpRating[43]     5.74217735    6.12009842   6.51098959   7.2777303  9397.6194\nexpRating[44]     2.86548271    3.23783353   3.62207451   4.3836304  8826.0418\nexpRating[45]     6.91433251    7.28839447   7.67193750   8.4309053  8955.2162\nexpRating[46]     7.87231903    8.26249288   8.62874346   9.3652923  8962.6833\nexpRating[47]     5.62836356    6.01626924   6.38005876   7.1009456  7984.3651\nexpRating[48]     3.71177621    4.09775089   4.47949589   5.1842692  8865.7796\nexpRating[49]     5.57234248    5.95758524   6.33730886   7.0737449  8853.9627\nexpRating[50]     2.27553707    2.64874245   3.01541505   3.7597324  9506.4163\nexpRating[51]     4.93212119    5.30028994   5.66896709   6.4187304  9655.2485\nexpRating[52]     0.87573015    1.22020153   1.58840227   2.2696202  5513.7700\nexpRating[53]     3.05947268    3.40001467   3.77524992   4.4642908  5007.9227\nexpRating[54]     1.67298170    2.00561844   2.38491584   3.0863980  5106.3445\nexpRating[55]     3.85907769    4.24030118   4.60584888   5.2986830  6759.5505\nexpRating[56]     1.60394657    1.98905587   2.35984771   3.0586184  7226.7864\nexpRating[57]    -0.30247995    0.07727046   0.45130937   1.1483067  7235.1777\nexpRating[58]     3.75813405    4.13934754   4.51574690   5.2688798  8604.4097\nexpRating[59]     3.81131094    4.19216079   4.56963680   5.2938018  9293.2031\nexpRating[60]     5.94178491    6.32436977   6.70142774   7.4285569  9225.9852\nexpRating[61]     7.23243104    7.60783666   7.96573419   8.7492110  9439.3194\nexpRating[62]     5.84491178    6.21162719   6.58295056   7.3101841  8846.9977\nexpRating[63]     6.93930247    7.31085439   7.68618602   8.4282058  9780.8290\nexpRating[64]     7.57021243    7.95702757   8.31874838   9.0519325  8147.6841\nexpRating[65]     8.85089651    9.23050614   9.62329741  10.3363630  8148.2170\nexpRating[66]     4.68619508    5.06674131   5.46281744   6.1795047  8151.7300\nexpRating[67]     1.69402127    2.07712358   2.45992006   3.1826795  7518.2529\nexpRating[68]     4.99386774    5.37290988   5.75846671   6.4918409  8819.0927\nexpRating[69]     3.84733836    4.23870364   4.61347117   5.3582371  6649.3664\nexpRating[70]     8.65491212    9.03298850   9.40218773  10.0515103  5497.0521\nexpRating[71]     6.63231137    7.01176920   7.38470983   8.0347899  5302.5192\nexpRating[72]     9.81985776   10.20804842  10.57629582  11.2063214  5426.3450\nexpRating[73]     2.49367131    2.85507449   3.23142021   3.9204631  5006.5221\nexpRating[74]     2.20236324    2.56560453   2.94167739   3.6205933  5197.7363\nexpRating[75]     3.23132897    3.59537647   3.96644830   4.6984645  5084.0637\nexpRating[76]     7.16496808    7.54467249   7.92463508   8.6031975  9865.1551\nexpRating[77]     4.91232726    5.29225909   5.68480625   6.3551011  9642.6943\nexpRating[78]     6.01184346    6.39359337   6.77558004   7.4560659  9655.4498\nexpRating[79]     0.52976140    0.90375703   1.30080913   2.0074226  7029.0921\nexpRating[80]     3.83109097    4.21394359   4.60396177   5.3391963  6922.9998\nexpRating[81]     3.72439460    4.09924995   4.48376807   5.2097318  6630.4209\nexpRating[82]     2.51998031    2.89009007   3.25322523   3.9538823  5335.0337\nexpRating[83]     2.21983357    2.58717888   2.95737574   3.6883735  5597.5766\nexpRating[84]     1.86952633    2.22330533   2.59557780   3.2969456  5144.2858\nexpRating[85]     3.98185816    4.37237906   4.75836350   5.5170779  9523.9586\nexpRating[86]     6.16723115    6.55222153   6.93132755   7.6737240  8415.7189\nexpRating[87]     4.77280068    5.16342960   5.54787570   6.2963895  9303.9438\nexpRating[88]     8.30044061    8.69707686   9.07032025   9.7899640  4893.8782\nexpRating[89]     7.34264319    7.72980259   8.10795280   8.8109912  4825.8031\nexpRating[90]     9.45890087    9.86638840  10.23597643  10.9747232  4806.7546\nexpRating[91]    10.01962391   10.39525227  10.77945364  11.4206455  5845.6168\nexpRating[92]     9.91182397   10.28431300  10.64791238  11.2592729  5779.7524\nexpRating[93]     8.52418174    8.89790789   9.25437538   9.8865024  5996.8395\nexpRating[94]     0.28173826    0.65293152   1.02611193   1.7338662  5940.1370\nexpRating[95]     1.34188409    1.72016175   2.07666918   2.7785604  5617.9664\nexpRating[96]     2.07566426    2.45265261   2.82793327   3.5312838  5781.6780\nexpRating[97]    10.26657914   10.64812144  10.99762346  11.5868273  5290.6452\nexpRating[98]     6.11380249    6.48486749   6.84387343   7.4142129  4678.5724\nexpRating[99]    10.17155017   10.53257085  10.87834856  11.4384677  4924.8402\nexpRating[100]    7.66747976    8.05033212   8.44457228   9.1605044  8221.1897\nexpRating[101]    7.38804528    7.76294558   8.12853197   8.8709594  8805.5272\nexpRating[102]    7.02535840    7.40445673   7.78613417   8.5196464  8580.5380\nexpRating[103]    4.41200690    4.80680503   5.17872568   5.8771906  8853.3904\nexpRating[104]    6.58747206    6.98412279   7.36686449   8.0770188  8557.9554\nexpRating[105]    5.20861287    5.58956450   5.97627178   6.6725978  8254.8236\nexpRating[106]    6.26806249    6.62954166   7.00161613   7.7589185  9486.6830\nexpRating[107]    4.85149873    5.23078853   5.62134867   6.3841384  8899.6584\nexpRating[108]    6.98037816    7.37310280   7.75295394   8.5131991  7979.1500\nexpRating[109]    3.93336134    4.32249425   4.69382672   5.4313852  9772.5802\nexpRating[110]    1.05878006    1.43564303   1.80586617   2.5308980  9082.9547\nexpRating[111]    5.10237403    5.48005566   5.86167709   6.5804423  9059.4950\nexpRating[112]    1.95734608    2.32234835   2.70051477   3.4340308  5169.2599\nexpRating[113]    0.94285743    1.30582543   1.68780914   2.4505854  5170.2811\nexpRating[114]   -0.90853118   -0.56566155  -0.18496663   0.5671644  5365.2518\nexpRating[115]    3.46728169    3.85008028   4.21659714   4.9746323  5387.1732\nexpRating[116]    1.17284856    1.56241563   1.92890861   2.6490707  5278.9504\nexpRating[117]    3.35730292    3.73974868   4.11845342   4.8314672  4830.9419\nexpRating[118]    2.32361792    2.66647846   3.02323404   3.7356896  6200.4370\nexpRating[119]   -0.13756960    0.20773357   0.57613464   1.2885368  5798.2984\nexpRating[120]    3.05064469    3.40413905   3.77132958   4.4663142  5854.9459\nexpRating[121]    8.12704471    8.51399802   8.87609843   9.5983407  7692.0176\nexpRating[122]    5.67000788    6.05386120   6.43640524   7.1137666  7323.8265\nexpRating[123]    7.46840508    7.84945011   8.23152651   8.9250385  7434.4556\nexpRating[124]    7.07274335    7.44818170   7.79456564   8.5123477  7158.7740\nexpRating[125]    6.05392127    6.41941403   6.79557360   7.4758062  7501.5465\nexpRating[126]    8.24344174    8.60063782   8.97672290   9.6666555  7206.8953\nexpRating[127]    6.47026274    6.84632890   7.21682898   7.8826366  6684.4651\nexpRating[128]    8.63704430    9.00764585   9.36819083  10.0470875  6688.2062\nexpRating[129]    8.26931015    8.64806825   9.01406471   9.6619298  6668.5302\nexpRating[130]    5.61218301    6.00009101   6.38637664   7.0926440  8012.4173\nexpRating[131]    8.79148799    9.18075516   9.56974692  10.3111063  8099.3861\nexpRating[132]    7.40146206    7.79344233   8.17490504   8.9292674  7980.6150\nexpRating[133]    7.13769033    7.51781214   7.88257246   8.5677682 10495.4864\nexpRating[134]    4.82734732    5.20851070   5.59087088   6.2778296  9808.8973\nexpRating[135]    5.98553130    6.36127061   6.74468487   7.4419421 10003.8114\nexpRating[136]    1.66176143    2.04513654   2.44590530   3.1586819  7593.8802\nexpRating[137]    1.71327944    2.10520429   2.50066580   3.1886621  7985.0292\nexpRating[138]   -0.18625018    0.19936635   0.57196498   1.2865881  7792.4989\nexpRating[139]    9.17787094    9.53578962   9.85423413  10.3757402  4762.5054\nexpRating[140]    8.16259328    8.51925241   8.85487439   9.3612214  4493.4382\nexpRating[141]    9.31577806    9.68351089   9.99728354  10.4948717  5013.8459\nexpRating[142]    6.82588388    7.22048573   7.58444023   8.2773979  8456.3085\nexpRating[143]    9.00873303    9.40256213   9.77762121  10.4450488  8246.1064\nexpRating[144]    7.61427847    8.01763571   8.37817739   9.0826230  8592.1172\nexpRating[145]    4.11952867    4.49863717   4.85379333   5.5716320  6549.9405\nexpRating[146]   -0.03330410    0.34074598   0.69749642   1.4124917  6310.6116\nexpRating[147]    2.61736932    2.97899224   3.36076607   4.0435555  6553.3389\nexpRating[148]    4.57584013    4.94372421   5.33115705   6.0691874 10247.7363\nexpRating[149]    2.70187289    3.08442026   3.46477527   4.1889763 10539.4730\nexpRating[150]    6.75687607    7.13144449   7.50927246   8.2385759 11063.0654\nexpRating[151]    1.63380271    1.92070265   2.23860472   2.9375632  3097.8045\nexpRating[152]    2.05929632    2.34993492   2.67717713   3.3481743  3836.7093\nexpRating[153]    2.80355760    3.09040165   3.42077031   4.0833695  2637.6847\nexpRating[154]    0.55382579    0.91024027   1.27393035   2.0092911  7261.1127\nexpRating[155]    1.58751954    1.97472008   2.34738026   3.0596744  7245.5871\nexpRating[156]    3.73070400    4.08936674   4.46853139   5.2061532  6587.0677\nexpRating[157]    5.54077691    5.92318210   6.29774506   7.0161298  8775.5000\nexpRating[158]    3.66729603    4.06592890   4.45863822   5.1291518  8032.4512\nexpRating[159]    7.72277056    8.10655171   8.49317480   9.2054217  8966.2308\nexpRating[160]    3.59311114    3.96361983   4.32034130   5.0620429  7295.6669\nexpRating[161]    1.14575478    1.50768412   1.88025322   2.6016186  7431.6272\nexpRating[162]    2.14109270    2.51025234   2.88800012   3.6228395  7203.8758\nexpRating[163]    3.31659166    3.70972605   4.09569301   4.8464622  7385.5896\nexpRating[164]    2.30853556    2.68958310   3.07789915   3.8068243  7973.8041\nexpRating[165]    4.47849922    4.87348595   5.26955087   5.9968154  7301.4859\nexpRating[166]    3.47524463    3.86536332   4.24148274   4.9890527  6281.9362\nexpRating[167]   -0.67193495   -0.28772596   0.08589641   0.8359713  5441.5591\nexpRating[168]    2.34609641    2.72218294   3.08862944   3.8227671  5686.4502\nexpRating[169]    2.82254562    3.19550999   3.57484724   4.2931086  9538.1160\nexpRating[170]    1.95425907    2.34336827   2.71610414   3.4436138  9280.1759\nexpRating[171]    4.61703718    5.00231730   5.36670656   6.0592801  9619.7759\nexpRating[172]    6.74972405    7.12259392   7.47856068   8.1004500  5014.8004\nexpRating[173]    7.74076335    8.12655573   8.47826840   9.1110908  5090.5095\nexpRating[174]    8.54285651    8.91480679   9.28279877   9.8877477  5094.4286\nexpRating[175]    3.25032551    3.63048305   4.01851785   4.7768945  9425.1571\nexpRating[176]    1.32681839    1.72516228   2.09069320   2.9011363  8662.7925\nexpRating[177]    3.98970702    4.37360925   4.76079395   5.4971456  8879.5267\nexpRating[178]    6.11396626    6.48215004   6.84954369   7.5737749  5307.2869\nexpRating[179]    9.41657945    9.78151984  10.15724675  10.9126128  5287.6360\nexpRating[180]    9.27924238    9.66737535  10.03554468  10.8063499  5020.1040\nexpRating[181]    2.38944174    2.76238551   3.14117067   3.8693881  4113.3617\nexpRating[182]    1.43818528    1.80767201   2.17452615   2.9028855  4661.7021\nexpRating[183]    2.53267845    2.91487804   3.27497277   4.0031550  4537.2852\nexpRating[184]    9.13827502    9.49932435   9.82259740  10.3169749  5351.9365\nexpRating[185]    9.56759641    9.92906970  10.26604411  10.7770825  5567.8309\nexpRating[186]    9.27971966    9.62940830   9.96836908  10.4686735  5648.6564\nexpRating[187]   10.31742885   10.66258933  10.99515736  11.5393181  5048.3565\nexpRating[188]    8.02729351    8.36763970   8.69457640   9.2497845  5057.8429\nexpRating[189]   10.18872650   10.54880625  10.89067288  11.4528337  5079.3923\nexpRating[190]    2.03774578    2.39060699   2.75458511   3.4700868  5479.9810\nexpRating[191]    1.06553385    1.42710894   1.79755003   2.5491980  5940.2995\nexpRating[192]    3.19952220    3.55428833   3.91969763   4.6306388  5981.1857\nexpRating[193]    6.53369253    6.89964466   7.28287990   8.0412712  9413.9480\nexpRating[194]    6.96335327    7.33895538   7.72122121   8.4363984  9322.4592\nexpRating[195]    7.80369847    8.18689542   8.58002509   9.2899233  9552.4566\nexpRating[196]    3.44856727    3.82488199   4.21471988   4.9684961  7477.4711\nexpRating[197]    2.43683387    2.81478570   3.20081205   3.9379892  8084.1959\nexpRating[198]    0.58089179    0.95753536   1.33448129   2.0559031  6812.4036\nexpRating[199]    0.40487475    0.77576914   1.15787655   1.8873399  6258.2364\nexpRating[200]    3.70244015    4.07616399   4.45801199   5.2166137  5723.3019\nexpRating[201]   -0.45603392   -0.08650337   0.30648865   1.0061542  5940.8529\nexpRating[202]    3.35581945    3.73060301   4.11620877   4.8184363  5502.3611\nexpRating[203]    4.20050800    4.58397292   4.95803028   5.6998802  5841.1800\nexpRating[204]    0.03932642    0.42767027   0.80222651   1.5200147  5437.3213\nexpRating[205]    9.27858495    9.61328904   9.91794289  10.4164182  6102.0871\nexpRating[206]    9.71076199   10.04434804  10.35746646  10.8149916  6141.8259\nexpRating[207]    9.41096245    9.74950023  10.06410994  10.5333725  6231.3314\nexpRating[208]    6.68124421    7.08549651   7.48129651   8.2346480 10790.9438\nexpRating[209]    4.45833903    4.83505408   5.23924221   5.9773685  9104.3401\nexpRating[210]    5.19704861    5.57643720   5.96592129   6.7217030  9922.4154\nexpRating[211]    4.66586404    5.03823024   5.43811880   6.1687365  9568.7904\nexpRating[212]    5.08820514    5.48187546   5.87489947   6.5779002  8902.2946\nexpRating[213]    2.63744701    3.03086507   3.42344017   4.1469375  8279.0680\nexpRating[214]    4.57228728    4.94836713   5.33544194   6.0863280  9958.9124\nexpRating[215]    5.57895525    5.95072183   6.34989346   7.0781274  9746.6085\nexpRating[216]    6.38354990    6.75693129   7.13956381   7.8975092 10503.2485\nexpRating[217]    4.67092613    5.03997943   5.42387681   6.1238601  6908.4618\nexpRating[218]    2.36434245    2.74931124   3.13066183   3.8390527  7316.2437\nexpRating[219]    3.17226461    3.54656109   3.92744449   4.6483378  7517.4603\nexpRating[220]    7.98411239    8.36264168   8.74493417   9.5138884  5946.0076\nexpRating[221]    4.67004272    5.05485671   5.44083078   6.1904589  6738.1481\nexpRating[222]    7.32135316    7.70947237   8.09562456   8.8496028  6943.9014\nexpRating[223]    9.09350026    9.46464018   9.82596776  10.4770134  5298.8871\nexpRating[224]    6.64096824    7.01721324   7.37382462   8.0361240  5275.2968\nexpRating[225]    8.80676816    9.16034561   9.52896333  10.1906086  5084.8066\nexpRating[226]    4.95414443    5.32972479   5.72198384   6.4745686  9051.3633\nexpRating[227]    2.93666803    3.31946058   3.70178446   4.4560966  9178.0094\nexpRating[228]    3.93700954    4.31849568   4.70793973   5.4733602  9269.4503\nexpRating[229]    1.47881967    1.88129197   2.28998106   3.0590151  6240.3840\nexpRating[230]    2.47714641    2.88146275   3.29473001   4.0505456  5858.5406\nexpRating[231]    0.61080652    1.01349964   1.43542253   2.2020475  6325.3977\nexpRating[232]    4.26662319    4.67750666   5.07391315   5.8669497  8929.8745\nexpRating[233]    5.55543604    5.95197102   6.34829270   7.0891745  9362.8493\nexpRating[234]    1.39108774    1.80085138   2.19280139   2.9837225  9667.8971\nexpRating[235]    6.71467973    7.09322048   7.47272627   8.2152090  9494.9369\nexpRating[236]    4.69223233    5.07707605   5.46438743   6.1888473  9344.8312\nexpRating[237]    5.69545198    6.07306550   6.47613976   7.2090680  9957.3393\nexpRating[238]    2.26665335    2.64368287   3.04218209   3.8072254  8838.9537\nexpRating[239]    4.41305602    4.79835811   5.18294116   5.9413993  9531.7803\nexpRating[240]    4.06773512    4.43433292   4.83809720   5.5625316  9805.7841\nexpRating[241]    4.34954957    4.72461980   5.08480262   5.8346700  8974.6702\nexpRating[242]    3.49465762    3.86596904   4.23220775   4.9686471  8662.1913\nexpRating[243]    6.50929848    6.87964083   7.24233828   7.9494315  8545.7369\nexpRating[244]    2.99801588    3.37609391   3.76297989   4.4865179  7469.8581\nexpRating[245]    1.97255922    2.37399875   2.75612074   3.4725730  8491.5362\nexpRating[246]    2.77546893    3.15765011   3.54776430   4.2890868  8624.1364\nexpRating[247]    5.20864785    5.59980920   5.99148192   6.7183891  9341.4454\nexpRating[248]    4.25760061    4.63809680   5.01453465   5.7572759  9581.6716\nexpRating[249]    5.00566472    5.37684638   5.76709606   6.4766446  9831.8462\nexpRating[250]    1.63056351    1.90868091   2.23665826   2.8683936  4483.4116\nexpRating[251]    2.04620133    2.33796758   2.66222450   3.3004137  4820.1400\nexpRating[252]   -0.39740178   -0.10787026   0.21298035   0.8706734  3894.5478\nexpRating[253]    2.72120802    3.10239639   3.48441968   4.2129864 10099.2536\nexpRating[254]    5.73049659    6.11908225   6.50184742   7.2307262  9652.4719\nexpRating[255]    5.36533652    5.75956154   6.14869249   6.8722509  8872.0099\nexpRating[256]    6.12190083    6.49150279   6.86003598   7.5895811  7102.5766\nexpRating[257]    5.26896366    5.63203719   6.00792311   6.7028651  6850.0094\nexpRating[258]    7.91489020    8.28716403   8.66941914   9.3716244  6690.4530\nexpRating[259]    2.28184125    2.64697306   3.01937421   3.7579290  5075.3383\nexpRating[260]   -0.59905612   -0.21881213   0.13090467   0.8592920  4959.2165\nexpRating[261]    2.06217078    2.42625412   2.80809265   3.5267447  5201.1857\nexpRating[262]    7.44726389    7.82816595   8.20242586   8.8299658  6129.8765\nexpRating[263]    7.50067789    7.87075136   8.25608529   8.9016798  6279.0252\nexpRating[264]    5.58520127    5.95944856   6.34032720   6.9900551  6351.8939\nexpRating[265]    5.58065288    5.95057450   6.32065387   7.0836084 10254.0628\nexpRating[266]    3.66743370    4.04252849   4.41070042   5.1293247  9669.3854\nexpRating[267]    6.32324976    6.69152172   7.06176719   7.8111420 10309.3639\nexpRating[268]    7.37696036    7.73143526   8.11429872   8.8184003  7095.4249\nexpRating[269]    5.50529994    5.87918316   6.23881790   6.9306284  7093.4166\nexpRating[270]    8.52418923    8.89256610   9.25765213   9.9167145  7577.9086\nexpRating[271]    3.52307025    3.90178484   4.29475197   5.0011449  7643.8068\nexpRating[272]    2.07626793    2.46302026   2.84181279   3.5580035  7498.4563\nexpRating[273]    2.14121749    2.50794000   2.89153487   3.6255217  8045.6808\nexpRating[274]    3.28200018    3.67329652   4.09362632   4.8400128  8650.6965\nexpRating[275]    3.73750460    4.11350914   4.51091834   5.2527204  8683.9518\nexpRating[276]    3.43055340    3.82155313   4.22815242   4.9556693  8309.5507\nexpRating[277]    4.37969362    4.75716885   5.13760587   5.8667761 10949.7000\nexpRating[278]    5.22868618    5.60018906   5.99159222   6.7372435 11203.7772\nexpRating[279]    4.09153478    4.45970806   4.84001863   5.5599962 10440.1282\nexpRating[280]    2.77909489    3.15171363   3.54325091   4.2565591  5751.8382\nexpRating[281]    4.05433705    4.43383850   4.82391256   5.5258679  5757.6983\nexpRating[282]    1.83339950    2.18024938   2.56203031   3.2829406  5416.7179\nexpRating[283]    8.13324277    8.49581303   8.85160005   9.6317604  6809.1705\nexpRating[284]    8.97289853    9.34532177   9.71587351  10.4620295  6897.2872\nexpRating[285]    7.84043304    8.19412950   8.56800697   9.3301145  6902.7496\nexpRating[286]    4.70263490    5.08103502   5.44759682   6.1511804  9235.1816\nexpRating[287]    3.29835558    3.67981712   4.07486648   4.7718911  8619.2549\nexpRating[288]    4.40163044    4.78477440   5.15882943   5.8755658  8887.3512\nexpRating[289]    5.85873038    6.24701913   6.61076909   7.3311544  8190.7492\nexpRating[290]    4.45761320    4.84161396   5.22031406   5.9911429 10224.5258\nexpRating[291]    5.55667441    5.94529051   6.32437380   7.1020863  8308.9817\nexpRating[292]    6.86307271    7.24062190   7.63665459   8.3554327  8337.1679\nexpRating[293]    5.46656968    5.86380228   6.23261882   6.9748822  8821.9602\nexpRating[294]    6.57542024    6.95439326   7.33113797   8.0437300  9061.8998\nexpRating[295]    4.78524479    5.16766296   5.53952301   6.2915285  8636.9090\nexpRating[296]    3.39615556    3.77041812   4.14828325   4.8830685  8780.0545\nexpRating[297]    4.49632244    4.87345820   5.25145416   5.9635665  8064.5584\nexpRating[298]    3.84781402    4.24837323   4.64814681   5.3714785  9681.3324\nexpRating[299]    4.93172980    5.30682380   5.68017834   6.3966143  9742.0245\nexpRating[300]    7.05277182    7.44239067   7.82375962   8.5709495  9478.7629\nlp__           -108.53231382 -101.49651136 -94.92098435 -83.8524981  1155.2360\n                    Rhat\nability[1]     1.0025928\nability[2]     1.0029876\nability[3]     1.0034769\nability[4]     1.0048583\nability[5]     1.0037141\nability[6]     1.0023338\nability[7]     1.0018395\nability[8]     1.0044783\nability[9]     1.0037875\nability[10]    1.0002623\nability[11]    1.0044740\nability[12]    1.0031131\nability[13]    1.0026522\nability[14]    1.0013265\nability[15]    1.0028615\nability[16]    1.0017338\nability[17]    1.0035515\nability[18]    1.0020301\nability[19]    1.0046069\nability[20]    1.0051368\nability[21]    1.0028849\nability[22]    1.0026476\nability[23]    1.0051480\nability[24]    1.0041199\nability[25]    1.0039160\nability[26]    1.0041981\nability[27]    1.0040712\nability[28]    1.0006033\nability[29]    1.0023521\nability[30]    1.0043518\nability[31]    1.0042633\nability[32]    1.0026653\nability[33]    1.0035749\nability[34]    1.0030793\nability[35]    1.0047631\nability[36]    1.0034229\nability[37]    1.0033582\nability[38]    1.0025352\nability[39]    1.0028383\nability[40]    1.0041264\nability[41]    1.0026454\nability[42]    1.0045185\nability[43]    1.0026131\nability[44]    1.0046712\nability[45]    1.0035924\nability[46]    1.0037439\nability[47]    1.0026786\nability[48]    1.0041770\nability[49]    1.0038058\nability[50]    1.0022812\nability[51]    1.0011463\nability[52]    1.0044899\nability[53]    1.0054500\nability[54]    1.0049177\nability[55]    1.0023946\nability[56]    1.0031859\nability[57]    1.0047187\nability[58]    1.0037911\nability[59]    1.0048616\nability[60]    1.0043129\nability[61]    1.0018596\nability[62]    1.0013388\nability[63]    1.0044705\nability[64]    1.0038096\nability[65]    1.0030006\nability[66]    1.0025448\nability[67]    1.0039406\nability[68]    1.0036546\nability[69]    1.0017728\nability[70]    1.0026231\nability[71]    1.0034515\nability[72]    1.0029341\nability[73]    1.0031813\nability[74]    1.0038266\nability[75]    1.0021676\nability[76]    1.0043429\nability[77]    1.0042633\nability[78]    1.0023986\nability[79]    1.0055740\nability[80]    1.0037172\nability[81]    1.0041157\nability[82]    1.0026771\nability[83]    1.0045367\nability[84]    1.0018863\nability[85]    1.0037392\nability[86]    1.0040306\nability[87]    1.0060756\nability[88]    1.0047703\nability[89]    1.0041187\nability[90]    1.0040982\nability[91]    1.0049375\nability[92]    1.0026843\nability[93]    1.0027231\nability[94]    1.0013003\nability[95]    1.0039142\nability[96]    1.0047510\nability[97]    1.0042202\nability[98]    1.0026495\nability[99]    1.0034026\nability[100]   1.0023889\nseverity[1]    1.0173727\nseverity[2]    1.0136610\nseverity[3]    1.0168219\nseverity[4]    1.0149423\nseverity[5]    1.0180916\nseverity[6]    1.0186598\nseverity[7]    1.0152482\nseverity[8]    1.0130704\nseverity[9]    1.0159302\nseverity[10]   1.0178880\nsigma          1.0008827\ntau            1.0001095\nexpRating[1]   1.0000292\nexpRating[2]   1.0000330\nexpRating[3]   0.9997150\nexpRating[4]   0.9999246\nexpRating[5]   0.9995042\nexpRating[6]   1.0004383\nexpRating[7]   0.9995521\nexpRating[8]   0.9993279\nexpRating[9]   0.9995292\nexpRating[10]  1.0000583\nexpRating[11]  1.0001609\nexpRating[12]  0.9997630\nexpRating[13]  0.9995805\nexpRating[14]  0.9997564\nexpRating[15]  0.9996540\nexpRating[16]  0.9993283\nexpRating[17]  0.9994185\nexpRating[18]  0.9994160\nexpRating[19]  1.0004817\nexpRating[20]  1.0001440\nexpRating[21]  0.9996234\nexpRating[22]  1.0000002\nexpRating[23]  0.9994227\nexpRating[24]  0.9995034\nexpRating[25]  0.9991680\nexpRating[26]  0.9994783\nexpRating[27]  0.9994353\nexpRating[28]  1.0011285\nexpRating[29]  1.0007135\nexpRating[30]  1.0012050\nexpRating[31]  0.9996327\nexpRating[32]  1.0000888\nexpRating[33]  0.9999205\nexpRating[34]  0.9993257\nexpRating[35]  0.9992515\nexpRating[36]  0.9991874\nexpRating[37]  1.0001005\nexpRating[38]  1.0004343\nexpRating[39]  0.9998109\nexpRating[40]  1.0009008\nexpRating[41]  1.0013196\nexpRating[42]  1.0002566\nexpRating[43]  0.9993747\nexpRating[44]  0.9994176\nexpRating[45]  0.9995430\nexpRating[46]  0.9992724\nexpRating[47]  0.9998873\nexpRating[48]  0.9993114\nexpRating[49]  0.9994158\nexpRating[50]  0.9993510\nexpRating[51]  0.9995510\nexpRating[52]  1.0006990\nexpRating[53]  1.0002378\nexpRating[54]  1.0005811\nexpRating[55]  0.9995750\nexpRating[56]  0.9992990\nexpRating[57]  0.9993883\nexpRating[58]  1.0000160\nexpRating[59]  0.9996186\nexpRating[60]  0.9999277\nexpRating[61]  0.9996104\nexpRating[62]  0.9997896\nexpRating[63]  1.0000087\nexpRating[64]  0.9996064\nexpRating[65]  0.9995083\nexpRating[66]  0.9993781\nexpRating[67]  0.9995985\nexpRating[68]  0.9998170\nexpRating[69]  1.0001964\nexpRating[70]  1.0001354\nexpRating[71]  1.0001211\nexpRating[72]  0.9996878\nexpRating[73]  1.0007786\nexpRating[74]  1.0001521\nexpRating[75]  0.9999949\nexpRating[76]  0.9993966\nexpRating[77]  0.9991771\nexpRating[78]  0.9995561\nexpRating[79]  0.9992877\nexpRating[80]  0.9994447\nexpRating[81]  0.9993996\nexpRating[82]  1.0001241\nexpRating[83]  1.0001360\nexpRating[84]  1.0010034\nexpRating[85]  0.9994613\nexpRating[86]  0.9993907\nexpRating[87]  0.9993652\nexpRating[88]  0.9997969\nexpRating[89]  0.9993614\nexpRating[90]  0.9996386\nexpRating[91]  0.9997224\nexpRating[92]  0.9998052\nexpRating[93]  0.9997330\nexpRating[94]  0.9992910\nexpRating[95]  0.9994586\nexpRating[96]  0.9993845\nexpRating[97]  0.9994357\nexpRating[98]  0.9994820\nexpRating[99]  0.9996762\nexpRating[100] 0.9995440\nexpRating[101] 0.9995048\nexpRating[102] 0.9993442\nexpRating[103] 0.9994672\nexpRating[104] 0.9994473\nexpRating[105] 0.9993984\nexpRating[106] 0.9995924\nexpRating[107] 0.9995330\nexpRating[108] 0.9993649\nexpRating[109] 0.9993004\nexpRating[110] 0.9991286\nexpRating[111] 0.9991761\nexpRating[112] 1.0000151\nexpRating[113] 1.0004357\nexpRating[114] 1.0000797\nexpRating[115] 1.0002063\nexpRating[116] 1.0005967\nexpRating[117] 1.0005279\nexpRating[118] 0.9997198\nexpRating[119] 0.9997174\nexpRating[120] 0.9994015\nexpRating[121] 0.9995055\nexpRating[122] 0.9996092\nexpRating[123] 0.9995887\nexpRating[124] 0.9996239\nexpRating[125] 0.9996866\nexpRating[126] 0.9995564\nexpRating[127] 1.0006612\nexpRating[128] 1.0003640\nexpRating[129] 1.0009779\nexpRating[130] 0.9997611\nexpRating[131] 1.0000765\nexpRating[132] 0.9996528\nexpRating[133] 0.9994055\nexpRating[134] 0.9994137\nexpRating[135] 0.9996536\nexpRating[136] 0.9993657\nexpRating[137] 0.9993889\nexpRating[138] 0.9995388\nexpRating[139] 0.9998556\nexpRating[140] 1.0001333\nexpRating[141] 0.9994818\nexpRating[142] 0.9994963\nexpRating[143] 0.9992961\nexpRating[144] 0.9993844\nexpRating[145] 0.9997023\nexpRating[146] 0.9998947\nexpRating[147] 0.9996292\nexpRating[148] 0.9993064\nexpRating[149] 0.9994764\nexpRating[150] 0.9993565\nexpRating[151] 1.0012623\nexpRating[152] 1.0005566\nexpRating[153] 1.0010351\nexpRating[154] 0.9996707\nexpRating[155] 0.9993206\nexpRating[156] 0.9997413\nexpRating[157] 1.0003106\nexpRating[158] 1.0003396\nexpRating[159] 1.0000280\nexpRating[160] 0.9997120\nexpRating[161] 0.9994673\nexpRating[162] 0.9994613\nexpRating[163] 0.9992532\nexpRating[164] 0.9993212\nexpRating[165] 0.9993675\nexpRating[166] 0.9997380\nexpRating[167] 0.9998821\nexpRating[168] 0.9997133\nexpRating[169] 0.9994435\nexpRating[170] 0.9994114\nexpRating[171] 0.9993018\nexpRating[172] 0.9994528\nexpRating[173] 0.9994025\nexpRating[174] 0.9995712\nexpRating[175] 0.9996901\nexpRating[176] 0.9998281\nexpRating[177] 0.9997181\nexpRating[178] 0.9996113\nexpRating[179] 0.9997325\nexpRating[180] 0.9996358\nexpRating[181] 1.0003638\nexpRating[182] 1.0000755\nexpRating[183] 1.0000974\nexpRating[184] 1.0005334\nexpRating[185] 0.9999546\nexpRating[186] 1.0003313\nexpRating[187] 0.9998994\nexpRating[188] 0.9999876\nexpRating[189] 0.9998848\nexpRating[190] 0.9994061\nexpRating[191] 0.9998494\nexpRating[192] 0.9995884\nexpRating[193] 0.9994457\nexpRating[194] 0.9996827\nexpRating[195] 0.9993336\nexpRating[196] 0.9994280\nexpRating[197] 0.9993268\nexpRating[198] 0.9993396\nexpRating[199] 0.9999550\nexpRating[200] 0.9997984\nexpRating[201] 1.0002998\nexpRating[202] 1.0008774\nexpRating[203] 1.0009474\nexpRating[204] 1.0008771\nexpRating[205] 1.0006411\nexpRating[206] 0.9997195\nexpRating[207] 0.9997704\nexpRating[208] 0.9990854\nexpRating[209] 0.9994392\nexpRating[210] 0.9992541\nexpRating[211] 0.9994834\nexpRating[212] 0.9995506\nexpRating[213] 0.9992404\nexpRating[214] 0.9992937\nexpRating[215] 0.9993525\nexpRating[216] 0.9995807\nexpRating[217] 0.9994425\nexpRating[218] 0.9993149\nexpRating[219] 0.9992882\nexpRating[220] 0.9997439\nexpRating[221] 0.9994765\nexpRating[222] 0.9992240\nexpRating[223] 0.9996625\nexpRating[224] 0.9998477\nexpRating[225] 0.9994082\nexpRating[226] 0.9998228\nexpRating[227] 0.9996004\nexpRating[228] 0.9997724\nexpRating[229] 0.9996572\nexpRating[230] 0.9994522\nexpRating[231] 0.9997719\nexpRating[232] 0.9992812\nexpRating[233] 0.9994134\nexpRating[234] 0.9991469\nexpRating[235] 0.9996622\nexpRating[236] 0.9997726\nexpRating[237] 0.9996939\nexpRating[238] 0.9994088\nexpRating[239] 0.9995604\nexpRating[240] 0.9992882\nexpRating[241] 0.9993693\nexpRating[242] 0.9996115\nexpRating[243] 0.9996237\nexpRating[244] 0.9998394\nexpRating[245] 0.9996482\nexpRating[246] 0.9998618\nexpRating[247] 0.9996236\nexpRating[248] 0.9997748\nexpRating[249] 0.9995855\nexpRating[250] 1.0016110\nexpRating[251] 1.0008879\nexpRating[252] 1.0020668\nexpRating[253] 0.9995830\nexpRating[254] 0.9996750\nexpRating[255] 0.9995718\nexpRating[256] 0.9995281\nexpRating[257] 0.9995599\nexpRating[258] 0.9997821\nexpRating[259] 0.9995609\nexpRating[260] 0.9996899\nexpRating[261] 0.9993406\nexpRating[262] 0.9995727\nexpRating[263] 0.9998837\nexpRating[264] 0.9999451\nexpRating[265] 0.9993290\nexpRating[266] 0.9993702\nexpRating[267] 0.9992190\nexpRating[268] 0.9995231\nexpRating[269] 0.9994429\nexpRating[270] 0.9993342\nexpRating[271] 0.9998475\nexpRating[272] 0.9993938\nexpRating[273] 0.9995169\nexpRating[274] 0.9992935\nexpRating[275] 0.9994690\nexpRating[276] 0.9994196\nexpRating[277] 0.9995202\nexpRating[278] 0.9995400\nexpRating[279] 0.9993178\nexpRating[280] 0.9999483\nexpRating[281] 0.9998647\nexpRating[282] 1.0002680\nexpRating[283] 1.0000930\nexpRating[284] 0.9999654\nexpRating[285] 1.0001183\nexpRating[286] 0.9996038\nexpRating[287] 0.9992160\nexpRating[288] 0.9996498\nexpRating[289] 0.9999038\nexpRating[290] 0.9997344\nexpRating[291] 0.9997139\nexpRating[292] 0.9992989\nexpRating[293] 0.9995723\nexpRating[294] 0.9992855\nexpRating[295] 0.9994100\nexpRating[296] 0.9993174\nexpRating[297] 0.9992682\nexpRating[298] 0.9992451\nexpRating[299] 0.9992847\nexpRating[300] 0.9992645\nlp__           1.0042111\n\n$c_summary\n, , chains = chain:1\n\n                stats\nparameter                 mean          sd          2.5%           25%\n  ability[1]        5.61974343  0.60565540    4.42509617  5.218026e+00\n  ability[2]        7.97012927  0.56884032    6.87910239  7.561659e+00\n  ability[3]        6.78831714  0.55631221    5.69810411  6.430203e+00\n  ability[4]        7.07142570  0.58935720    5.96201939  6.668596e+00\n  ability[5]        3.08830706  0.58830527    1.89952556  2.696436e+00\n  ability[6]        4.53008962  0.56972526    3.42362068  4.147137e+00\n  ability[7]        8.90809460  0.51775520    7.83744605  8.560081e+00\n  ability[8]        4.21144428  0.57055509    3.12650737  3.831785e+00\n  ability[9]        2.84067428  0.60558841    1.63005654  2.447990e+00\n  ability[10]       8.79926517  0.55791521    7.65318082  8.418938e+00\n  ability[11]       5.66253626  0.60863726    4.48750557  5.271948e+00\n  ability[12]       6.64757231  0.61643468    5.43619000  6.223117e+00\n  ability[13]       1.64620315  0.44587429    1.02413526  1.290918e+00\n  ability[14]       9.31400887  0.42940884    8.33440222  9.051749e+00\n  ability[15]       5.81553284  0.58573748    4.65110281  5.453795e+00\n  ability[16]       6.64498638  0.61287878    5.46887396  6.228908e+00\n  ability[17]       5.19425219  0.58262196    4.02934482  4.809353e+00\n  ability[18]       1.96455920  0.49057152    1.12419474  1.588170e+00\n  ability[19]       2.62455734  0.56750918    1.52888118  2.211434e+00\n  ability[20]       4.85195967  0.57195240    3.76974499  4.464434e+00\n  ability[21]       6.84765690  0.62406079    5.63203954  6.453894e+00\n  ability[22]       7.61018336  0.59650527    6.41016397  7.204785e+00\n  ability[23]       3.77682188  0.58228783    2.62756297  3.406467e+00\n  ability[24]       8.68216504  0.52550108    7.60131961  8.322869e+00\n  ability[25]       2.14430774  0.51226885    1.24224316  1.771816e+00\n  ability[26]       5.93023533  0.59058251    4.85735751  5.489407e+00\n  ability[27]       2.61479895  0.53837826    1.59137398  2.243368e+00\n  ability[28]       2.13096508  0.53030101    1.18004370  1.733438e+00\n  ability[29]       5.07638879  0.59588109    3.99194577  4.634729e+00\n  ability[30]       8.37637514  0.58669834    7.27317839  7.958908e+00\n  ability[31]       8.78977294  0.55542567    7.66159242  8.389166e+00\n  ability[32]       2.36197908  0.55294921    1.29265394  1.990329e+00\n  ability[33]       9.02939273  0.55310480    7.83309045  8.636876e+00\n  ability[34]       7.28886301  0.56170387    6.20405501  6.887633e+00\n  ability[35]       5.49496765  0.61244005    4.34465362  5.098331e+00\n  ability[36]       5.88209757  0.61174457    4.71087953  5.477009e+00\n  ability[37]       3.98587369  0.59666781    2.82143835  3.569800e+00\n  ability[38]       2.01273374  0.53568831    1.10782493  1.602594e+00\n  ability[39]       2.25793062  0.55999391    1.23123276  1.874899e+00\n  ability[40]       1.92641135  0.50476894    1.08585087  1.556363e+00\n  ability[41]       7.75431414  0.55621374    6.70309263  7.373652e+00\n  ability[42]       7.10044979  0.57652564    6.02916271  6.704099e+00\n  ability[43]       8.56388789  0.57905301    7.44749432  8.175548e+00\n  ability[44]       7.67756753  0.64007291    6.50730393  7.245470e+00\n  ability[45]       5.90015494  0.60174164    4.72591397  5.490137e+00\n  ability[46]       2.75227729  0.57222823    1.68495671  2.357439e+00\n  ability[47]       9.16905032  0.49569031    8.14014055  8.830634e+00\n  ability[48]       7.90205175  0.58932278    6.73932458  7.513463e+00\n  ability[49]       2.88253332  0.56335917    1.79093692  2.512564e+00\n  ability[50]       5.63039453  0.57951264    4.43473990  5.237386e+00\n  ability[51]       1.62275024  0.42234452    1.03117488  1.304619e+00\n  ability[52]       2.60494027  0.58057322    1.48216051  2.209629e+00\n  ability[53]       6.61870413  0.60121749    5.45022686  6.200233e+00\n  ability[54]       3.21732231  0.58701212    2.12878257  2.801891e+00\n  ability[55]       3.39149866  0.58966692    2.29607398  2.982187e+00\n  ability[56]       2.28641501  0.57308368    1.25201139  1.894717e+00\n  ability[57]       4.88692200  0.55114583    3.81764503  4.501764e+00\n  ability[58]       8.79881266  0.55860556    7.68149107  8.419337e+00\n  ability[59]       4.27407080  0.57571374    3.14529672  3.880383e+00\n  ability[60]       8.19748119  0.57391693    7.08323253  7.801629e+00\n  ability[61]       2.46025869  0.57133912    1.34569173  2.073561e+00\n  ability[62]       9.16308148  0.48991359    8.14599259  8.813668e+00\n  ability[63]       9.04770952  0.49539534    7.99929268  8.712661e+00\n  ability[64]       2.06857141  0.54889896    1.15116092  1.664111e+00\n  ability[65]       6.57589711  0.57385070    5.50150071  6.203807e+00\n  ability[66]       3.51260297  0.59664794    2.38646138  3.125417e+00\n  ability[67]       2.47564936  0.60664062    1.36937427  2.034511e+00\n  ability[68]       2.94415938  0.63160484    1.63102563  2.516550e+00\n  ability[69]       9.25665502  0.46811742    8.24569816  8.935697e+00\n  ability[70]       5.46565826  0.59022648    4.36316763  5.049507e+00\n  ability[71]       4.72396905  0.57254481    3.55263102  4.351612e+00\n  ability[72]       6.64328141  0.58893391    5.49059798  6.239152e+00\n  ability[73]       3.45655322  0.56115424    2.39141556  3.076018e+00\n  ability[74]       7.59681982  0.59734360    6.48538629  7.160678e+00\n  ability[75]       8.70142381  0.53735758    7.60029819  8.344630e+00\n  ability[76]       5.02799838  0.57201210    3.88373928  4.638782e+00\n  ability[77]       3.57428253  0.63991089    2.30800798  3.143959e+00\n  ability[78]       4.34876648  0.62914471    3.10359499  3.914765e+00\n  ability[79]       6.78290164  0.58852330    5.62962744  6.387485e+00\n  ability[80]       4.34646846  0.58446221    3.13022006  3.967202e+00\n  ability[81]       6.42570231  0.57904591    5.23908936  6.042756e+00\n  ability[82]       3.04956827  0.57690796    1.89865827  2.656085e+00\n  ability[83]       5.24843221  0.61390183    4.01215843  4.837335e+00\n  ability[84]       1.62128523  0.40590132    1.03131505  1.296863e+00\n  ability[85]       5.65298369  0.59832723    4.50127316  5.246338e+00\n  ability[86]       8.15644820  0.57407911    6.96245711  7.781411e+00\n  ability[87]       2.32061242  0.57763668    1.30163727  1.902561e+00\n  ability[88]       8.50338128  0.57677465    7.34521424  8.143495e+00\n  ability[89]       6.58710132  0.58150677    5.47911768  6.220037e+00\n  ability[90]       8.42151623  0.57995006    7.30557835  8.048424e+00\n  ability[91]       3.15453444  0.55965747    2.07814447  2.780525e+00\n  ability[92]       3.34384577  0.58649688    2.25193870  2.952524e+00\n  ability[93]       4.00265345  0.55794855    2.93114157  3.618570e+00\n  ability[94]       2.83161276  0.58004640    1.68360151  2.461631e+00\n  ability[95]       7.76572515  0.61047074    6.64248137  7.331456e+00\n  ability[96]       4.31498239  0.57681324    3.14418230  3.913106e+00\n  ability[97]       5.49411649  0.57341980    4.40942743  5.104920e+00\n  ability[98]       6.50624175  0.58194672    5.34092504  6.107077e+00\n  ability[99]       4.40982716  0.56892228    3.38956793  4.014971e+00\n  ability[100]      5.94743149  0.55001806    4.84510186  5.575506e+00\n  severity[1]       0.32842969  0.26920248   -0.19898414  1.428158e-01\n  severity[2]       0.75417205  0.25563637    0.24281885  5.898375e-01\n  severity[3]      -1.69449026  0.26910249   -2.22303264 -1.875233e+00\n  severity[4]       1.61242734  0.27989957    1.07904320  1.417584e+00\n  severity[5]      -0.68441254  0.27419175   -1.23609185 -8.478634e-01\n  severity[6]      -0.64323662  0.27581488   -1.17740446 -8.350531e-01\n  severity[7]      -2.55002214  0.27605515   -3.07754479 -2.743540e+00\n  severity[8]       0.46062292  0.26282121   -0.08335124  2.960190e-01\n  severity[9]       1.48943595  0.27189660    0.94759540  1.308887e+00\n  severity[10]      0.10527195  0.28522534   -0.47626992 -9.005421e-02\n  sigma             0.93498015  0.04910823    0.84665519  9.013542e-01\n  tau               1.49273607  0.40907936    0.93798740  1.222995e+00\n  expRating[1]      3.92525317  0.58543303    2.82754997  3.517461e+00\n  expRating[2]      7.23217077  0.57374258    6.09281625  6.848033e+00\n  expRating[3]      4.97650682  0.56735596    3.89573840  4.620382e+00\n  expRating[4]      8.29855896  0.54872182    7.23625073  7.914277e+00\n  expRating[5]      9.58255661  0.53809971    8.54153856  9.217667e+00\n  expRating[6]      7.32689266  0.55049848    6.25481731  6.936837e+00\n  expRating[7]      6.14508052  0.53693076    5.03931729  5.802116e+00\n  expRating[8]      7.24894006  0.52945794    6.20114358  6.905134e+00\n  expRating[9]      6.89358909  0.53043694    5.84760510  6.538329e+00\n  expRating[10]     7.82559775  0.57364219    6.76311726  7.455243e+00\n  expRating[11]     6.42818908  0.57582160    5.28794148  6.051806e+00\n  expRating[12]     4.52140356  0.56726215    3.45638974  4.135564e+00\n  expRating[13]     3.84247911  0.56545555    2.73529486  3.458004e+00\n  expRating[14]     4.70073440  0.57058196    3.55998919  4.335408e+00\n  expRating[15]     2.40389452  0.56390515    1.29987672  2.034549e+00\n  expRating[16]     6.14251696  0.55330811    5.03460169  5.771655e+00\n  expRating[17]     3.84567708  0.54798594    2.78038036  3.488015e+00\n  expRating[18]     4.63536158  0.53808419    3.61668974  4.255927e+00\n  expRating[19]     9.23652429  0.50886144    8.17658262  8.907329e+00\n  expRating[20]     7.21360434  0.50952943    6.18005227  6.873845e+00\n  expRating[21]     9.36871753  0.49747789    8.30752337  9.042269e+00\n  expRating[22]     2.51695401  0.54209889    1.43771080  2.166519e+00\n  expRating[23]     3.56820766  0.55539856    2.46052440  3.193697e+00\n  expRating[24]     5.70088023  0.55839538    4.60573261  5.319732e+00\n  expRating[25]     3.16910398  0.56895058    2.04694242  2.784869e+00\n  expRating[26]     1.14618402  0.57217860    0.01473610  7.601273e-01\n  expRating[27]     0.29065215  0.56358360   -0.78364537 -1.119040e-01\n  expRating[28]    10.41169251  0.55963646    9.32114448  1.005189e+01\n  expRating[29]     9.25988810  0.55380088    8.15744493  8.884741e+00\n  expRating[30]    10.28870112  0.54970028    9.17646492  9.925982e+00\n  expRating[31]     7.27496359  0.59008711    6.10145070  6.888332e+00\n  expRating[32]     7.15197220  0.59170339    6.02161008  6.783292e+00\n  expRating[33]     5.76780821  0.58312691    4.66749060  5.386409e+00\n  expRating[34]     7.40174436  0.60726130    6.16380087  7.018044e+00\n  expRating[35]     8.25999965  0.59508592    7.06251678  7.887785e+00\n  expRating[36]     5.96315977  0.60283161    4.73044103  5.561672e+00\n  expRating[37]     2.40037520  0.45278845    1.61614070  2.068526e+00\n  expRating[38]    -0.04828711  0.45930256   -0.79817228 -3.864631e-01\n  expRating[39]     2.10682607  0.45700934    1.29912759  1.795170e+00\n  expRating[40]     9.64243856  0.45420759    8.63486092  9.359818e+00\n  expRating[41]     8.67077226  0.44769827    7.71793680  8.391490e+00\n  expRating[42]     9.77463180  0.45034355    8.72219261  9.492219e+00\n  expRating[43]     6.14396253  0.56251697    4.99400124  5.785766e+00\n  expRating[44]     3.26551070  0.57114268    2.14113081  2.903472e+00\n  expRating[45]     7.30496879  0.56436598    6.15315060  6.963425e+00\n  expRating[46]     8.25741371  0.59458680    7.14483721  7.882817e+00\n  expRating[47]     6.00174976  0.58140997    4.91966798  5.606086e+00\n  expRating[48]     4.09496424  0.58724788    2.97250626  3.716237e+00\n  expRating[49]     5.94842424  0.55778765    4.82414535  5.591718e+00\n  expRating[50]     2.64423005  0.56615292    1.48157686  2.267475e+00\n  expRating[51]     5.29952415  0.55918899    4.21086139  4.912490e+00\n  expRating[52]     1.28014666  0.48732961    0.41995754  9.347315e-01\n  expRating[53]     3.45399514  0.49659075    2.56133093  3.101590e+00\n  expRating[54]     2.06983115  0.48753013    1.14344113  1.739610e+00\n  expRating[55]     4.23698467  0.55661499    3.12701278  3.848146e+00\n  expRating[56]     1.98132072  0.53364080    0.94941868  1.590741e+00\n  expRating[57]     0.07453520  0.54301192   -0.95953915 -2.937834e-01\n  expRating[58]     4.16754713  0.54497246    3.09705542  3.793974e+00\n  expRating[59]     4.20872306  0.55003657    3.13928542  3.833075e+00\n  expRating[60]     6.34139562  0.54948444    5.33604713  5.942072e+00\n  expRating[61]     7.60182895  0.58588760    6.46427820  7.205045e+00\n  expRating[62]     6.20442028  0.58346784    5.09643496  5.810941e+00\n  expRating[63]     7.30827982  0.59663344    6.12737435  6.928178e+00\n  expRating[64]     7.93861305  0.57209828    6.83531588  7.563546e+00\n  expRating[65]     9.22261070  0.57609553    8.11570075  8.808970e+00\n  expRating[66]     5.06016122  0.57808158    3.87953889  4.668960e+00\n  expRating[67]     2.08233161  0.56477244    0.97402750  1.714128e+00\n  expRating[68]     5.38924921  0.57493778    4.21935892  5.048501e+00\n  expRating[69]     4.23744480  0.54638825    3.16833082  3.891502e+00\n  expRating[70]     9.01059473  0.52662537    7.97845995  8.649111e+00\n  expRating[71]     6.98767478  0.51313769    5.98920310  6.640942e+00\n  expRating[72]    10.17160099  0.51700180    9.15302252  9.819068e+00\n  expRating[73]     2.89847979  0.49013863    1.99174147  2.554069e+00\n  expRating[74]     2.60493066  0.49134178    1.69131308  2.264178e+00\n  expRating[75]     3.63374368  0.50704233    2.65926495  3.283876e+00\n  expRating[76]     7.54266267  0.56443864    6.47734922  7.150323e+00\n  expRating[77]     5.28699872  0.55000175    4.25603035  4.894465e+00\n  expRating[78]     6.39085826  0.55790048    5.36118350  5.976838e+00\n  expRating[79]     0.92030868  0.52673681   -0.06346025  5.609600e-01\n  expRating[80]     4.22722628  0.52151347    3.20158031  3.880357e+00\n  expRating[81]     4.10423490  0.52006072    3.12348974  3.758018e+00\n  expRating[82]     2.88513714  0.51496936    1.94350891  2.530486e+00\n  expRating[83]     2.59158801  0.51573650    1.65896668  2.226588e+00\n  expRating[84]     2.23623704  0.52030708    1.23458839  1.871984e+00\n  expRating[85]     4.39197625  0.58939381    3.23219390  3.983958e+00\n  expRating[86]     6.56582474  0.58037711    5.46546451  6.156941e+00\n  expRating[87]     5.18166075  0.57424158    4.12785861  4.752936e+00\n  expRating[88]     8.70480483  0.56590573    7.63408166  8.310968e+00\n  expRating[89]     7.73313852  0.56508640    6.60803934  7.333486e+00\n  expRating[90]     9.86581109  0.57423833    8.74458691  9.469121e+00\n  expRating[91]    10.40220027  0.54834815    9.29571765  1.002038e+01\n  expRating[92]    10.27920888  0.52460410    9.18490901  9.938671e+00\n  expRating[93]     8.89504489  0.54017809    7.82777670  8.511834e+00\n  expRating[94]     0.66748882  0.53714979   -0.32849656  2.983962e-01\n  expRating[95]     1.71874247  0.53417367    0.62628156  1.337914e+00\n  expRating[96]     2.46725104  0.53904294    1.43630836  2.101868e+00\n  expRating[97]    10.64182007  0.54166699    9.49312222  1.028688e+01\n  expRating[98]     6.47937059  0.54230622    5.30868359  6.106618e+00\n  expRating[99]    10.51882868  0.54184024    9.35681969  1.015758e+01\n  expRating[100]    8.04303507  0.53907607    7.00686925  7.669342e+00\n  expRating[101]    7.74948594  0.54319661    6.70235792  7.365525e+00\n  expRating[102]    7.39413497  0.54544508    6.38804542  7.047772e+00\n  expRating[103]    4.81055511  0.57533673    3.77026482  4.444813e+00\n  expRating[104]    6.98440359  0.59952680    5.80582628  6.588341e+00\n  expRating[105]    5.60023960  0.58775703    4.46666424  5.216576e+00\n  expRating[106]    6.63626963  0.59797553    5.52702573  6.237414e+00\n  expRating[107]    5.23886096  0.58621959    4.10134611  4.839788e+00\n  expRating[108]    7.37153352  0.58786542    6.23339278  6.953689e+00\n  expRating[109]    4.31430338  0.57404935    3.22599235  3.927822e+00\n  expRating[110]    1.43585155  0.58657231    0.34741914  1.029755e+00\n  expRating[111]    5.47530964  0.57354038    4.41520672  5.078324e+00\n  expRating[112]    2.34116343  0.52658182    1.40388296  1.951654e+00\n  expRating[113]    1.32832120  0.52756837    0.38608406  9.475278e-01\n  expRating[114]   -0.53728840  0.52457019   -1.44962372 -9.185876e-01\n  expRating[115]    3.87035795  0.55373298    2.81432255  3.467080e+00\n  expRating[116]    1.57351808  0.54458410    0.51587498  1.200468e+00\n  expRating[117]    3.74736656  0.55625874    2.69085215  3.361629e+00\n  expRating[118]    2.68058340  0.50873389    1.79517652  2.298902e+00\n  expRating[119]    0.23192108  0.49759008   -0.62596216 -1.322052e-01\n  expRating[120]    3.41584729  0.51019468    2.50764860  3.043636e+00\n  expRating[121]    8.50848619  0.54101558    7.48199037  8.129508e+00\n  expRating[122]    6.05982387  0.53477398    5.01527588  5.681717e+00\n  expRating[123]    7.85958609  0.54640604    6.82914562  7.477601e+00\n  expRating[124]    7.42887948  0.56487511    6.29863997  7.042818e+00\n  expRating[125]    6.41603725  0.55629625    5.26788373  6.021456e+00\n  expRating[126]    8.58988574  0.56561182    7.51228475  8.225171e+00\n  expRating[127]    6.86939763  0.56665498    5.75936025  6.483815e+00\n  expRating[128]    9.02451082  0.57361637    7.82608489  8.649700e+00\n  expRating[129]    8.66915985  0.55612549    7.53926873  8.300595e+00\n  expRating[130]    5.98307726  0.61263344    4.83498886  5.558111e+00\n  expRating[131]    9.16700347  0.61177060    7.98307299  8.741183e+00\n  expRating[132]    7.78283948  0.62050635    6.60170517  7.349487e+00\n  expRating[133]    7.51258228  0.58006491    6.45471682  7.120225e+00\n  expRating[134]    5.21574241  0.57844316    4.12899235  4.831314e+00\n  expRating[135]    6.36077787  0.57391750    5.27659557  5.975816e+00\n  expRating[136]    2.06786475  0.54833243    1.05744510  1.675444e+00\n  expRating[137]    2.10904067  0.54008313    1.14470279  1.722444e+00\n  expRating[138]    0.20225515  0.55598764   -0.84976229 -1.666167e-01\n  expRating[139]    9.49748001  0.49817428    8.48123912  9.159750e+00\n  expRating[140]    8.48463778  0.50862246    7.47186974  8.122002e+00\n  expRating[141]    9.62967324  0.50197397    8.63822211  9.288567e+00\n  expRating[142]    7.21763921  0.56600760    6.05897492  6.843697e+00\n  expRating[143]    9.39148769  0.56949306    8.25371191  9.040152e+00\n  expRating[144]    8.00732370  0.56648443    6.91443331  7.614595e+00\n  expRating[145]    4.49496065  0.53592995    3.49679927  4.143768e+00\n  expRating[146]    0.33251118  0.54896259   -0.73563222 -2.995595e-02\n  expRating[147]    2.98780527  0.54634883    1.98762062  2.587376e+00\n  expRating[148]    4.94598199  0.55693717    3.77429031  4.588355e+00\n  expRating[149]    3.08037239  0.55897838    1.92084799  2.718587e+00\n  expRating[150]    7.11983047  0.55007896    6.00512390  6.756943e+00\n  expRating[151]    1.95117993  0.46709683    1.12091551  1.629728e+00\n  expRating[152]    2.37692230  0.45515777    1.59488911  2.057251e+00\n  expRating[153]    3.11218619  0.46358249    2.32188933  2.791811e+00\n  expRating[154]    0.91045000  0.56007487   -0.19011751  5.444660e-01\n  expRating[155]    1.96170365  0.55563426    0.92637141  1.566433e+00\n  expRating[156]    4.09437621  0.57584988    2.97687526  3.739626e+00\n  expRating[157]    5.93429159  0.57180986    4.80035989  5.528646e+00\n  expRating[158]    4.06868199  0.58732661    2.92274908  3.665317e+00\n  expRating[159]    8.10814008  0.56252919    7.00739256  7.732233e+00\n  expRating[160]    3.97149436  0.56843340    2.79940653  3.587250e+00\n  expRating[161]    1.52283205  0.56072193    0.45744041  1.142760e+00\n  expRating[162]    2.53290977  0.57091484    1.39075521  2.150702e+00\n  expRating[163]    3.71992835  0.57869498    2.62525840  3.331459e+00\n  expRating[164]    2.70708612  0.58019579    1.56531432  2.326412e+00\n  expRating[165]    4.88093461  0.57135778    3.73319851  4.484927e+00\n  expRating[166]    3.89884234  0.55516403    2.87115136  3.510981e+00\n  expRating[167]   -0.26360713  0.56398053   -1.31598130 -6.539206e-01\n  expRating[168]    2.74703793  0.56873968    1.66026589  2.355955e+00\n  expRating[169]    3.19243174  0.53859503    2.17904812  2.817074e+00\n  expRating[170]    2.33689986  0.53543414    1.31110937  1.938263e+00\n  expRating[171]    4.99219395  0.52443858    4.01239995  4.632750e+00\n  expRating[172]    7.10432240  0.53269408    6.03554341  6.735155e+00\n  expRating[173]    8.11440013  0.54787720    7.00092583  7.730110e+00\n  expRating[174]    8.90408462  0.53975721    7.84282014  8.529210e+00\n  expRating[175]    3.63083418  0.53984353    2.53901127  3.271408e+00\n  expRating[176]    1.72404866  0.55607231    0.66872761  1.349240e+00\n  expRating[177]    4.37934275  0.55473101    3.28542561  3.985294e+00\n  expRating[178]    6.50299093  0.55770243    5.40826399  6.113945e+00\n  expRating[179]    9.80990853  0.55333303    8.73334351  9.449908e+00\n  expRating[180]    9.68691714  0.58317326    8.54355608  9.292770e+00\n  expRating[181]    2.78868838  0.55108463    1.68952715  2.405136e+00\n  expRating[182]    1.81702208  0.54407688    0.76676814  1.450531e+00\n  expRating[183]    2.92088162  0.55261324    1.84299710  2.552301e+00\n  expRating[184]    9.49151117  0.50047613    8.40562136  9.161823e+00\n  expRating[185]    9.91725354  0.49346906    8.90190577  9.585465e+00\n  expRating[186]    9.62370441  0.48677117    8.62151805  9.286129e+00\n  expRating[187]   10.66013686  0.49807653    9.62346454  1.033979e+01\n  expRating[188]    8.36329698  0.49950461    7.33293014  8.036647e+00\n  expRating[189]   10.53714547  0.49000561    9.48957035  1.021332e+01\n  expRating[190]    2.39700110  0.53690589    1.41182569  2.022271e+00\n  expRating[191]    1.42533479  0.55146411    0.45933099  1.037628e+00\n  expRating[192]    3.55800736  0.54569235    2.58230733  3.170137e+00\n  expRating[193]    6.90432680  0.56900144    5.78284915  6.553492e+00\n  expRating[194]    7.33006917  0.56151096    6.24361811  6.957015e+00\n  expRating[195]    8.18832445  0.57773679    7.04064438  7.804610e+00\n  expRating[196]    3.84103266  0.58815522    2.67886749  3.459217e+00\n  expRating[197]    2.82819043  0.59398972    1.62600644  2.430777e+00\n  expRating[198]    0.96258083  0.58894715   -0.17095556  5.801086e-01\n  expRating[199]    0.78115910  0.57568122   -0.26783075  3.659614e-01\n  expRating[200]    4.08807670  0.58669919    2.97514796  3.667681e+00\n  expRating[201]   -0.07437278  0.57039708   -1.10469753 -4.780726e-01\n  expRating[202]    3.69833144  0.60394613    2.52279193  3.297210e+00\n  expRating[203]    4.55658672  0.61038475    3.30511515  4.168796e+00\n  expRating[204]    0.39413724  0.59861215   -0.81960633 -8.894346e-03\n  expRating[205]    9.58508471  0.47540498    8.56051403  9.278345e+00\n  expRating[206]   10.01082707  0.46678852    9.03051133  9.705172e+00\n  expRating[207]    9.71727794  0.46903581    8.74090826  9.416607e+00\n  expRating[208]    7.07808560  0.57392550    6.03918933  6.681186e+00\n  expRating[209]    4.82242165  0.55888407    3.78920172  4.427444e+00\n  expRating[210]    5.57093022  0.57376259    4.48657904  5.163615e+00\n  expRating[211]    5.05239874  0.56577398    3.86106048  4.684545e+00\n  expRating[212]    5.47814110  0.56175009    4.31684370  5.123201e+00\n  expRating[213]    3.02947879  0.56310527    1.82372387  2.667421e+00\n  expRating[214]    4.94879115  0.56474942    3.80683361  4.584643e+00\n  expRating[215]    5.95886887  0.57700186    4.75333402  5.577204e+00\n  expRating[216]    6.74855336  0.57247119    5.68461525  6.372134e+00\n  expRating[217]    5.06898056  0.54423840    4.04112126  4.700053e+00\n  expRating[218]    2.77214068  0.55362143    1.66849959  2.405857e+00\n  expRating[219]    3.56182517  0.54847644    2.48184035  3.198051e+00\n  expRating[220]    8.35099188  0.56917791    7.29988745  7.941748e+00\n  expRating[221]    5.04679768  0.56811832    3.95049046  4.662431e+00\n  expRating[222]    7.70209178  0.57123224    6.58589910  7.321588e+00\n  expRating[223]    9.45559586  0.53616769    8.39344670  9.114277e+00\n  expRating[224]    7.00693355  0.53458426    5.90202129  6.647889e+00\n  expRating[225]    9.16204673  0.52531475    8.07166388  8.839438e+00\n  expRating[226]    5.35642807  0.56221739    4.30147215  4.977117e+00\n  expRating[227]    3.33350812  0.55294891    2.25086107  2.941318e+00\n  expRating[228]    4.34358584  0.55836681    3.25474964  3.956227e+00\n  expRating[229]    1.87979227  0.61238371    0.65905414  1.481106e+00\n  expRating[230]    2.88986999  0.61294707    1.65000221  2.487548e+00\n  expRating[231]    1.02426039  0.62383234   -0.20300586  5.969453e-01\n  expRating[232]    4.67719617  0.60854936    3.43327240  4.291254e+00\n  expRating[233]    5.96119381  0.60445405    4.73041838  5.568830e+00\n  expRating[234]    1.79874434  0.60724112    0.56454225  1.380034e+00\n  expRating[235]    7.11133133  0.57999810    5.96707839  6.726610e+00\n  expRating[236]    5.08841138  0.57171861    4.00004761  4.687165e+00\n  expRating[237]    6.09848910  0.57937220    4.96716187  5.713642e+00\n  expRating[238]    2.65197819  0.56836995    1.50992729  2.294404e+00\n  expRating[239]    4.80709138  0.56295629    3.73715512  4.417688e+00\n  expRating[240]    4.45174041  0.56601989    3.37562660  4.060096e+00\n  expRating[241]    4.73121205  0.56552642    3.63270019  4.371550e+00\n  expRating[242]    3.87568017  0.56886901    2.74198896  3.533499e+00\n  expRating[243]    6.88632523  0.54289735    5.80729785  6.548118e+00\n  expRating[244]    3.37799797  0.55682480    2.27692712  3.006099e+00\n  expRating[245]    2.36515574  0.55914697    1.28726196  1.976588e+00\n  expRating[246]    3.15484023  0.55239193    2.10342303  2.800239e+00\n  expRating[247]    5.57686190  0.59454764    4.39303671  5.192934e+00\n  expRating[248]    4.60519559  0.57374961    3.45042456  4.247680e+00\n  expRating[249]    5.35370416  0.56932067    4.19356474  4.983012e+00\n  expRating[250]    1.94971492  0.42987292    1.20061503  1.645131e+00\n  expRating[251]    2.37545728  0.42736659    1.64970648  2.057982e+00\n  expRating[252]   -0.07320504  0.41735210   -0.77647129 -3.858616e-01\n  expRating[253]    3.10296155  0.58313940    1.89919955  2.727910e+00\n  expRating[254]    6.11360661  0.58046638    4.97428149  5.731687e+00\n  expRating[255]    5.75825564  0.59323742    4.66419578  5.366986e+00\n  expRating[256]    6.46195794  0.55633363    5.38201918  6.078587e+00\n  expRating[257]    5.60642606  0.55895583    4.55846422  5.242347e+00\n  expRating[258]    8.26172016  0.55627001    7.14302542  7.899298e+00\n  expRating[259]    2.64904211  0.58099743    1.58200530  2.244940e+00\n  expRating[260]   -0.22940972  0.56921533   -1.25607090 -6.488939e-01\n  expRating[261]    2.42588438  0.58243320    1.31446361  2.026471e+00\n  expRating[262]    7.81896874  0.55825062    6.68103377  7.449467e+00\n  expRating[263]    7.86014466  0.56112964    6.75704472  7.485237e+00\n  expRating[264]    5.95335914  0.56547585    4.85631678  5.577055e+00\n  expRating[265]    5.94386471  0.55787692    4.84092179  5.587730e+00\n  expRating[266]    4.03707919  0.56224161    2.93595877  3.675686e+00\n  expRating[267]    6.69237328  0.55794174    5.60622868  6.340787e+00\n  expRating[268]    7.73710370  0.56417715    6.61386040  7.380948e+00\n  expRating[269]    5.87149410  0.54872516    4.80557974  5.504639e+00\n  expRating[270]    8.88213916  0.55564911    7.81368897  8.497214e+00\n  expRating[271]    3.90870649  0.55182949    2.83292627  3.550237e+00\n  expRating[272]    2.47012190  0.54479936    1.39621992  2.112976e+00\n  expRating[273]    2.51129782  0.55160811    1.37172933  2.171092e+00\n  expRating[274]    3.67227546  0.58134909    2.56201044  3.275714e+00\n  expRating[275]    4.09801783  0.57356262    2.97370545  3.720004e+00\n  expRating[276]    3.80446870  0.58524793    2.68116027  3.411170e+00\n  expRating[277]    4.75682550  0.54290849    3.74012834  4.392757e+00\n  expRating[278]    5.61508078  0.55023400    4.54797921  5.243287e+00\n  expRating[279]    4.46327637  0.53278194    3.44235650  4.117820e+00\n  expRating[280]    3.16004245  0.56612736    2.03570573  2.779911e+00\n  expRating[281]    4.44404009  0.56333607    3.36215895  4.064808e+00\n  expRating[282]    2.18837614  0.54332044    1.10585342  1.844516e+00\n  expRating[283]    8.51989720  0.58058057    7.42385014  8.113599e+00\n  expRating[284]    9.37815249  0.58193755    8.25512935  9.003453e+00\n  expRating[285]    8.22634807  0.58595140    7.04264997  7.839410e+00\n  expRating[286]    5.06915444  0.55851241    3.96225071  4.688320e+00\n  expRating[287]    3.67174577  0.57263885    2.53814308  3.289741e+00\n  expRating[288]    4.77560531  0.55799160    3.69359058  4.382082e+00\n  expRating[289]    6.24828854  0.55544908    5.18469555  5.873066e+00\n  expRating[290]    4.85087987  0.55214333    3.75686886  4.487517e+00\n  expRating[291]    5.95473941  0.54237135    4.87256068  5.583732e+00\n  expRating[292]    7.26041380  0.58439565    6.16972403  6.852374e+00\n  expRating[293]    5.86300513  0.58004323    4.75279212  5.456140e+00\n  expRating[294]    6.96686467  0.57256802    5.87043787  6.552363e+00\n  expRating[295]    5.16399921  0.55379136    4.14130322  4.773590e+00\n  expRating[296]    3.76659054  0.54487265    2.73553627  3.394280e+00\n  expRating[297]    4.87045008  0.53516848    3.86717208  4.508199e+00\n  expRating[298]    4.25294123  0.54364172    3.18858178  3.869364e+00\n  expRating[299]    5.30419488  0.53439615    4.20914414  4.961474e+00\n  expRating[300]    7.43686744  0.53814704    6.39258736  7.076480e+00\n  lp__           -102.16958645 10.25421623 -123.67102322 -1.085828e+02\n                stats\nparameter                  50%         75%       97.5%\n  ability[1]        5.60485680   6.0210387   6.8176243\n  ability[2]        7.99498992   8.3492334   9.0385637\n  ability[3]        6.77867831   7.1740984   7.8688141\n  ability[4]        7.05717033   7.4631171   8.3067393\n  ability[5]        3.10015008   3.4849073   4.1997057\n  ability[6]        4.52772299   4.8985905   5.6428513\n  ability[7]        8.92272188   9.2839158   9.8528980\n  ability[8]        4.21499469   4.5872345   5.3632894\n  ability[9]        2.85110149   3.2403483   4.0311530\n  ability[10]       8.82673374   9.1924942   9.8135110\n  ability[11]       5.66777035   6.0537916   6.9801283\n  ability[12]       6.65533243   7.0552206   7.9141159\n  ability[13]       1.57303359   1.9498399   2.6719186\n  ability[14]       9.37020661   9.6515461   9.9419375\n  ability[15]       5.80624252   6.1896707   6.9795303\n  ability[16]       6.63359425   7.0510777   7.9051641\n  ability[17]       5.20301834   5.6011689   6.3216710\n  ability[18]       1.92976806   2.3025479   3.0162637\n  ability[19]       2.61198093   3.0184084   3.7291738\n  ability[20]       4.85880216   5.2450924   5.9210744\n  ability[21]       6.84351022   7.2732296   8.0048784\n  ability[22]       7.62738075   8.0145371   8.7746327\n  ability[23]       3.80438911   4.1434454   4.9514641\n  ability[24]       8.67744246   9.0603908   9.6101948\n  ability[25]       2.12631352   2.4834270   3.1591698\n  ability[26]       5.92323695   6.3369959   7.0442405\n  ability[27]       2.61814980   2.9718831   3.6342923\n  ability[28]       2.10648784   2.4756226   3.1994121\n  ability[29]       5.05529957   5.5079430   6.2332072\n  ability[30]       8.37035841   8.7431154   9.5732055\n  ability[31]       8.79754596   9.2094272   9.7664386\n  ability[32]       2.34480493   2.7533221   3.4750496\n  ability[33]       9.07951043   9.4703882   9.9127584\n  ability[34]       7.29318283   7.6615051   8.3403609\n  ability[35]       5.50832134   5.8893263   6.6442820\n  ability[36]       5.88943368   6.2869530   7.0265478\n  ability[37]       3.99302544   4.4136788   5.0982113\n  ability[38]       1.98810882   2.3712397   3.1315866\n  ability[39]       2.24714949   2.6515893   3.3505611\n  ability[40]       1.88197213   2.2306132   2.9889394\n  ability[41]       7.73924061   8.1158904   8.8423954\n  ability[42]       7.09254852   7.4910605   8.2500309\n  ability[43]       8.55245693   8.9598616   9.6497610\n  ability[44]       7.66451933   8.0870040   8.9164697\n  ability[45]       5.90869207   6.3186237   7.0849979\n  ability[46]       2.73329196   3.1808272   3.8699416\n  ability[47]       9.21765574   9.5436606   9.9537571\n  ability[48]       7.91533978   8.2769880   9.1374037\n  ability[49]       2.89096602   3.2538390   3.9955560\n  ability[50]       5.65439234   6.0310405   6.7330004\n  ability[51]       1.55148179   1.8809578   2.5939436\n  ability[52]       2.60682232   2.9804000   3.7696743\n  ability[53]       6.61647131   7.0448198   7.7378705\n  ability[54]       3.21730267   3.5923390   4.3894463\n  ability[55]       3.41012376   3.7693624   4.5194129\n  ability[56]       2.24413699   2.6712823   3.5154393\n  ability[57]       4.87360950   5.2430227   5.9636334\n  ability[58]       8.82761717   9.1916081   9.8635971\n  ability[59]       4.28477782   4.6450306   5.4412166\n  ability[60]       8.21361673   8.6000440   9.2979297\n  ability[61]       2.45103247   2.8103794   3.6794444\n  ability[62]       9.19467008   9.5479758   9.9686173\n  ability[63]       9.06503301   9.4342279   9.8824007\n  ability[64]       2.03227613   2.4519635   3.1945450\n  ability[65]       6.54323176   6.9424836   7.7790439\n  ability[66]       3.49869755   3.9143903   4.6900784\n  ability[67]       2.46215402   2.8884382   3.6613903\n  ability[68]       2.95522140   3.3499083   4.1175259\n  ability[69]       9.32314983   9.6355705   9.9546168\n  ability[70]       5.46323215   5.8807425   6.5795727\n  ability[71]       4.74147873   5.1040713   5.8446076\n  ability[72]       6.63491859   7.0549853   7.8304383\n  ability[73]       3.43356399   3.8225186   4.6448631\n  ability[74]       7.61145719   7.9841458   8.8394808\n  ability[75]       8.70360594   9.0787883   9.7421692\n  ability[76]       5.04386233   5.4139578   6.1439397\n  ability[77]       3.56682309   3.9980672   4.8207021\n  ability[78]       4.35519917   4.7861945   5.5715112\n  ability[79]       6.76881343   7.1747361   8.0033988\n  ability[80]       4.35964447   4.7101998   5.4949561\n  ability[81]       6.41658449   6.8002795   7.5841941\n  ability[82]       3.03600727   3.4494641   4.1496697\n  ability[83]       5.26568491   5.6570724   6.4231962\n  ability[84]       1.56531325   1.8673975   2.5490537\n  ability[85]       5.66413011   6.0681367   6.8444488\n  ability[86]       8.17703235   8.5431943   9.2286487\n  ability[87]       2.30336083   2.6686222   3.5724551\n  ability[88]       8.49088185   8.8993552   9.5801782\n  ability[89]       6.58400494   6.9422820   7.8113482\n  ability[90]       8.39535596   8.7799949   9.6376669\n  ability[91]       3.15283375   3.5261201   4.2729606\n  ability[92]       3.33660029   3.7272214   4.4792513\n  ability[93]       4.01516006   4.3625143   5.1260717\n  ability[94]       2.85923496   3.2195783   3.9284384\n  ability[95]       7.74714779   8.1776268   9.0137420\n  ability[96]       4.34291752   4.7173973   5.4319720\n  ability[97]       5.47593766   5.8826056   6.7233953\n  ability[98]       6.53656780   6.9194567   7.6168865\n  ability[99]       4.38222178   4.8055925   5.6162105\n  ability[100]      5.93865193   6.3217247   7.0173305\n  severity[1]       0.32129340   0.5156112   0.8280331\n  severity[2]       0.75400600   0.9225778   1.2450327\n  severity[3]      -1.67768348  -1.5151831  -1.2091287\n  severity[4]       1.62204219   1.8105537   2.1241806\n  severity[5]      -0.68195060  -0.5007481  -0.1523441\n  severity[6]      -0.63247826  -0.4622547  -0.1349356\n  severity[7]      -2.54155507  -2.3572182  -1.9908115\n  severity[8]       0.47609884   0.6348836   0.9490976\n  severity[9]       1.49706523   1.6748256   1.9956876\n  severity[10]      0.12419071   0.3003088   0.6264241\n  sigma             0.93219182   0.9654415   1.0408830\n  tau               1.41763607   1.6884055   2.4946427\n  expRating[1]      3.92830878   4.3178437   5.0738749\n  expRating[2]      7.23308008   7.6100094   8.3993371\n  expRating[3]      4.97625511   5.3432537   6.1165330\n  expRating[4]      8.29105334   8.6697035   9.4035793\n  expRating[5]      9.57995780   9.9411177  10.6690700\n  expRating[6]      7.32506451   7.7057395   8.4140909\n  expRating[7]      6.16821709   6.4917359   7.1587379\n  expRating[8]      7.25768030   7.6137279   8.2333317\n  expRating[9]      6.89234680   7.2687475   7.8742009\n  expRating[10]     7.82465495   8.2182514   8.9674947\n  expRating[11]     6.44931259   6.8035479   7.5599235\n  expRating[12]     4.52615446   4.9021665   5.6159855\n  expRating[13]     3.87400622   4.2179985   4.9997164\n  expRating[14]     4.71013446   5.0776066   5.8376751\n  expRating[15]     2.41667050   2.7837297   3.4738577\n  expRating[16]     6.14521279   6.5210467   7.1953894\n  expRating[17]     3.84400200   4.2273681   4.8844833\n  expRating[18]     4.60434637   5.0088073   5.6711742\n  expRating[19]     9.24032252   9.5787647  10.2077788\n  expRating[20]     7.23463534   7.5683629   8.1536697\n  expRating[21]     9.39760551   9.7006498  10.2672634\n  expRating[22]     2.51530596   2.8573587   3.5563167\n  expRating[23]     3.55504799   3.9577030   4.6711570\n  expRating[24]     5.70819260   6.0720553   6.7679281\n  expRating[25]     3.17232853   3.5543619   4.3188990\n  expRating[26]     1.14988606   1.5378573   2.2459810\n  expRating[27]     0.29388164   0.6759191   1.4228737\n  expRating[28]    10.42343119  10.8159413  11.4820129\n  expRating[29]     9.24212473   9.6391586  10.3033677\n  expRating[30]    10.30813311  10.7034795  11.2837926\n  expRating[31]     7.26970512   7.6403300   8.4295264\n  expRating[32]     7.13239684   7.5228439   8.3429401\n  expRating[33]     5.77028565   6.1285887   6.9677823\n  expRating[34]     7.39525465   7.8055040   8.6003512\n  expRating[35]     8.27050806   8.6482802   9.4405132\n  expRating[36]     5.96537094   6.3549608   7.1339555\n  expRating[37]     2.37351779   2.6735463   3.3588891\n  expRating[38]    -0.08835914   0.2543054   0.9185914\n  expRating[39]     2.06042140   2.3905476   3.0633292\n  expRating[40]     9.67631456   9.9570427  10.4807653\n  expRating[41]     8.69134604   8.9905213   9.4670830\n  expRating[42]     9.80085147  10.0902793  10.5527444\n  expRating[43]     6.14760496   6.5140434   7.3097982\n  expRating[44]     3.25661706   3.6342643   4.3998987\n  expRating[45]     7.28702080   7.6849626   8.4349731\n  expRating[46]     8.23310641   8.6385753   9.4364562\n  expRating[47]     5.99875014   6.3830382   7.1402843\n  expRating[48]     4.10628342   4.4658478   5.2683801\n  expRating[49]     5.96552370   6.3523336   7.0025934\n  expRating[50]     2.64877338   3.0191493   3.6975496\n  expRating[51]     5.28718587   5.7102204   6.3810382\n  expRating[52]     1.26271658   1.5968366   2.2491097\n  expRating[53]     3.42024398   3.8212314   4.4560596\n  expRating[54]     2.04506481   2.4049038   3.0635727\n  expRating[55]     4.25476979   4.6247449   5.3052766\n  expRating[56]     1.99823438   2.3270353   3.0680436\n  expRating[57]     0.06941949   0.4506340   1.1481334\n  expRating[58]     4.15988444   4.5317107   5.2731869\n  expRating[59]     4.19303347   4.5856895   5.2546112\n  expRating[60]     6.34935085   6.7146657   7.4038326\n  expRating[61]     7.60184443   7.9857425   8.7661933\n  expRating[62]     6.22242817   6.6072587   7.3555677\n  expRating[63]     7.30404741   7.7083385   8.5236626\n  expRating[64]     7.95975062   8.3048741   9.0851816\n  expRating[65]     9.24177206   9.6207183  10.3682323\n  expRating[66]     5.06696066   5.4676025   6.2347025\n  expRating[67]     2.09519888   2.4534931   3.1830466\n  expRating[68]     5.39072288   5.7353204   6.5673462\n  expRating[69]     4.24175950   4.5854303   5.3069706\n  expRating[70]     9.01079595   9.3682370   9.9959028\n  expRating[71]     6.97369286   7.3415865   7.9509231\n  expRating[72]    10.18302043  10.5274745  11.1312480\n  expRating[73]     2.90108873   3.2342270   3.8583156\n  expRating[74]     2.60043581   2.9373493   3.5954296\n  expRating[75]     3.63534550   3.9676609   4.6400327\n  expRating[76]     7.56804202   7.9337163   8.6424363\n  expRating[77]     5.29886809   5.6910571   6.3328881\n  expRating[78]     6.39695288   6.7834605   7.4509457\n  expRating[79]     0.90266615   1.2695464   1.9526286\n  expRating[80]     4.21861116   4.5779745   5.2702088\n  expRating[81]     4.09819850   4.4551275   5.1495481\n  expRating[82]     2.87252762   3.2375970   3.8697617\n  expRating[83]     2.57957214   2.9535875   3.6695514\n  expRating[84]     2.22330533   2.5870831   3.2621392\n  expRating[85]     4.39733814   4.7898754   5.5613684\n  expRating[86]     6.55582601   6.9704614   7.7003417\n  expRating[87]     5.17702850   5.5829059   6.2726414\n  expRating[88]     8.69097215   9.0816248   9.8777978\n  expRating[89]     7.72473854   8.1040142   8.8549137\n  expRating[90]     9.86907483  10.2300954  11.0046256\n  expRating[91]    10.40984310  10.8099548  11.3932867\n  expRating[92]    10.29323269  10.6537819  11.2162189\n  expRating[93]     8.91534833   9.2469391   9.8844537\n  expRating[94]     0.65426252   1.0402401   1.7311590\n  expRating[95]     1.73076564   2.0983607   2.7466911\n  expRating[96]     2.45794887   2.8574064   3.5320384\n  expRating[97]    10.68723213  11.0258740  11.6000851\n  expRating[98]     6.52521929   6.8931632   7.4124534\n  expRating[99]    10.57243378  10.9148814  11.4223299\n  expRating[100]    8.04653547   8.4137989   9.1228833\n  expRating[101]    7.76736882   8.0896577   8.8439649\n  expRating[102]    7.38819600   7.7451578   8.5319587\n  expRating[103]    4.80210868   5.1835357   5.9499975\n  expRating[104]    6.96861359   7.3633526   8.1388468\n  expRating[105]    5.58795081   5.9883908   6.6961253\n  expRating[106]    6.63006080   7.0282763   7.8916094\n  expRating[107]    5.23097902   5.6473714   6.4297249\n  expRating[108]    7.38745949   7.7682080   8.4952135\n  expRating[109]    4.30866235   4.7103371   5.3931378\n  expRating[110]    1.43210115   1.8189872   2.5207512\n  expRating[111]    5.44742415   5.9006343   6.5595419\n  expRating[112]    2.32007909   2.6889040   3.4155171\n  expRating[113]    1.31010002   1.6803265   2.4045578\n  expRating[114]   -0.59868708  -0.1948707   0.5880267\n  expRating[115]    3.87765464   4.2355239   4.9722584\n  expRating[116]    1.56755807   1.9671429   2.6312038\n  expRating[117]    3.73782508   4.1214601   4.8315003\n  expRating[118]    2.65848296   3.0187780   3.6913121\n  expRating[119]    0.19694341   0.5940671   1.2674050\n  expRating[120]    3.40957682   3.7809904   4.4529809\n  expRating[121]    8.52262605   8.8604208   9.5735123\n  expRating[122]    6.05120452   6.4227715   7.0580689\n  expRating[123]    7.83567511   8.2378220   8.9232432\n  expRating[124]    7.45454591   7.8077697   8.5068214\n  expRating[125]    6.43587715   6.7859023   7.4950205\n  expRating[126]    8.58478892   8.9813865   9.6717671\n  expRating[127]    6.88280414   7.2493453   7.9495060\n  expRating[128]    9.03672203   9.4242237  10.1423861\n  expRating[129]    8.67534709   9.0637840   9.7278106\n  expRating[130]    5.97906299   6.4025463   7.1666564\n  expRating[131]    9.15602824   9.5973851  10.3583767\n  expRating[132]    7.77439116   8.2214814   8.9712478\n  expRating[133]    7.52210135   7.9037868   8.5700624\n  expRating[134]    5.23018416   5.6266177   6.2834904\n  expRating[135]    6.36917395   6.7546234   7.4221094\n  expRating[136]    2.03805513   2.4708994   3.1008842\n  expRating[137]    2.10609738   2.4973443   3.1234430\n  expRating[138]    0.19099030   0.5860887   1.2865881\n  expRating[139]    9.53049016   9.8472696  10.3953996\n  expRating[140]    8.53900411   8.8528286   9.3731486\n  expRating[141]    9.65882121   9.9917923  10.5109631\n  expRating[142]    7.22155449   7.5667211   8.4120041\n  expRating[143]    9.39483558   9.7546462  10.5213747\n  expRating[144]    8.01475943   8.3864257   9.1083724\n  expRating[145]    4.49408186   4.8494800   5.5127494\n  expRating[146]    0.34069094   0.7115819   1.4022309\n  expRating[147]    2.99812842   3.3448988   4.0469769\n  expRating[148]    4.95525592   5.3098435   6.0342911\n  expRating[149]    3.06932283   3.4419033   4.1292682\n  expRating[150]    7.12871495   7.4751476   8.1454200\n  expRating[151]    1.91577708   2.2245292   2.9727565\n  expRating[152]    2.33881075   2.6435046   3.3847764\n  expRating[153]    3.06879272   3.4305056   4.1035415\n  expRating[154]    0.91276616   1.2664021   2.0027813\n  expRating[155]    1.96693844   2.3421188   3.0793131\n  expRating[156]    4.08324966   4.4884739   5.2534725\n  expRating[157]    5.95489717   6.3074901   7.0357060\n  expRating[158]    4.07433614   4.4821144   5.1759644\n  expRating[159]    8.10319987   8.5053261   9.1471686\n  expRating[160]    3.98475009   4.3303430   5.0880154\n  expRating[161]    1.53060452   1.8939178   2.5978381\n  expRating[162]    2.54987893   2.9255142   3.6248881\n  expRating[163]    3.73403506   4.1202018   4.8251406\n  expRating[164]    2.72314792   3.1029561   3.8070077\n  expRating[165]    4.90040891   5.2795158   5.9610679\n  expRating[166]    3.88418373   4.2601236   5.0140460\n  expRating[167]   -0.27099816   0.1274765   0.8731906\n  expRating[168]    2.74482818   3.1025833   3.8638504\n  expRating[169]    3.19999863   3.5500924   4.2301896\n  expRating[170]    2.33595136   2.6885778   3.3802802\n  expRating[171]    4.99975989   5.3405187   5.9624733\n  expRating[172]    7.14118755   7.4786861   8.0843528\n  expRating[173]    8.14031492   8.4974607   9.1416403\n  expRating[174]    8.90107306   9.3040180   9.9425094\n  expRating[175]    3.62866989   3.9973796   4.6725669\n  expRating[176]    1.73868572   2.0808032   2.8582125\n  expRating[177]    4.36953671   4.7538742   5.4688395\n  expRating[178]    6.51540695   6.9010838   7.5300827\n  expRating[179]    9.81466659  10.1836505  10.8665552\n  expRating[180]    9.70865036  10.0770989  10.8485777\n  expRating[181]    2.80016527   3.1522981   3.8978555\n  expRating[182]    1.82140104   2.1771493   2.9282461\n  expRating[183]    2.93109170   3.2969939   4.0847893\n  expRating[184]    9.51275953   9.8521506  10.3855353\n  expRating[185]    9.92852459  10.2831757  10.8257949\n  expRating[186]    9.65027545   9.9823649  10.4795348\n  expRating[187]   10.65893235  10.9990972  11.5439670\n  expRating[188]    8.38947844   8.7094135   9.2672135\n  expRating[189]   10.54822528  10.8967479  11.4173710\n  expRating[190]    2.37500039   2.7599467   3.4919745\n  expRating[191]    1.39999388   1.7993033   2.5484247\n  expRating[192]    3.53884519   3.9253801   4.6581245\n  expRating[193]    6.89386712   7.2634034   8.0640452\n  expRating[194]    7.33906135   7.7056804   8.4232543\n  expRating[195]    8.17610762   8.5785946   9.2791551\n  expRating[196]    3.82324131   4.2271377   4.9981676\n  expRating[197]    2.82464211   3.2120030   3.9910671\n  expRating[198]    0.97003958   1.3527600   2.1071499\n  expRating[199]    0.75237821   1.1927052   1.9414744\n  expRating[200]    4.06942026   4.4842526   5.3339610\n  expRating[201]   -0.10911902   0.3146849   1.0706002\n  expRating[202]    3.68798212   4.1036310   4.9239171\n  expRating[203]    4.56411194   4.9548833   5.7268133\n  expRating[204]    0.41152832   0.7847527   1.5654653\n  expRating[205]    9.60424116   9.9144104  10.4346778\n  expRating[206]   10.03097939  10.3521719  10.8272616\n  expRating[207]    9.74162608  10.0617246  10.5104519\n  expRating[208]    7.08009117   7.4535683   8.1826059\n  expRating[209]    4.79705315   5.2216793   5.8590281\n  expRating[210]    5.55025722   5.9835831   6.6803962\n  expRating[211]    5.03802377   5.4473392   6.1618506\n  expRating[212]    5.50594242   5.8501656   6.5621408\n  expRating[213]    3.05043103   3.4007610   4.1088691\n  expRating[214]    4.93811994   5.3388823   6.0317895\n  expRating[215]    5.96585197   6.3441663   7.0957276\n  expRating[216]    6.75422442   7.1287534   7.9036028\n  expRating[217]    5.05707640   5.4587495   6.1025866\n  expRating[218]    2.74791383   3.1528495   3.7991049\n  expRating[219]    3.55710236   3.9277212   4.6750675\n  expRating[220]    8.34604934   8.7337542   9.4871761\n  expRating[221]    5.00749312   5.4413593   6.1906803\n  expRating[222]    7.69679893   8.0809471   8.8125117\n  expRating[223]    9.46212562   9.8095770  10.4763766\n  expRating[224]    7.00474835   7.3657146   8.0362951\n  expRating[225]    9.15048262   9.5229272  10.1767410\n  expRating[226]    5.34398520   5.7430296   6.4550628\n  expRating[227]    3.31915823   3.7186067   4.4226923\n  expRating[228]    4.33664133   4.7424547   5.3699805\n  expRating[229]    1.87532960   2.3000229   3.0426898\n  expRating[230]    2.87906163   3.3071939   4.1210699\n  expRating[231]    1.00087499   1.4448053   2.2245074\n  expRating[232]    4.68577192   5.0931789   5.8933167\n  expRating[233]    5.98663536   6.3711485   7.1421619\n  expRating[234]    1.81051057   2.2113694   2.9825856\n  expRating[235]    7.10256702   7.4805761   8.2804844\n  expRating[236]    5.09227461   5.4820008   6.2405100\n  expRating[237]    6.08592042   6.5043203   7.2387586\n  expRating[238]    2.64772723   3.0380376   3.7668837\n  expRating[239]    4.80354554   5.1871138   5.9084542\n  expRating[240]    4.45449110   4.8381301   5.5305257\n  expRating[241]    4.73237682   5.0844254   5.8458716\n  expRating[242]    3.88272832   4.2241924   5.0351132\n  expRating[243]    6.87203989   7.2237236   7.9647620\n  expRating[244]    3.36728106   3.7320621   4.4940006\n  expRating[245]    2.37028518   2.7191994   3.4415874\n  expRating[246]    3.14388760   3.5049474   4.2787534\n  expRating[247]    5.56933067   5.9699639   6.7447436\n  expRating[248]    4.59159911   4.9966774   5.7485858\n  expRating[249]    5.35312881   5.7310378   6.4805908\n  expRating[250]    1.91972743   2.2335953   2.8067068\n  expRating[251]    2.34648971   2.6538964   3.2628798\n  expRating[252]   -0.09532915   0.2001258   0.8175649\n  expRating[253]    3.09691269   3.5031537   4.2165923\n  expRating[254]    6.12605033   6.4973254   7.2258426\n  expRating[255]    5.75545022   6.1710365   6.9119824\n  expRating[256]    6.46072687   6.8495055   7.5315452\n  expRating[257]    5.61690861   5.9976592   6.7031759\n  expRating[258]    8.26153875   8.6367148   9.3371279\n  expRating[259]    2.64585737   3.0119567   3.8339161\n  expRating[260]   -0.21219316   0.1320225   0.9253944\n  expRating[261]    2.40334660   2.8187196   3.5724961\n  expRating[262]    7.81366112   8.2000428   8.8276168\n  expRating[263]    7.85052985   8.2567145   8.9718628\n  expRating[264]    5.95141941   6.3267144   7.0743557\n  expRating[265]    5.92339866   6.3120630   7.1133636\n  expRating[266]    4.03351620   4.3949573   5.1155952\n  expRating[267]    6.69541402   7.0408845   7.8240465\n  expRating[268]    7.71528890   8.1368813   8.8789385\n  expRating[269]    5.86319901   6.2188711   6.9628429\n  expRating[270]    8.87787956   9.2538357   9.9852128\n  expRating[271]    3.91746708   4.2668318   5.0012844\n  expRating[272]    2.48646643   2.8425482   3.5424777\n  expRating[273]    2.51525275   2.8852486   3.5561976\n  expRating[274]    3.65855017   4.0663484   4.8478172\n  expRating[275]    4.08771284   4.4802794   5.2327738\n  expRating[276]    3.80756887   4.2031261   4.9681613\n  expRating[277]    4.75549145   5.1258123   5.8162554\n  expRating[278]    5.59902746   5.9956758   6.7005315\n  expRating[279]    4.45535348   4.8010675   5.4964139\n  expRating[280]    3.15120646   3.5375761   4.3152397\n  expRating[281]    4.43965072   4.8398247   5.5069107\n  expRating[282]    2.17692909   2.5491073   3.2137916\n  expRating[283]    8.52601137   8.9028523   9.6793955\n  expRating[284]    9.38668694   9.7798796  10.5463799\n  expRating[285]    8.21911399   8.6240847   9.3564431\n  expRating[286]    5.08908281   5.4438341   6.1248205\n  expRating[287]    3.64231807   4.0865105   4.7681197\n  expRating[288]    4.79013640   5.1903366   5.8756708\n  expRating[289]    6.26719849   6.6111077   7.3306834\n  expRating[290]    4.84460201   5.1820984   5.9559263\n  expRating[291]    5.94911619   6.3023139   7.0458519\n  expRating[292]    7.25288745   7.6783823   8.3678395\n  expRating[293]    5.85705765   6.2560165   6.9956087\n  expRating[294]    6.96489208   7.3684642   8.1115676\n  expRating[295]    5.14674279   5.5167785   6.2978476\n  expRating[296]    3.76884555   4.1430578   4.8615909\n  expRating[297]    4.85932286   5.2237369   5.9323251\n  expRating[298]    4.25935681   4.6446743   5.2482471\n  expRating[299]    5.30204315   5.6635698   6.3185954\n  expRating[300]    7.43290216   7.8309580   8.4512007\n  lp__           -101.78819473 -95.1798455 -83.1917708\n\n, , chains = chain:2\n\n                stats\nparameter                 mean          sd          2.5%           25%\n  ability[1]        5.63288609  0.59559794    4.44782898    5.23774680\n  ability[2]        8.02078676  0.55112053    6.90832316    7.65587292\n  ability[3]        6.84272894  0.56106290    5.73790207    6.45762699\n  ability[4]        7.10847614  0.54651804    6.04567080    6.74007092\n  ability[5]        3.12006939  0.57077600    1.98030018    2.75401424\n  ability[6]        4.58768200  0.56624234    3.47229001    4.18979920\n  ability[7]        8.95236318  0.51151089    7.88539783    8.61110737\n  ability[8]        4.25614453  0.58060386    3.15201367    3.86793494\n  ability[9]        2.89029577  0.57896505    1.78448056    2.47849399\n  ability[10]       8.79009624  0.52009271    7.67530821    8.43988026\n  ability[11]       5.71196276  0.55867410    4.60640862    5.32624486\n  ability[12]       6.68805982  0.58593466    5.52169688    6.27762307\n  ability[13]       1.71437150  0.43388602    1.05137077    1.38625821\n  ability[14]       9.32598425  0.41570479    8.36359804    9.05141743\n  ability[15]       5.85612569  0.60646192    4.74860875    5.44508121\n  ability[16]       6.68886364  0.59198533    5.52189938    6.28128594\n  ability[17]       5.24911349  0.56728799    4.10870730    4.88519331\n  ability[18]       1.97024234  0.51279611    1.11191777    1.58135556\n  ability[19]       2.67587937  0.57570036    1.55211110    2.29242382\n  ability[20]       4.89859869  0.60059193    3.73353492    4.48589010\n  ability[21]       6.89134107  0.54839681    5.72226725    6.53108082\n  ability[22]       7.66984744  0.58295098    6.48128771    7.29273894\n  ability[23]       3.81057539  0.54796497    2.70738967    3.45766679\n  ability[24]       8.76514995  0.57756382    7.59991432    8.36053403\n  ability[25]       2.15091859  0.54386285    1.14008338    1.76781063\n  ability[26]       5.99424552  0.56873888    4.92770050    5.60507902\n  ability[27]       2.65902096  0.57898227    1.54561516    2.22811961\n  ability[28]       2.18281135  0.54931771    1.17256985    1.80270071\n  ability[29]       5.11401015  0.61999930    3.87659914    4.69304711\n  ability[30]       8.41644748  0.56555530    7.33166744    8.04902494\n  ability[31]       8.81813323  0.53648893    7.77985734    8.47204940\n  ability[32]       2.40669935  0.55331500    1.42501698    1.99093578\n  ability[33]       9.06945487  0.50622389    8.05553806    8.73515233\n  ability[34]       7.35223080  0.57138016    6.19731839    6.97065841\n  ability[35]       5.54168317  0.55546097    4.45153470    5.13497536\n  ability[36]       5.92404096  0.58735161    4.76800819    5.52578498\n  ability[37]       4.04210677  0.57264877    2.84663479    3.68972714\n  ability[38]       2.05756793  0.54150743    1.09622466    1.67391023\n  ability[39]       2.27636343  0.54496551    1.24758095    1.89497550\n  ability[40]       1.97964938  0.48966262    1.16311194    1.60719504\n  ability[41]       7.80419165  0.57970528    6.64573294    7.41436451\n  ability[42]       7.15935406  0.55837045    6.08972352    6.76305986\n  ability[43]       8.57240471  0.52795340    7.52210353    8.20014518\n  ability[44]       7.77013149  0.60390944    6.62738395    7.37661562\n  ability[45]       5.94101444  0.57846110    4.78159896    5.55508763\n  ability[46]       2.79938362  0.58697329    1.59886839    2.40126087\n  ability[47]       9.24156560  0.47453528    8.20512247    8.95153546\n  ability[48]       7.93931105  0.58543544    6.76315650    7.52369891\n  ability[49]       2.92821805  0.55759195    1.79867784    2.55733381\n  ability[50]       5.68866976  0.57622765    4.55101233    5.32307907\n  ability[51]       1.64995760  0.42721438    1.04081661    1.32610440\n  ability[52]       2.67678922  0.56925473    1.64717017    2.25787787\n  ability[53]       6.65779670  0.54510634    5.66099932    6.30324370\n  ability[54]       3.27333712  0.54945397    2.15127231    2.90324652\n  ability[55]       3.42045453  0.57861302    2.30489765    3.01044952\n  ability[56]       2.32235474  0.56842635    1.26482339    1.94242029\n  ability[57]       4.94544688  0.56854119    3.75738058    4.57858229\n  ability[58]       8.85795167  0.54156780    7.71155085    8.50213186\n  ability[59]       4.34037680  0.58941435    3.16439563    3.93953598\n  ability[60]       8.21042888  0.56010648    7.02563764    7.83535929\n  ability[61]       2.47741205  0.57824500    1.33206564    2.09002040\n  ability[62]       9.17074616  0.49019854    8.11451238    8.84078454\n  ability[63]       9.06963648  0.50351041    8.00206373    8.74479915\n  ability[64]       2.12772973  0.52689219    1.16735086    1.75205327\n  ability[65]       6.63859759  0.57301792    5.57026890    6.23841740\n  ability[66]       3.55375707  0.62831246    2.28613817    3.14330785\n  ability[67]       2.53650547  0.57142755    1.44520560    2.13316489\n  ability[68]       3.05846606  0.56829864    1.94414387    2.66662181\n  ability[69]       9.30608496  0.44587121    8.31766212    9.02581985\n  ability[70]       5.53006551  0.60359306    4.39612229    5.10349130\n  ability[71]       4.76454856  0.57836662    3.66004121    4.34082824\n  ability[72]       6.69444791  0.56743489    5.57367705    6.30587951\n  ability[73]       3.48685587  0.56737054    2.34520741    3.11399415\n  ability[74]       7.67575450  0.58071190    6.53174647    7.26912389\n  ability[75]       8.73165427  0.52575671    7.73926980    8.35587316\n  ability[76]       5.03888152  0.59470707    3.81213794    4.65376954\n  ability[77]       3.61942956  0.59429734    2.46768660    3.21163645\n  ability[78]       4.40248922  0.56807529    3.30854897    4.02462218\n  ability[79]       6.82317006  0.60586923    5.63412065    6.40763619\n  ability[80]       4.39441027  0.61195734    3.17585186    3.97448104\n  ability[81]       6.45710187  0.56830078    5.37150308    6.07256544\n  ability[82]       3.10994037  0.58307848    2.05525018    2.69582411\n  ability[83]       5.34766007  0.60654243    4.20903128    4.90554127\n  ability[84]       1.64975279  0.42645450    1.03818493    1.32192157\n  ability[85]       5.71154228  0.55718874    4.55961840    5.35881986\n  ability[86]       8.24203691  0.57737673    7.12599436    7.84479119\n  ability[87]       2.39993209  0.50923642    1.42138321    2.05704119\n  ability[88]       8.53712711  0.53746521    7.46216389    8.19566720\n  ability[89]       6.64734547  0.57636662    5.44886810    6.27305822\n  ability[90]       8.48249964  0.55940881    7.33371931    8.12091518\n  ability[91]       3.20395080  0.56850626    2.14192696    2.81470363\n  ability[92]       3.41807257  0.56062575    2.30341697    3.05835550\n  ability[93]       4.05304853  0.57692883    2.94471056    3.64752375\n  ability[94]       2.87427486  0.57711649    1.71278022    2.49084341\n  ability[95]       7.79294898  0.58945690    6.61668885    7.42611206\n  ability[96]       4.38891010  0.54317397    3.29982574    4.00338665\n  ability[97]       5.51282051  0.58191425    4.40664278    5.13217563\n  ability[98]       6.52697127  0.58443690    5.34115351    6.13812692\n  ability[99]       4.46261418  0.58224239    3.34711029    4.06780894\n  ability[100]      6.00681855  0.62353188    4.81988432    5.59785176\n  severity[1]       0.27337690  0.26785813   -0.27166745    0.08946385\n  severity[2]       0.70877167  0.25772692    0.20484450    0.53719080\n  severity[3]      -1.74411184  0.27479960   -2.28496699   -1.92679192\n  severity[4]       1.55745790  0.26998232    1.03952816    1.37357456\n  severity[5]      -0.74206444  0.27071469   -1.26926253   -0.91665443\n  severity[6]      -0.69727616  0.27064788   -1.22561037   -0.88048618\n  severity[7]      -2.60308255  0.27639053   -3.15821467   -2.78103470\n  severity[8]       0.41443349  0.25446160   -0.08542940    0.24223035\n  severity[9]       1.44474226  0.26964899    0.91033113    1.26955969\n  severity[10]      0.04586936  0.26647390   -0.45499695   -0.13292516\n  sigma             0.93313009  0.04953846    0.84208175    0.89765894\n  tau               1.50290846  0.44236755    0.94831164    1.21026249\n  expRating[1]      3.88877426  0.57627925    2.75287188    3.49285203\n  expRating[2]      7.19034400  0.56326413    6.04280382    6.80835852\n  expRating[3]      4.93560993  0.58146132    3.80138307    4.52872322\n  expRating[4]      8.29416366  0.52881323    7.20919870    7.92992805\n  expRating[5]      9.57824466  0.54687520    8.50962530    9.22725559\n  expRating[6]      7.32351059  0.53444569    6.24815021    6.96966681\n  expRating[7]      6.14545278  0.54119706    5.06219874    5.78885247\n  expRating[8]      7.25716243  0.53353878    6.21318304    6.91856779\n  expRating[9]      6.88859830  0.54025690    5.85189683    6.53837813\n  expRating[10]     7.81724781  0.53389645    6.73061190    7.45476852\n  expRating[11]     6.41119997  0.52977502    5.38283012    6.05123877\n  expRating[12]     4.50539359  0.54149113    3.48094254    4.15318816\n  expRating[13]     3.82884106  0.54398815    2.80222791    3.47011995\n  expRating[14]     4.67752729  0.55597381    3.63151044    4.28809550\n  expRating[15]     2.37800495  0.54844182    1.33237411    1.99562822\n  expRating[16]     6.14513991  0.53836767    5.06583404    5.77858550\n  expRating[17]     3.84561756  0.54590210    2.82761006    3.45323511\n  expRating[18]     4.63355136  0.55325957    3.60315137    4.24796132\n  expRating[19]     9.22574008  0.49629256    8.16669325    8.89162052\n  expRating[20]     7.20825134  0.50038810    6.16326929    6.88943670\n  expRating[21]     9.36679666  0.50637672    8.38220626    9.01161762\n  expRating[22]     2.51203269  0.56973289    1.35420205    2.14340627\n  expRating[23]     3.55886837  0.56669326    2.44806013    3.19886977\n  expRating[24]     5.70088679  0.55950964    4.63864061    5.33588746\n  expRating[25]     3.16367267  0.55789497    2.07715823    2.78530903\n  expRating[26]     1.14618393  0.54632217    0.08452074    0.77517696\n  expRating[27]     0.28721322  0.54574999   -0.76454032   -0.08694284\n  expRating[28]    10.34755414  0.50241166    9.35852264   10.01381443\n  expRating[29]     9.20452972  0.51990981    8.14826285    8.85149146\n  expRating[30]    10.23483850  0.51427092    9.18788541    9.89351264\n  expRating[31]     7.26942067  0.55515618    6.16609615    6.88267881\n  expRating[32]     7.15670502  0.53725163    6.09951981    6.79388368\n  expRating[33]     5.75783212  0.55059961    4.67759697    5.38848695\n  expRating[34]     7.39683149  0.56774745    6.25994100    7.00594701\n  expRating[35]     8.24551773  0.57658494    7.10775146    7.84782053\n  expRating[36]     5.94599538  0.57643970    4.81767630    5.55430678\n  expRating[37]     2.42314317  0.44741744    1.66240570    2.11894883\n  expRating[38]    -0.02974034  0.44425139   -0.80076676   -0.34398258\n  expRating[39]     2.12880498  0.45550713    1.35288270    1.80515096\n  expRating[40]     9.59936115  0.43710105    8.68664836    9.31322602\n  expRating[41]     8.62870809  0.43828993    7.69303559    8.35117230\n  expRating[42]     9.74041774  0.42859832    8.82645251    9.46146390\n  expRating[43]     6.12950259  0.59199417    5.03849142    5.71864991\n  expRating[44]     3.25304314  0.57655602    2.17401957    2.85057269\n  expRating[45]     7.30086795  0.59409030    6.20295249    6.89786140\n  expRating[46]     8.24632154  0.56531327    7.12412554    7.86882884\n  expRating[47]     5.99158747  0.56199997    4.86503640    5.60317076\n  expRating[48]     4.08578109  0.57374623    3.00509833    3.70647223\n  expRating[49]     5.95788517  0.55701485    4.86417682    5.58024779\n  expRating[50]     2.64603095  0.56253266    1.54106012    2.26477167\n  expRating[51]     5.29498285  0.55871234    4.20796124    4.92034044\n  expRating[52]     1.22817790  0.51505491    0.32014311    0.86238063\n  expRating[53]     3.41498460  0.51074660    2.48490992    3.03868609\n  expRating[54]     2.01611170  0.52429420    1.04151733    1.64855977\n  expRating[55]     4.23333727  0.54606719    3.17346619    3.88493743\n  expRating[56]     1.97860321  0.55762802    0.87396047    1.60206444\n  expRating[57]     0.07279682  0.55076820   -1.04843042   -0.29054712\n  expRating[58]     4.15653425  0.58505923    3.02526277    3.76228377\n  expRating[59]     4.20132253  0.59447202    3.09301074    3.79304999\n  expRating[60]     6.34334095  0.57116006    5.25357950    5.95638457\n  expRating[61]     7.60011274  0.53820351    6.47476039    7.27813056\n  expRating[62]     6.19406491  0.52283406    5.07503642    5.87631431\n  expRating[63]     7.30577456  0.54312759    6.16523111    6.97513312\n  expRating[64]     7.94322434  0.54696961    6.85999275    7.57030298\n  expRating[65]     9.22730534  0.55891818    8.08530038    8.86432427\n  expRating[66]     5.06676489  0.55778002    3.98789141    4.69222990\n  expRating[67]     2.06646355  0.52919319    1.04681319    1.72305748\n  expRating[68]     5.36803330  0.53962032    4.30022911    5.02137714\n  expRating[69]     4.22500888  0.54261461    3.15546745    3.86920031\n  expRating[70]     9.03852685  0.58278148    7.85450771    8.64632045\n  expRating[71]     7.02103811  0.57762667    5.89002837    6.61876441\n  expRating[72]    10.20989221  0.56895086    9.08715192    9.82055597\n  expRating[73]     2.85969026  0.54419494    1.87059106    2.47495981\n  expRating[74]     2.56535207  0.53538337    1.52758563    2.19146165\n  expRating[75]     3.59566085  0.52855914    2.63105793    3.23070455\n  expRating[76]     7.55170343  0.54622144    6.47603767    7.18869816\n  expRating[77]     5.29696936  0.56548475    4.21985736    4.91108610\n  expRating[78]     6.40867901  0.55542508    5.32260384    6.01375003\n  expRating[79]     0.91490912  0.57152055   -0.22618657    0.50145141\n  expRating[80]     4.21647886  0.58342618    3.09095431    3.81276153\n  expRating[81]     4.10376321  0.58191451    2.96612874    3.70531888\n  expRating[82]     2.89158302  0.54261992    1.86958609    2.51426196\n  expRating[83]     2.59724484  0.54600630    1.54079018    2.20465288\n  expRating[84]     2.22868071  0.53487381    1.22218200    1.88612670\n  expRating[85]     4.37194570  0.59736802    3.15375655    3.96293183\n  expRating[86]     6.55875240  0.58698182    5.39014889    6.16030770\n  expRating[87]     5.15987950  0.61658236    3.94014932    4.73564203\n  expRating[88]     8.68982438  0.55186697    7.61329035    8.31088410\n  expRating[89]     7.71917131  0.55074906    6.66879263    7.33675156\n  expRating[90]     9.86118973  0.56672809    8.79465792    9.44282531\n  expRating[91]    10.37559114  0.55039283    9.27298219   10.00797523\n  expRating[92]    10.26287549  0.52826646    9.19645981    9.91779946\n  expRating[93]     8.86400259  0.53221451    7.84268680    8.50105253\n  expRating[94]     0.66258751  0.51581168   -0.28043245    0.31093216\n  expRating[95]     1.70942318  0.53661275    0.74513063    1.31097461\n  expRating[96]     2.45256870  0.53551055    1.49932580    2.05939590\n  expRating[97]    10.62691277  0.51200355    9.57332710   10.29111723\n  expRating[98]     6.46637232  0.49426672    5.43472682    6.12455440\n  expRating[99]    10.51419713  0.50154318    9.50601372   10.17688741\n  expRating[100]    8.06100247  0.55921471    6.93837818    7.66828832\n  expRating[101]    7.76666429  0.55356068    6.63430034    7.39986705\n  expRating[102]    7.39810016  0.56732639    6.31296713    7.02015684\n  expRating[103]    4.79961873  0.52766036    3.75937115    4.42188482\n  expRating[104]    6.98642543  0.53949139    5.94736356    6.60616376\n  expRating[105]    5.58755252  0.54532743    4.54007546    5.20028594\n  expRating[106]    6.63281263  0.55790176    5.55647513    6.28299195\n  expRating[107]    5.22676479  0.58987985    4.01533045    4.84501313\n  expRating[108]    7.36878322  0.58769174    6.15837764    6.99205026\n  expRating[109]    4.31548367  0.56572858    3.09313822    3.96756829\n  expRating[110]    1.43902422  0.56628825    0.34546607    1.06662933\n  expRating[111]    5.48684903  0.55506309    4.39600285    5.12611217\n  expRating[112]    2.33094483  0.53372699    1.39908648    1.94313706\n  expRating[113]    1.31550349  0.52706464    0.33112118    0.93982648\n  expRating[114]   -0.54551462  0.53081051   -1.50406781   -0.91853174\n  expRating[115]    3.83382134  0.53808227    2.79148174    3.45488652\n  expRating[116]    1.53429899  0.53107109    0.51528895    1.15372928\n  expRating[117]    3.72110569  0.53835139    2.70206175    3.33636447\n  expRating[118]    2.68842105  0.49787135    1.78709517    2.33493076\n  expRating[119]    0.23553754  0.50228625   -0.64227446   -0.13847509\n  expRating[120]    3.42439164  0.50582776    2.47542040    3.07852659\n  expRating[121]    8.51296332  0.57887143    7.37932679    8.15468359\n  expRating[122]    6.06007981  0.55517118    4.98563990    5.70517611\n  expRating[123]    7.85006101  0.54382944    6.80845734    7.48890331\n  expRating[124]    7.43273096  0.54133552    6.43537741    7.04983279\n  expRating[125]    6.41728962  0.54413380    5.32637092    6.03536193\n  expRating[126]    8.60409632  0.53528001    7.52888936    8.24683497\n  expRating[127]    6.82829287  0.51790322    5.79621684    6.47880507\n  expRating[128]    8.98683819  0.51312220    7.97462563    8.65119992\n  expRating[129]    8.61827407  0.51911417    7.60950983    8.27429291\n  expRating[130]    6.02601965  0.57140377    4.90072790    5.65244128\n  expRating[131]    9.21487375  0.57110014    8.14264360    8.82602301\n  expRating[132]    7.81600085  0.56988738    6.67314742    7.43171863\n  expRating[133]    7.49847234  0.57590847    6.34843770    7.09408849\n  expRating[134]    5.19895000  0.56662231    4.08095734    4.81325700\n  expRating[135]    6.35544792  0.56576679    5.22946912    5.97904891\n  expRating[136]    2.05731918  0.56330286    0.92504978    1.66742671\n  expRating[137]    2.10210746  0.56775357    0.98891514    1.68506098\n  expRating[138]    0.19630107  0.56282215   -0.97847537   -0.14744740\n  expRating[139]    9.51494249  0.47685370    8.53854364    9.19360788\n  expRating[140]    8.49950115  0.49091814    7.44345144    8.17327021\n  expRating[141]    9.65599908  0.47944611    8.64686755    9.34724105\n  expRating[142]    7.19724661  0.56408977    6.05156514    6.80893261\n  expRating[143]    9.38405331  0.57040657    8.23313004    9.00271743\n  expRating[144]    7.98518040  0.56826792    6.86268308    7.57855844\n  expRating[145]    4.48567595  0.54097770    3.41709592    4.10696596\n  expRating[146]    0.32513550  0.53438856   -0.70379363   -0.02171380\n  expRating[147]    2.97408740  0.52853292    1.91103681    2.63914949\n  expRating[148]    4.94660532  0.55265360    3.84567563    4.58441395\n  expRating[149]    3.08558721  0.56370641    1.96165151    2.71085334\n  expRating[150]    7.13341202  0.55143344    6.04556756    6.79706895\n  expRating[151]    1.92333450  0.47537612    1.10653629    1.58938946\n  expRating[152]    2.35872927  0.46594464    1.54711057    2.01143246\n  expRating[153]    3.09469986  0.45683421    2.26279625    2.78367956\n  expRating[154]    0.93267738  0.54827006   -0.11468457    0.58335518\n  expRating[155]    1.97951306  0.56215304    0.96133354    1.56962368\n  expRating[156]    4.12153148  0.55129011    3.14079854    3.72063346\n  expRating[157]    5.91573226  0.53142277    4.85065292    5.56188864\n  expRating[158]    4.05471415  0.53720906    2.98417063    3.68822165\n  expRating[159]    8.10253896  0.54076644    7.06228565    7.74715993\n  expRating[160]    3.98210879  0.53321629    2.97746853    3.61584204\n  expRating[161]    1.52922528  0.52630771    0.45058631    1.18314113\n  expRating[162]    2.53127268  0.52481229    1.53754445    2.16067133\n  expRating[163]    3.69383143  0.55742671    2.63419645    3.30522690\n  expRating[164]    2.67839009  0.55890467    1.60201422    2.30504458\n  expRating[165]    4.86519679  0.56414080    3.80244110    4.47015024\n  expRating[166]    3.87981264  0.56005577    2.84349597    3.47606383\n  expRating[167]   -0.28072781  0.57631898   -1.36874595   -0.67603625\n  expRating[168]    2.73678822  0.55702451    1.70481662    2.36252959\n  expRating[169]    3.20133504  0.53985116    2.14247577    2.84394966\n  expRating[170]    2.34236433  0.55453971    1.24010821    2.00282788\n  expRating[171]    4.99131623  0.54832207    3.90563800    4.62448279\n  expRating[172]    7.11383983  0.53209290    6.00141288    6.77651837\n  expRating[173]    8.11588723  0.52635024    7.00937126    7.76418817\n  expRating[174]    8.90382102  0.54091520    7.83758215    8.52956838\n  expRating[175]    3.64310063  0.57471549    2.52770336    3.26373736\n  expRating[176]    1.73729425  0.57493915    0.57881436    1.35448830\n  expRating[177]    4.38624615  0.55925947    3.28333252    4.02694102\n  expRating[178]    6.46631704  0.53016132    5.41539380    6.10375355\n  expRating[179]    9.76788678  0.54480678    8.62077631    9.41740473\n  expRating[180]    9.65517114  0.55431045    8.54436326    9.28797488\n  expRating[181]    2.75078895  0.57271786    1.65403128    2.36803912\n  expRating[182]    1.78013589  0.56784111    0.62072162    1.40779821\n  expRating[183]    2.89184554  0.56266499    1.78843879    2.51544204\n  expRating[184]    9.44412306  0.48755060    8.43461683    9.12718644\n  expRating[185]    9.87951783  0.50038528    8.79818386    9.55461019\n  expRating[186]    9.58517965  0.47846135    8.55114729    9.28568089\n  expRating[187]   10.62709438  0.50290923    9.58104439   10.29460424\n  expRating[188]    8.32757204  0.49486713    7.29168175    8.00655739\n  expRating[189]   10.51437874  0.49905534    9.47108099   10.17545388\n  expRating[190]    2.40110662  0.51834800    1.43379933    2.04202695\n  expRating[191]    1.43045356  0.53851018    0.41435045    1.07077908\n  expRating[192]    3.57247198  0.51351941    2.62670088    3.21260352\n  expRating[193]    6.91197449  0.54618573    5.89628730    6.51517068\n  expRating[194]    7.34736926  0.55184378    6.30517738    6.96350545\n  expRating[195]    8.19605549  0.55421387    7.17351022    7.82051584\n  expRating[196]    3.82713396  0.59376412    2.64263225    3.40694142\n  expRating[197]    2.81169262  0.59093100    1.63025351    2.40648057\n  expRating[198]    0.95067452  0.59121618   -0.24572811    0.57129720\n  expRating[199]    0.79239363  0.54056367   -0.21266139    0.41785707\n  expRating[200]    4.09396338  0.54460206    3.09119895    3.71971152\n  expRating[201]   -0.06657708  0.55081597   -1.09105750   -0.45632531\n  expRating[202]    3.76723773  0.53820684    2.74394610    3.38876344\n  expRating[203]    4.61592396  0.55453750    3.51250012    4.24289817\n  expRating[204]    0.45538351  0.54610790   -0.61662015    0.05603097\n  expRating[205]    9.57946186  0.47016772    8.63471882    9.27014539\n  expRating[206]   10.01485663  0.46062236    9.07719956    9.72505815\n  expRating[207]    9.72051845  0.45311989    8.77360805    9.41399714\n  expRating[208]    7.08752341  0.59056362    5.99590325    6.67026617\n  expRating[209]    4.83278934  0.57543127    3.74936155    4.44608331\n  expRating[210]    5.57593486  0.57714593    4.50175693    5.18497760\n  expRating[211]    5.03792545  0.55615440    3.95272757    4.66376989\n  expRating[212]    5.47332023  0.56082812    4.41376045    5.08011398\n  expRating[213]    3.02043672  0.56913597    1.91689136    2.62077816\n  expRating[214]    4.95033607  0.53698288    3.92431110    4.57285607\n  expRating[215]    5.95238347  0.54033629    4.84960433    5.60333323\n  expRating[216]    6.74031726  0.53124985    5.71965031    6.38596449\n  expRating[217]    5.04431377  0.54522195    3.90914618    4.69672081\n  expRating[218]    2.74479143  0.54684017    1.67833425    2.38047082\n  expRating[219]    3.53272523  0.55191350    2.40581193    3.18682202\n  expRating[220]    8.38452618  0.56769104    7.27467857    7.99313914\n  expRating[221]    5.07267196  0.58337527    3.97057078    4.68456236\n  expRating[222]    7.72162386  0.57665931    6.63553867    7.32196672\n  expRating[223]    9.44042594  0.50790230    8.43811000    9.10134190\n  expRating[224]    6.98754243  0.51202373    5.96327623    6.64859162\n  expRating[225]    9.14608776  0.49702752    8.16462473    8.82033843\n  expRating[226]    5.31225842  0.55919873    4.22650211    4.93614778\n  expRating[227]    3.29476968  0.59345006    2.08330662    2.90195769\n  expRating[228]    4.29681708  0.58198408    3.18742851    3.90665632\n  expRating[229]    1.87531772  0.58994124    0.76910821    1.47968427\n  expRating[230]    2.87736512  0.57920153    1.70121973    2.46894993\n  expRating[231]    1.01634701  0.57045467   -0.07657344    0.64173393\n  expRating[232]    4.67586612  0.54772303    3.59383681    4.32545851\n  expRating[233]    5.95994713  0.54184122    4.91571073    5.58028201\n  expRating[234]    1.79940667  0.55478635    0.69024552    1.44068328\n  expRating[235]    7.09654696  0.58079058    5.84618827    6.71184861\n  expRating[236]    5.07905822  0.58537823    3.91573394    4.70247690\n  expRating[237]    6.08110562  0.58243015    4.93527253    5.66522289\n  expRating[238]    2.65029843  0.59102085    1.52357181    2.26298710\n  expRating[239]    4.80884376  0.59697725    3.66525669    4.41454825\n  expRating[240]    4.44027963  0.59190524    3.24102995    4.05306158\n  expRating[241]    4.71299004  0.55779319    3.55205336    4.36331386\n  expRating[242]    3.85401933  0.55601206    2.74971287    3.49635963\n  expRating[243]    6.87153536  0.55742980    5.73381609    6.49427783\n  expRating[244]    3.38331727  0.55988063    2.34690972    3.00286926\n  expRating[245]    2.36787593  0.56679173    1.31858287    1.95092953\n  expRating[246]    3.15580973  0.57536399    2.11813519    2.74746157\n  expRating[247]    5.62103697  0.58321619    4.50098556    5.21885588\n  expRating[248]    4.65038391  0.59707875    3.49176544    4.24880935\n  expRating[249]    5.39352943  0.58409957    4.22871051    4.98471265\n  expRating[250]    1.92312969  0.45516517    1.14683229    1.60064223\n  expRating[251]    2.35852446  0.44575078    1.57560202    2.03337452\n  expRating[252]   -0.09435905  0.46048874   -0.87236223   -0.42723071\n  expRating[253]    3.10845973  0.54888188    2.03530123    2.74114379\n  expRating[254]    6.12597577  0.53947576    5.04864336    5.77670796\n  expRating[255]    5.75741164  0.54387121    4.67881213    5.37536295\n  expRating[256]    6.49792507  0.55804760    5.38138984    6.14251105\n  expRating[257]    5.63895436  0.55738487    4.55215323    5.25851783\n  expRating[258]    8.28790626  0.56057066    7.20444356    7.90645345\n  expRating[259]    2.67330899  0.49667586    1.73029887    2.32773420\n  expRating[260]   -0.20315046  0.50159711   -1.15427406   -0.53927868\n  expRating[261]    2.44580144  0.51564283    1.51682617    2.08839664\n  expRating[262]    7.79506267  0.53323192    6.74901208    7.41440528\n  expRating[263]    7.83985094  0.53129641    6.77123400    7.48440662\n  expRating[264]    5.93404456  0.52666399    4.87572397    5.57676018\n  expRating[265]    5.95006931  0.54875361    4.88208530    5.59040635\n  expRating[266]    4.04426292  0.55038880    2.92845138    3.68548162\n  expRating[267]    6.69321483  0.55751299    5.64712786    6.31622978\n  expRating[268]    7.74043520  0.53772034    6.69944094    7.37175197\n  expRating[269]    5.87941709  0.56273937    4.72300538    5.50577428\n  expRating[270]    8.89693313  0.54690662    7.76731534    8.54472461\n  expRating[271]    3.91272247  0.56858741    2.85631725    3.50943561\n  expRating[272]    2.46188636  0.57378394    1.37266748    2.07359885\n  expRating[273]    2.50667464  0.56863845    1.40672603    2.13371555\n  expRating[274]    3.69144946  0.55980603    2.57790472    3.30346987\n  expRating[275]    4.12684424  0.54506610    3.03649058    3.77417616\n  expRating[276]    3.83250605  0.57038931    2.66206611    3.44581281\n  expRating[277]    4.76182020  0.56278172    3.70294539    4.36572600\n  expRating[278]    5.61050643  0.55806413    4.57533671    5.22708715\n  expRating[279]    4.46748201  0.56588819    3.38777954    4.06419298\n  expRating[280]    3.14765176  0.55489151    1.93937068    2.80474571\n  expRating[281]    4.43173276  0.56518252    3.35422185    4.04996673\n  expRating[282]    2.17699869  0.56025561    1.01980693    1.82506991\n  expRating[283]    8.50172065  0.56931643    7.42773869    8.15459073\n  expRating[284]    9.35040688  0.58787231    8.24593368    8.98087202\n  expRating[285]    8.20738246  0.58327877    7.08148454    7.84231733\n  expRating[286]    5.09768177  0.53842800    4.05175067    4.73515011\n  expRating[287]    3.69163394  0.52447406    2.62893671    3.33509714\n  expRating[288]    4.80334359  0.52450123    3.79422158    4.42972861\n  expRating[289]    6.22159218  0.56671682    5.11876264    5.84880678\n  expRating[290]    4.81554435  0.57953852    3.67668061    4.42986351\n  expRating[291]    5.92725400  0.57180269    4.86531514    5.53555694\n  expRating[292]    7.23574294  0.56101816    6.10823447    6.85233386\n  expRating[293]    5.82969511  0.55620028    4.75871590    5.45222333\n  expRating[294]    6.94140476  0.55215020    5.83210827    6.60350572\n  expRating[295]    5.17138585  0.55405842    4.07596056    4.80737510\n  expRating[296]    3.76533801  0.55703465    2.68087779    3.39404485\n  expRating[297]    4.87704766  0.56050271    3.78122383    4.49974242\n  expRating[298]    4.26270671  0.61032812    3.07374013    3.84329023\n  expRating[299]    5.30954239  0.59672171    4.13589991    4.89970690\n  expRating[300]    7.45156081  0.59828742    6.23307580    7.06128692\n  lp__           -100.46052184 10.51690910 -123.04446635 -107.38195923\n                stats\nparameter                 50%         75%       97.5%\n  ability[1]       5.63725774   6.0193625   6.8151438\n  ability[2]       8.01246628   8.3780798   9.1032035\n  ability[3]       6.82276227   7.2298098   7.9662654\n  ability[4]       7.11002568   7.4766964   8.1994814\n  ability[5]       3.10990382   3.4964667   4.2368271\n  ability[6]       4.60129948   4.9675601   5.6291911\n  ability[7]       8.97063283   9.3347928   9.8683804\n  ability[8]       4.24684677   4.6126859   5.4167942\n  ability[9]       2.87549956   3.2996053   4.0503524\n  ability[10]      8.80184536   9.1809054   9.6921937\n  ability[11]      5.72522217   6.0829918   6.7691520\n  ability[12]      6.69377203   7.0799123   7.7865659\n  ability[13]      1.67134613   1.9953527   2.6916059\n  ability[14]      9.38122934   9.6609696   9.9258954\n  ability[15]      5.85426044   6.2458528   7.1123030\n  ability[16]      6.68492636   7.0892876   7.8971298\n  ability[17]      5.24620270   5.6540572   6.3300410\n  ability[18]      1.95302952   2.2937154   3.0112342\n  ability[19]      2.67298205   3.0850376   3.8332574\n  ability[20]      4.89759885   5.2944429   6.0790805\n  ability[21]      6.89823086   7.2631785   7.9475943\n  ability[22]      7.66821455   8.0668001   8.8037921\n  ability[23]      3.83452197   4.1901329   4.7939679\n  ability[24]      8.79552690   9.1761032   9.8123024\n  ability[25]      2.12148830   2.5224076   3.2235311\n  ability[26]      5.95019850   6.4089953   7.0424176\n  ability[27]      2.67166944   3.0697194   3.8308679\n  ability[28]      2.16732309   2.5498939   3.2770924\n  ability[29]      5.10794369   5.5279856   6.3112390\n  ability[30]      8.44530793   8.8006623   9.5358068\n  ability[31]      8.82164084   9.1967706   9.8227334\n  ability[32]      2.40152841   2.7905971   3.5156703\n  ability[33]      9.08788994   9.4510829   9.9425056\n  ability[34]      7.36309365   7.7455507   8.4162687\n  ability[35]      5.55626202   5.9061725   6.6004505\n  ability[36]      5.93261737   6.2964016   7.0468952\n  ability[37]      4.05851175   4.3983256   5.1513722\n  ability[38]      2.06228122   2.3929238   3.1802212\n  ability[39]      2.26575881   2.6341351   3.3738205\n  ability[40]      1.94657517   2.3312797   3.0097176\n  ability[41]      7.79757524   8.1919917   8.9098755\n  ability[42]      7.15302068   7.5569216   8.1769012\n  ability[43]      8.57464661   8.9384010   9.5753474\n  ability[44]      7.77187307   8.1521405   9.0698655\n  ability[45]      5.94584049   6.3252987   7.0486283\n  ability[46]      2.82999219   3.2033500   3.9120843\n  ability[47]      9.27196617   9.6125163   9.9738338\n  ability[48]      7.95432814   8.3373843   9.0580476\n  ability[49]      2.93603978   3.3312044   3.9230401\n  ability[50]      5.67130353   6.0748941   6.7994050\n  ability[51]      1.58400697   1.9082115   2.5680190\n  ability[52]      2.68250675   3.0596515   3.8136503\n  ability[53]      6.65176619   7.0016778   7.7623625\n  ability[54]      3.24994807   3.6253722   4.3644062\n  ability[55]      3.41132230   3.8082732   4.5552630\n  ability[56]      2.31754573   2.6863466   3.4744612\n  ability[57]      4.94270268   5.3009492   6.1139326\n  ability[58]      8.88953947   9.2553492   9.8612790\n  ability[59]      4.34096354   4.7361930   5.4834136\n  ability[60]      8.22359167   8.5858260   9.3329666\n  ability[61]      2.48605640   2.8525757   3.6345618\n  ability[62]      9.23291193   9.5245069   9.9387175\n  ability[63]      9.10271968   9.4371156   9.9150931\n  ability[64]      2.11710615   2.4903005   3.1355275\n  ability[65]      6.64104922   7.0571572   7.7577426\n  ability[66]      3.55764194   3.9804677   4.7663914\n  ability[67]      2.51482874   2.9350255   3.6836918\n  ability[68]      3.05997976   3.4758628   4.0784908\n  ability[69]      9.34877718   9.6649868   9.9600063\n  ability[70]      5.54562673   5.9384568   6.7528288\n  ability[71]      4.74121283   5.1633627   5.9153947\n  ability[72]      6.68730645   7.0736373   7.8057511\n  ability[73]      3.49373196   3.8356692   4.6352517\n  ability[74]      7.67801028   8.0659541   8.8248374\n  ability[75]      8.72467346   9.0967481   9.7510216\n  ability[76]      5.02729600   5.4600061   6.1572851\n  ability[77]      3.62845969   4.0087392   4.8276048\n  ability[78]      4.39750055   4.7721000   5.5140190\n  ability[79]      6.79648560   7.2477000   8.0618508\n  ability[80]      4.39857888   4.8123620   5.5984382\n  ability[81]      6.45977230   6.8368915   7.5900748\n  ability[82]      3.09527929   3.4984118   4.2531841\n  ability[83]      5.36604268   5.7483780   6.5606310\n  ability[84]      1.58971315   1.9170655   2.6042853\n  ability[85]      5.71715496   6.0700495   6.8210361\n  ability[86]      8.25111001   8.6358421   9.3889089\n  ability[87]      2.40249181   2.7171080   3.4269741\n  ability[88]      8.54026004   8.9224639   9.4943694\n  ability[89]      6.64264341   7.0369019   7.7922306\n  ability[90]      8.50709877   8.8579801   9.5611017\n  ability[91]      3.19402389   3.6149908   4.2783541\n  ability[92]      3.41616507   3.7983150   4.4840212\n  ability[93]      4.05126378   4.4364515   5.1738477\n  ability[94]      2.88263833   3.2637104   3.9181444\n  ability[95]      7.76254569   8.1623863   9.1528635\n  ability[96]      4.37514418   4.7650248   5.4287670\n  ability[97]      5.48445419   5.9187052   6.7356692\n  ability[98]      6.52272801   6.9203690   7.6820810\n  ability[99]      4.47258344   4.8566255   5.5853748\n  ability[100]     6.00683694   6.4023247   7.2597594\n  severity[1]      0.27240897   0.4689420   0.7884538\n  severity[2]      0.71129190   0.8729481   1.2129861\n  severity[3]     -1.75008810  -1.5678745  -1.1997274\n  severity[4]      1.54473846   1.7462902   2.0803104\n  severity[5]     -0.74269409  -0.5459082  -0.2324750\n  severity[6]     -0.70554435  -0.4989483  -0.1741776\n  severity[7]     -2.59793381  -2.4216809  -2.0792222\n  severity[8]      0.42221492   0.5846944   0.8968677\n  severity[9]      1.45295685   1.6295017   1.9616449\n  severity[10]     0.04579202   0.2330086   0.5772168\n  sigma            0.93317753   0.9655136   1.0378893\n  tau              1.40244648   1.7062985   2.5676774\n  expRating[1]     3.89341025   4.2602915   5.0212803\n  expRating[2]     7.18574547   7.5684272   8.3657696\n  expRating[3]     4.93445798   5.3174392   6.0748281\n  expRating[4]     8.30003586   8.6371480   9.2969781\n  expRating[5]     9.58025654   9.9430705  10.6822664\n  expRating[6]     7.34060806   7.6728006   8.3586757\n  expRating[7]     6.12330671   6.4977848   7.2149841\n  expRating[8]     7.23075927   7.6257903   8.3508042\n  expRating[9]     6.88063678   7.2582244   7.9855110\n  expRating[10]    7.82808205   8.1646471   8.8348556\n  expRating[11]    6.40408053   6.7760903   7.4635463\n  expRating[12]    4.50309608   4.8591553   5.6179887\n  expRating[13]    3.83771546   4.1792303   4.8701741\n  expRating[14]    4.69761934   5.0271957   5.7952502\n  expRating[15]    2.36629391   2.7456694   3.5123358\n  expRating[16]    6.13968455   6.5029349   7.1480553\n  expRating[17]    3.83237666   4.2321106   4.9311057\n  expRating[18]    4.64112536   5.0140119   5.7110037\n  expRating[19]    9.25203398   9.5861757  10.1026965\n  expRating[20]    7.22329921   7.5574846   8.0990075\n  expRating[21]    9.38783696   9.7363770  10.2867344\n  expRating[22]    2.52017002   2.8740233   3.6222530\n  expRating[23]    3.54693425   3.9293836   4.7547657\n  expRating[24]    5.69604357   6.0596119   6.8065922\n  expRating[25]    3.14574058   3.5330197   4.2818746\n  expRating[26]    1.13114768   1.5321737   2.2802752\n  expRating[27]    0.29678211   0.6753698   1.3168376\n  expRating[28]   10.35613432  10.7095356  11.2122378\n  expRating[29]    9.23881449   9.5812976  10.1545614\n  expRating[30]   10.24745382  10.6049953  11.1789902\n  expRating[31]    7.27864676   7.6287445   8.3632971\n  expRating[32]    7.15916105   7.5109079   8.2120053\n  expRating[33]    5.73736128   6.1215704   6.8035804\n  expRating[34]    7.40099541   7.8000699   8.5189624\n  expRating[35]    8.23358018   8.6490694   9.3175447\n  expRating[36]    5.96013337   6.3292447   7.0615011\n  expRating[37]    2.38008287   2.7017505   3.3286717\n  expRating[38]   -0.05959717   0.2546201   0.9050993\n  expRating[39]    2.10714792   2.4196301   3.1393304\n  expRating[40]    9.62065624   9.9124422  10.3846653\n  expRating[41]    8.66899590   8.9347722   9.3931636\n  expRating[42]    9.77337443  10.0430319  10.4977344\n  expRating[43]    6.13172997   6.5428359   7.3488642\n  expRating[44]    3.25371401   3.6272443   4.4503147\n  expRating[45]    7.28726117   7.7214173   8.4895425\n  expRating[46]    8.24335045   8.6145359   9.3861852\n  expRating[47]    6.00976257   6.3673025   7.0978160\n  expRating[48]    4.08911326   4.4842068   5.1543402\n  expRating[49]    5.97869367   6.3504935   7.0348547\n  expRating[50]    2.64623266   3.0346201   3.7427989\n  expRating[51]    5.28917280   5.6583280   6.3880899\n  expRating[52]    1.20481985   1.5704397   2.2685925\n  expRating[53]    3.39754269   3.7674105   4.4487460\n  expRating[54]    1.98766517   2.3388615   3.1254623\n  expRating[55]    4.23057591   4.6005549   5.3114645\n  expRating[56]    1.96942339   2.3361887   3.0549078\n  expRating[57]    0.08202506   0.4544374   1.1291081\n  expRating[58]    4.14054680   4.5456276   5.3890546\n  expRating[59]    4.18251134   4.5810021   5.3801458\n  expRating[60]    6.33496254   6.7280925   7.4654892\n  expRating[61]    7.61959242   7.9436349   8.5844969\n  expRating[62]    6.21153863   6.5278701   7.1942243\n  expRating[63]    7.32255816   7.6464100   8.3280760\n  expRating[64]    7.97248385   8.3115660   8.9480705\n  expRating[65]    9.23140704   9.6098080  10.2840442\n  expRating[66]    5.07072256   5.4367028   6.1551807\n  expRating[67]    2.06475985   2.4155711   3.0661671\n  expRating[68]    5.37421710   5.7384428   6.4281039\n  expRating[69]    4.22869881   4.5929977   5.3054078\n  expRating[70]    9.07066749   9.4329500  10.1343223\n  expRating[71]    7.03073672   7.4317538   8.0847507\n  expRating[72]   10.23350090  10.5988642  11.2757294\n  expRating[73]    2.84937312   3.2327953   3.9557009\n  expRating[74]    2.57112091   2.9351400   3.6173422\n  expRating[75]    3.56953898   3.9523067   4.6990692\n  expRating[76]    7.54635892   7.9316385   8.5967302\n  expRating[77]    5.29836107   5.6829428   6.3892116\n  expRating[78]    6.43259434   6.7967454   7.4609893\n  expRating[79]    0.90511812   1.3222323   2.0227875\n  expRating[80]    4.21624563   4.6000866   5.3400941\n  expRating[81]    4.10076552   4.4920728   5.2020407\n  expRating[82]    2.90011106   3.2385553   4.0189272\n  expRating[83]    2.60711639   2.9578328   3.7228170\n  expRating[84]    2.19615895   2.5555240   3.3451670\n  expRating[85]    4.38747270   4.7792422   5.5065873\n  expRating[86]    6.54682002   6.9575984   7.7070327\n  expRating[87]    5.17067457   5.5702003   6.3801419\n  expRating[88]    8.72130548   9.0626143   9.7347168\n  expRating[89]    7.72335654   8.0891727   8.8092224\n  expRating[90]    9.87617411  10.2596966  10.9141997\n  expRating[91]   10.36417812  10.7585180  11.4204657\n  expRating[92]   10.27408998  10.6320746  11.2474818\n  expRating[93]    8.87418158   9.2512630   9.8612383\n  expRating[94]    0.62653712   1.0081826   1.6921974\n  expRating[95]    1.70256879   2.0674212   2.7729695\n  expRating[96]    2.42480801   2.8336459   3.5228422\n  expRating[97]   10.66353518  10.9767484  11.5873574\n  expRating[98]    6.47083411   6.8245049   7.3693532\n  expRating[99]   10.54341768  10.8837371  11.3637757\n  expRating[100]   8.03955538   8.4688006   9.0915671\n  expRating[101]   7.76175819   8.1423442   8.8270629\n  expRating[102]   7.39993272   7.7818707   8.5138294\n  expRating[103]   4.82493441   5.1664941   5.7818338\n  expRating[104]   6.98119370   7.3683159   8.0678340\n  expRating[105]   5.58487438   5.9737516   6.6168633\n  expRating[106]   6.61705932   6.9951681   7.7542321\n  expRating[107]   5.22689413   5.6127669   6.3540982\n  expRating[108]   7.37278181   7.7534117   8.5183599\n  expRating[109]   4.33448011   4.6682539   5.4836590\n  expRating[110]   1.43036041   1.8121152   2.5495912\n  expRating[111]   5.49584459   5.8373847   6.5486171\n  expRating[112]   2.29770549   2.7006126   3.4055623\n  expRating[113]   1.29604413   1.6741011   2.3627659\n  expRating[114]  -0.57169887  -0.1975785   0.5449892\n  expRating[115]   3.84307764   4.1832711   4.8920910\n  expRating[116]   1.54526251   1.8766240   2.5946792\n  expRating[117]   3.72569176   4.0784520   4.7665776\n  expRating[118]   2.67697091   3.0070376   3.7188552\n  expRating[119]   0.21628637   0.5723829   1.2812038\n  expRating[120]   3.40800251   3.7545237   4.4688487\n  expRating[121]   8.50631727   8.8716453   9.7453967\n  expRating[122]   6.04950178   6.4188664   7.1654330\n  expRating[123]   7.83949359   8.1903413   8.9313278\n  expRating[124]   7.43187595   7.8128058   8.4768349\n  expRating[125]   6.43552581   6.8000429   7.4289515\n  expRating[126]   8.59969351   8.9786158   9.6087062\n  expRating[127]   6.83986509   7.1904598   7.8459239\n  expRating[128]   8.98887442   9.3339098  10.0271612\n  expRating[129]   8.63158481   8.9780274   9.5958115\n  expRating[130]   6.02712008   6.4024339   7.1461841\n  expRating[131]   9.21437210   9.5968968  10.3413989\n  expRating[132]   7.82545223   8.1794637   8.9580571\n  expRating[133]   7.51440489   7.8949940   8.5793199\n  expRating[134]   5.18488155   5.5784871   6.2830177\n  expRating[135]   6.35810417   6.7449163   7.4419879\n  expRating[136]   2.06496668   2.4397512   3.1474556\n  expRating[137]   2.11056910   2.4974877   3.1646047\n  expRating[138]   0.22182711   0.5693082   1.3033377\n  expRating[139]   9.53981747   9.8408400  10.3627324\n  expRating[140]   8.52440642   8.8619004   9.3582819\n  expRating[141]   9.69110649  10.0004916  10.4558175\n  expRating[142]   7.20775945   7.5850648   8.2575572\n  expRating[143]   9.40571355   9.7833647  10.3939173\n  expRating[144]   8.01449243   8.3815644   9.0236201\n  expRating[145]   4.50267906   4.8578863   5.5300139\n  expRating[146]   0.33425829   0.7010672   1.3885439\n  expRating[147]   2.95800709   3.3568345   3.9897154\n  expRating[148]   4.92659131   5.2922612   6.0274332\n  expRating[149]   3.10737012   3.4313759   4.1689325\n  expRating[150]   7.11148688   7.4938870   8.2450615\n  expRating[151]   1.89494175   2.2341860   2.9623296\n  expRating[152]   2.32197886   2.6761991   3.3169234\n  expRating[153]   3.06857000   3.3939297   4.0251311\n  expRating[154]   0.92029321   1.2893413   2.0098568\n  expRating[155]   1.96483390   2.3751403   3.0783033\n  expRating[156]   4.09428574   4.4981148   5.2442233\n  expRating[157]   5.91607467   6.2649570   6.9188329\n  expRating[158]   4.07139805   4.4282111   5.1284417\n  expRating[159]   8.09837383   8.4630684   9.1426609\n  expRating[160]   3.97570042   4.3376618   5.0332800\n  expRating[161]   1.51349364   1.9038401   2.5437414\n  expRating[162]   2.51290955   2.8764487   3.5689461\n  expRating[163]   3.70451683   4.0638924   4.8237238\n  expRating[164]   2.68156710   3.0380063   3.8044638\n  expRating[165]   4.83931408   5.2671296   6.0090784\n  expRating[166]   3.88303833   4.2460563   4.9677914\n  expRating[167]  -0.27389777   0.0886353   0.8430581\n  expRating[168]   2.72742749   3.0987856   3.9108926\n  expRating[169]   3.17993685   3.5555113   4.2689563\n  expRating[170]   2.33826640   2.7066887   3.4705422\n  expRating[171]   4.98802194   5.3638489   6.0224953\n  expRating[172]   7.12834404   7.4842170   8.1146714\n  expRating[173]   8.12391276   8.4838124   9.0905239\n  expRating[174]   8.92737340   9.2931500   9.8841366\n  expRating[175]   3.64804922   4.0193242   4.8416119\n  expRating[176]   1.73790567   2.0970034   2.9405899\n  expRating[177]   4.37983582   4.7540851   5.4958780\n  expRating[178]   6.47757767   6.8229721   7.5136656\n  expRating[179]   9.77495164  10.1386987  10.8404211\n  expRating[180]   9.64441843  10.0231900  10.7672306\n  expRating[181]   2.74664540   3.1495684   3.8368234\n  expRating[182]   1.78910015   2.1559228   2.9074201\n  expRating[183]   2.93080135   3.2509801   3.9866315\n  expRating[184]   9.46731288   9.7882637  10.3066267\n  expRating[185]   9.88643817  10.2497374  10.7416217\n  expRating[186]   9.60512650   9.9078498  10.4549712\n  expRating[187]  10.63787909  10.9707305  11.5483485\n  expRating[188]   8.33553570   8.6639377   9.2079075\n  expRating[189]  10.53733831  10.8653240  11.4597699\n  expRating[190]   2.38187793   2.7532780   3.4599800\n  expRating[191]   1.41294161   1.7793485   2.5403570\n  expRating[192]   3.54575320   3.9176606   4.5964134\n  expRating[193]   6.89772117   7.2876374   7.9831883\n  expRating[194]   7.34120313   7.7391965   8.4165188\n  expRating[195]   8.20324157   8.5693262   9.2628128\n  expRating[196]   3.83339548   4.2256617   4.9584994\n  expRating[197]   2.82233901   3.2249829   3.9734961\n  expRating[198]   0.96296356   1.3674398   2.0573885\n  expRating[199]   0.78822404   1.1517325   1.8980120\n  expRating[200]   4.08278466   4.4496459   5.2194427\n  expRating[201]  -0.08380597   0.3195276   0.9844473\n  expRating[202]   3.76259617   4.1471672   4.7495630\n  expRating[203]   4.60803282   5.0009796   5.7171531\n  expRating[204]   0.45210973   0.8265731   1.5637120\n  expRating[205]   9.61585786   9.9272039  10.4010472\n  expRating[206]  10.04825906  10.3573066  10.7894646\n  expRating[207]   9.76359682  10.0581240  10.4844793\n  expRating[208]   7.08209322   7.4797694   8.2729013\n  expRating[209]   4.83487247   5.2218373   5.9740167\n  expRating[210]   5.57894685   5.9652436   6.7482907\n  expRating[211]   5.03101130   5.4251522   6.1520306\n  expRating[212]   5.45051461   5.8667878   6.5914535\n  expRating[213]   3.01300574   3.4357188   4.0714444\n  expRating[214]   4.94435046   5.3029754   6.0243520\n  expRating[215]   5.93579990   6.3236234   7.0355518\n  expRating[216]   6.75660079   7.1121126   7.7662463\n  expRating[217]   5.05118635   5.3978841   6.1336394\n  expRating[218]   2.76269816   3.1073581   3.7963757\n  expRating[219]   3.52341272   3.8937442   4.6145991\n  expRating[220]   8.40221538   8.7595851   9.5290431\n  expRating[221]   5.08636941   5.4544197   6.2441768\n  expRating[222]   7.73316153   8.1016231   8.8513207\n  expRating[223]   9.44726748   9.8083422  10.3483627\n  expRating[224]   6.99487644   7.3241615   7.9923789\n  expRating[225]   9.13750666   9.4894095  10.0935144\n  expRating[226]   5.27805592   5.6992529   6.3692719\n  expRating[227]   3.29703893   3.7116455   4.4291506\n  expRating[228]   4.30300001   4.7118341   5.4073859\n  expRating[229]   1.90280042   2.2871551   3.0390068\n  expRating[230]   2.88366926   3.3013010   3.9926578\n  expRating[231]   1.02253191   1.4040187   2.1247345\n  expRating[232]   4.69777281   5.0383659   5.7751780\n  expRating[233]   5.96951570   6.3207527   6.9842040\n  expRating[234]   1.81189106   2.1496089   2.9325970\n  expRating[235]   7.10119216   7.4954788   8.1384492\n  expRating[236]   5.08769445   5.4827507   6.1677635\n  expRating[237]   6.08405731   6.4851040   7.2135534\n  expRating[238]   2.65036023   3.0593017   3.8298444\n  expRating[239]   4.79996585   5.2223181   6.0074068\n  expRating[240]   4.43691937   4.8598502   5.5741931\n  expRating[241]   4.71634544   5.0485744   5.8609690\n  expRating[242]   3.85628386   4.2318624   4.9204393\n  expRating[243]   6.86341635   7.2417250   7.9771940\n  expRating[244]   3.37523349   3.7481255   4.4715040\n  expRating[245]   2.37108154   2.7575992   3.4518992\n  expRating[246]   3.14841532   3.5645998   4.2572884\n  expRating[247]   5.62200823   6.0292269   6.7711334\n  expRating[248]   4.64861996   5.0177004   5.8293929\n  expRating[249]   5.39449728   5.7906262   6.5130687\n  expRating[250]   1.87306739   2.2265653   2.8869903\n  expRating[251]   2.32845249   2.6515142   3.2853378\n  expRating[252]  -0.14170672   0.1975244   0.9138608\n  expRating[253]   3.12528097   3.4572029   4.1721376\n  expRating[254]   6.14170598   6.4659509   7.1626721\n  expRating[255]   5.76880562   6.1249614   6.7868931\n  expRating[256]   6.49438106   6.8701601   7.5941046\n  expRating[257]   5.63912284   6.0278602   6.7189882\n  expRating[258]   8.26980432   8.6799023   9.3276052\n  expRating[259]   2.65349470   3.0079654   3.6789771\n  expRating[260]  -0.20120174   0.1070162   0.8166664\n  expRating[261]   2.43848479   2.7935306   3.4991620\n  expRating[262]   7.82228868   8.1841666   8.7759247\n  expRating[263]   7.84402093   8.2107938   8.8662027\n  expRating[264]   5.93710264   6.3157655   6.9261853\n  expRating[265]   5.95523910   6.2924439   6.9885156\n  expRating[266]   4.03138038   4.4383988   5.1046721\n  expRating[267]   6.68931869   7.0698154   7.7528083\n  expRating[268]   7.74262551   8.1154329   8.8361925\n  expRating[269]   5.88318854   6.2902248   6.9509216\n  expRating[270]   8.90168403   9.2632717   9.9325936\n  expRating[271]   3.91087720   4.3318966   4.9755836\n  expRating[272]   2.45961342   2.8568383   3.5648472\n  expRating[273]   2.50429636   2.9008859   3.5994266\n  expRating[274]   3.69108660   4.0886258   4.7368965\n  expRating[275]   4.13202977   4.4965974   5.1971841\n  expRating[276]   3.85444717   4.2360595   4.8691030\n  expRating[277]   4.75923332   5.1412624   5.9406245\n  expRating[278]   5.60186189   5.9797322   6.7473803\n  expRating[279]   4.45156652   4.8561116   5.5731307\n  expRating[280]   3.15552963   3.5343734   4.2000236\n  expRating[281]   4.43097901   4.8156885   5.5172073\n  expRating[282]   2.18888446   2.5483666   3.2535824\n  expRating[283]   8.47447002   8.8247876   9.7239444\n  expRating[284]   9.30918547   9.6972123  10.6680464\n  expRating[285]   8.18193203   8.5534841   9.4468699\n  expRating[286]   5.10690340   5.4626549   6.1186800\n  expRating[287]   3.70194966   4.0697097   4.7056119\n  expRating[288]   4.81165123   5.1795153   5.8479316\n  expRating[289]   6.20649751   6.5927260   7.3293941\n  expRating[290]   4.79329992   5.1929776   5.9914284\n  expRating[291]   5.91210632   6.2940587   7.1168031\n  expRating[292]   7.23559727   7.6362354   8.3417746\n  expRating[293]   5.84726381   6.1984566   6.9488871\n  expRating[294]   6.93362890   7.3004093   8.0217380\n  expRating[295]   5.17034333   5.5331763   6.2349234\n  expRating[296]   3.76678008   4.1239497   4.8239492\n  expRating[297]   4.87944045   5.2636526   5.9661927\n  expRating[298]   4.24757549   4.6772116   5.4319586\n  expRating[299]   5.30902984   5.6846909   6.5058742\n  expRating[300]   7.44593785   7.8121459   8.6264710\n  lp__           -99.61667360 -93.0156110 -82.2872041\n\n, , chains = chain:3\n\n                stats\nparameter                 mean         sd          2.5%           25%\n  ability[1]        5.62957421 0.56173219    4.51827385    5.24112446\n  ability[2]        8.04637950 0.58623178    6.84808950    7.65706559\n  ability[3]        6.82505945 0.62129251    5.59729404    6.41613975\n  ability[4]        7.12005904 0.60992800    5.89025477    6.71529937\n  ability[5]        3.13651685 0.57137155    1.98135237    2.75703885\n  ability[6]        4.57947835 0.59453230    3.44500467    4.15472982\n  ability[7]        8.94403726 0.53688052    7.84633716    8.56103427\n  ability[8]        4.23563879 0.58290164    3.12612426    3.86256476\n  ability[9]        2.86197595 0.60126011    1.72688717    2.42002957\n  ability[10]       8.82089310 0.56793011    7.59700732    8.44530513\n  ability[11]       5.71871872 0.60228733    4.55013284    5.32264412\n  ability[12]       6.69755128 0.61568357    5.45720202    6.27242779\n  ability[13]       1.68919745 0.43753497    1.05490045    1.33328666\n  ability[14]       9.33003521 0.42700712    8.35770188    9.05898774\n  ability[15]       5.82657846 0.57726525    4.71777089    5.41857822\n  ability[16]       6.67173671 0.55391549    5.60069732    6.30732644\n  ability[17]       5.24996210 0.59280960    4.09136939    4.88196983\n  ability[18]       1.93958025 0.53105487    1.09485827    1.51010538\n  ability[19]       2.65502386 0.59248499    1.45885247    2.24471127\n  ability[20]       4.85279380 0.61571822    3.63488864    4.49412164\n  ability[21]       6.91148639 0.58992237    5.78174846    6.52104384\n  ability[22]       7.67459468 0.61231354    6.49684646    7.26760027\n  ability[23]       3.82112542 0.58362726    2.74547889    3.42589854\n  ability[24]       8.73005991 0.56156799    7.51241116    8.38843263\n  ability[25]       2.11285433 0.56161606    1.08666985    1.70364645\n  ability[26]       5.96089871 0.58195989    4.83762432    5.58260467\n  ability[27]       2.64628178 0.59529854    1.50944258    2.25024785\n  ability[28]       2.15626526 0.57100404    1.13751986    1.74426347\n  ability[29]       5.08459000 0.56627801    3.99316285    4.71724840\n  ability[30]       8.39150410 0.57266828    7.31896295    7.98839211\n  ability[31]       8.82722830 0.56015952    7.70110490    8.45855765\n  ability[32]       2.38417960 0.57498334    1.29300174    1.96875765\n  ability[33]       9.04710321 0.52276458    7.95969026    8.69408435\n  ability[34]       7.33721933 0.59341719    6.22444238    6.94011937\n  ability[35]       5.52458208 0.55542463    4.36561326    5.15648120\n  ability[36]       5.91376321 0.58629303    4.79505696    5.51469373\n  ability[37]       4.02225302 0.61868818    2.85623426    3.60663896\n  ability[38]       2.05659780 0.55534390    1.11845020    1.62863171\n  ability[39]       2.29454780 0.55506354    1.25459370    1.90892164\n  ability[40]       1.94434436 0.51155058    1.13584679    1.53940555\n  ability[41]       7.77631482 0.60577137    6.51852668    7.36123147\n  ability[42]       7.15789741 0.54346406    6.07521911    6.80617036\n  ability[43]       8.57393705 0.56224386    7.48302554    8.19505724\n  ability[44]       7.70736130 0.59668568    6.43130281    7.33506722\n  ability[45]       5.94958288 0.56763703    4.86810525    5.54837271\n  ability[46]       2.76456092 0.63467313    1.54354085    2.32238583\n  ability[47]       9.20332226 0.50480328    8.08414579    8.88880343\n  ability[48]       7.93336465 0.57999327    6.75782430    7.56753582\n  ability[49]       2.91880516 0.58430493    1.74867461    2.51794314\n  ability[50]       5.66447718 0.59117409    4.50130029    5.27137629\n  ability[51]       1.66383828 0.41149989    1.04411055    1.36131546\n  ability[52]       2.64349982 0.56619716    1.47152928    2.27961113\n  ability[53]       6.62753612 0.59991952    5.46220954    6.22585037\n  ability[54]       3.24409901 0.57024974    1.99451662    2.89479493\n  ability[55]       3.40911180 0.59509994    2.23382271    3.00811344\n  ability[56]       2.28134813 0.57513742    1.20390872    1.89604452\n  ability[57]       4.92741307 0.63162889    3.63911530    4.54731095\n  ability[58]       8.83813130 0.54302554    7.79417518    8.46656997\n  ability[59]       4.32126792 0.62283812    3.09686817    3.90343037\n  ability[60]       8.22158257 0.61380050    7.13501995    7.80544889\n  ability[61]       2.47146679 0.56533150    1.36877164    2.07492846\n  ability[62]       9.17637368 0.49475409    8.11877748    8.84504306\n  ability[63]       9.09526137 0.51485948    8.01579472    8.74891413\n  ability[64]       2.11744683 0.54548264    1.17023615    1.70736290\n  ability[65]       6.60716474 0.58006886    5.46967149    6.20163645\n  ability[66]       3.54015697 0.54075842    2.52529109    3.15665579\n  ability[67]       2.52595337 0.57724382    1.28908399    2.16554961\n  ability[68]       3.01783263 0.57342704    1.80888078    2.68658765\n  ability[69]       9.29118137 0.44279861    8.32478252    8.99315833\n  ability[70]       5.51960161 0.61188560    4.33810425    5.10310917\n  ability[71]       4.76699538 0.58334196    3.65037017    4.36194465\n  ability[72]       6.69796954 0.62258637    5.47060198    6.25572357\n  ability[73]       3.46830385 0.58818754    2.36062473    3.06519058\n  ability[74]       7.63330384 0.59840452    6.51699113    7.23301916\n  ability[75]       8.72430511 0.55491262    7.64886049    8.31332776\n  ability[76]       5.06005310 0.60697878    3.85498345    4.65231805\n  ability[77]       3.63547855 0.58904624    2.48604643    3.24284327\n  ability[78]       4.37799273 0.62618673    3.16617086    3.97804560\n  ability[79]       6.79779547 0.58501775    5.64615586    6.43501019\n  ability[80]       4.38000867 0.60911440    3.19521885    3.97842000\n  ability[81]       6.44449776 0.56863552    5.35135479    6.03859745\n  ability[82]       3.11739086 0.58309153    2.05036076    2.67524845\n  ability[83]       5.32233553 0.59245800    4.16249497    4.90568529\n  ability[84]       1.64002170 0.43713373    1.02848755    1.30285111\n  ability[85]       5.67033978 0.62631147    4.49316067    5.23420868\n  ability[86]       8.23631426 0.57017448    7.15441825    7.85010661\n  ability[87]       2.36534620 0.56513258    1.31701079    1.99043535\n  ability[88]       8.53682519 0.54399044    7.44008247    8.16981123\n  ability[89]       6.61770861 0.57555601    5.51561269    6.22969436\n  ability[90]       8.44404600 0.56447075    7.31019970    8.05582852\n  ability[91]       3.18894507 0.58622789    2.06611667    2.82233544\n  ability[92]       3.38786769 0.59959678    2.25584021    2.97638074\n  ability[93]       4.01478332 0.60830599    2.75791002    3.60829564\n  ability[94]       2.87754787 0.56469978    1.85848665    2.48357000\n  ability[95]       7.76621473 0.54466096    6.68734915    7.42435245\n  ability[96]       4.35890823 0.57121820    3.28481745    3.97251135\n  ability[97]       5.51599927 0.57305064    4.34022319    5.14535417\n  ability[98]       6.52209926 0.59044828    5.31128373    6.13977031\n  ability[99]       4.43019006 0.56750222    3.27522029    4.04020793\n  ability[100]      5.97956861 0.62132233    4.68256307    5.55882472\n  severity[1]       0.29261417 0.27402342   -0.24503997    0.09722419\n  severity[2]       0.71871148 0.27272745    0.18744979    0.54299925\n  severity[3]      -1.73008387 0.27473518   -2.26322683   -1.91097485\n  severity[4]       1.56903026 0.28465344    1.04882922    1.36837733\n  severity[5]      -0.72841259 0.27324053   -1.26836016   -0.91546724\n  severity[6]      -0.66919678 0.28002013   -1.21715697   -0.85258036\n  severity[7]      -2.59016416 0.29286309   -3.16277210   -2.78689819\n  severity[8]       0.43562148 0.26885427   -0.07092975    0.25065762\n  severity[9]       1.46433339 0.27337800    0.92448849    1.27074532\n  severity[10]      0.07158235 0.27584917   -0.48879016   -0.11422981\n  sigma             0.93990612 0.05018209    0.85083968    0.90546940\n  tau               1.50024417 0.45315346    0.90282253    1.19324125\n  expRating[1]      3.89949034 0.54298846    2.79188404    3.53806607\n  expRating[2]      7.19860447 0.52812102    6.10498585    6.85442195\n  expRating[3]      4.96037743 0.53731965    3.90817624    4.60580748\n  expRating[4]      8.33899367 0.57652404    7.24722839    7.93331047\n  expRating[5]      9.61540976 0.57258232    8.52049489    9.21867592\n  expRating[6]      7.37718272 0.57668822    6.26586587    6.99696721\n  expRating[7]      6.15586267 0.59685947    5.00814470    5.76143226\n  expRating[8]      7.26068093 0.58956659    6.15638855    6.84458133\n  expRating[9]      6.89664180 0.59163360    5.76216307    6.51116565\n  expRating[10]     7.83877052 0.59629078    6.74487299    7.43411454\n  expRating[11]     6.45086226 0.58197731    5.31781393    6.04252064\n  expRating[12]     4.52989488 0.58058836    3.39759857    4.11929424\n  expRating[13]     3.85522833 0.56253192    2.73684447    3.46331573\n  expRating[14]     4.70554711 0.56379600    3.58844330    4.33768369\n  expRating[15]     2.40810426 0.55687062    1.24457254    2.02424079\n  expRating[16]     6.14850861 0.56688498    5.03688674    5.76426288\n  expRating[17]     3.85106576 0.57653505    2.78008085    3.47092115\n  expRating[18]     4.65106070 0.56711020    3.54171532    4.25827893\n  expRating[19]     9.23665143 0.52948281    8.16395154    8.85426237\n  expRating[20]     7.21395340 0.52638717    6.14696878    6.85426541\n  expRating[21]     9.37965874 0.53484431    8.26725662    8.99647574\n  expRating[22]     2.50555493 0.56253717    1.43649602    2.10808802\n  expRating[23]     3.56644201 0.56419226    2.43425099    3.18850729\n  expRating[24]     5.69997219 0.55162018    4.67998795    5.32970233\n  expRating[25]     3.15459012 0.57281666    2.07408123    2.73994386\n  expRating[26]     1.13189208 0.57348579    0.08055869    0.73437432\n  expRating[27]     0.27181179 0.56544603   -0.80543348   -0.12497334\n  expRating[28]    10.38992336 0.56922011    9.28619940    9.99827935\n  expRating[29]     9.25651459 0.56933479    8.12046000    8.88563308\n  expRating[30]    10.28522650 0.56437017    9.12105023    9.92644933\n  expRating[31]     7.28774898 0.58179380    6.12273882    6.89744388\n  expRating[32]     7.18305212 0.57800664    6.01723775    6.79096593\n  expRating[33]     5.79030107 0.57894521    4.64611687    5.41706361\n  expRating[34]     7.41626276 0.60770172    6.19524427    7.01796369\n  expRating[35]     8.26658154 0.59235531    7.08662383    7.86399005\n  expRating[36]     5.96913869 0.59827956    4.79135813    5.57842672\n  expRating[37]     2.40790893 0.45563680    1.63296739    2.08001854\n  expRating[38]    -0.04088642 0.45754919   -0.80630391   -0.36991089\n  expRating[39]     2.12481893 0.45162955    1.41519349    1.77712996\n  expRating[40]     9.62264938 0.45013647    8.64824810    9.33394318\n  expRating[41]     8.66083843 0.45516637    7.72221407    8.36888560\n  expRating[42]     9.76565669 0.44622426    8.84215870    9.49465628\n  expRating[43]     6.11919262 0.56153005    5.04287109    5.70990464\n  expRating[44]     3.23641430 0.55646603    2.16089286    2.85708931\n  expRating[45]     7.29091185 0.56420406    6.19669971    6.91142674\n  expRating[46]     8.24076697 0.54566379    7.17197765    7.87178058\n  expRating[47]     6.00253993 0.53268522    4.98862141    5.64931895\n  expRating[48]     4.08157255 0.53056776    3.06064817    3.70846630\n  expRating[49]     5.96867358 0.58792658    4.87386604    5.57232846\n  expRating[50]     2.65979794 0.56769637    1.50305770    2.30692342\n  expRating[51]     5.32154445 0.55903520    4.18608751    4.96747794\n  expRating[52]     1.21116766 0.52627869    0.29620528    0.84228911\n  expRating[53]     3.40391365 0.51775908    2.46819995    3.04600331\n  expRating[54]     2.01116260 0.52930371    1.04345098    1.64270265\n  expRating[55]     4.22405411 0.55954888    3.19536570    3.82581486\n  expRating[56]     1.98582708 0.56054093    0.93979848    1.58393182\n  expRating[57]     0.06485970 0.58200119   -1.01143120   -0.33290131\n  expRating[58]     4.12438121 0.57788755    2.99911993    3.72661909\n  expRating[59]     4.18359702 0.59186081    2.98907234    3.78641819\n  expRating[60]     6.31712719 0.59269291    5.14648154    5.95104381\n  expRating[61]     7.63019787 0.57777721    6.49361203    7.25319342\n  expRating[62]     6.24228961 0.56351066    5.11912090    5.87188287\n  expRating[63]     7.34710787 0.57198425    6.19928125    6.97681462\n  expRating[64]     7.96720885 0.59231476    6.75831945    7.60418076\n  expRating[65]     9.24362494 0.59101658    8.05888508    8.88250289\n  expRating[66]     5.08443052 0.57738514    3.95676773    4.70109563\n  expRating[67]     2.09104155 0.57808588    1.02815981    1.69249700\n  expRating[68]     5.39015568 0.57156518    4.36443306    4.99383545\n  expRating[69]     4.25674690 0.57927797    3.17818207    3.84115588\n  expRating[70]     9.02267408 0.54826863    7.83955338    8.67065430\n  expRating[71]     6.99997605 0.55118918    5.87173594    6.61929210\n  expRating[72]    10.19439331 0.54574987    9.05786113    9.82825483\n  expRating[73]     2.83156581 0.55294884    1.79873404    2.43190713\n  expRating[74]     2.54847581 0.55952076    1.45101230    2.17460136\n  expRating[75]     3.57718772 0.56736585    2.50618179    3.17215805\n  expRating[76]     7.52992896 0.55703719    6.42992849    7.15492578\n  expRating[77]     5.29170192 0.55963988    4.18214542    4.90538283\n  expRating[78]     6.39652019 0.55067208    5.36914061    6.03024618\n  expRating[79]     0.91619791 0.58031009   -0.18483620    0.52051009\n  expRating[80]     4.21531204 0.58341868    3.09738877    3.81054305\n  expRating[81]     4.11061517 0.58685336    2.95650137    3.70053668\n  expRating[82]     2.87497674 0.55487246    1.88268787    2.46767377\n  expRating[83]     2.59188674 0.55672230    1.58803134    2.19079348\n  expRating[84]     2.22784761 0.55344653    1.23015209    1.82821461\n  expRating[85]     4.35617741 0.54977287    3.27263085    3.98665326\n  expRating[86]     6.54892339 0.55010120    5.45956503    6.18708967\n  expRating[87]     5.15617235 0.54113113    4.04486605    4.81515052\n  expRating[88]     8.68411826 0.55470064    7.64080281    8.28551787\n  expRating[89]     7.72230731 0.54273416    6.65528780    7.34782915\n  expRating[90]     9.85583749 0.56200082    8.75567915    9.45515811\n  expRating[91]    10.39625856 0.55668235    9.32043275   10.03939351\n  expRating[92]    10.29156169 0.54710508    9.23667917    9.92751264\n  expRating[93]     8.89881065 0.55120726    7.75185206    8.51869813\n  expRating[94]     0.65409574 0.57596172   -0.45344847    0.24677272\n  expRating[95]     1.71498282 0.54318489    0.72041229    1.34170645\n  expRating[96]     2.45576195 0.54868594    1.39615862    2.04451799\n  expRating[97]    10.61613347 0.53743259    9.52220134   10.25994930\n  expRating[98]     6.45693905 0.53856421    5.39138363    6.09861718\n  expRating[99]    10.51143660 0.52256360    9.43537589   10.17909296\n  expRating[100]    8.05593081 0.57731986    6.91883576    7.66126802\n  expRating[101]    7.77284081 0.55605798    6.70127765    7.38295630\n  expRating[102]    7.40880168 0.59080356    6.22481890    7.03398774\n  expRating[103]    4.79616949 0.53461084    3.71778038    4.43334395\n  expRating[104]    6.98891547 0.54671170    5.89643442    6.60450883\n  expRating[105]    5.59616443 0.53717222    4.54436121    5.22054087\n  expRating[106]    6.63247469 0.56750137    5.52085739    6.27221529\n  expRating[107]    5.24456643 0.57161620    4.14564715    4.85955344\n  expRating[108]    7.37809660 0.56316074    6.27200159    6.98669715\n  expRating[109]    4.31486719 0.60010800    3.16337443    3.90068550\n  expRating[110]    1.43208886 0.59262561    0.33858687    1.03007744\n  expRating[111]    5.48658641 0.60927738    4.33242131    5.08950773\n  expRating[112]    2.34921197 0.54250944    1.37160906    1.96031733\n  expRating[113]    1.32818521 0.55309364    0.35657485    0.93716070\n  expRating[114]   -0.53356636 0.56037374   -1.57028950   -0.92897570\n  expRating[115]    3.86357806 0.56026368    2.86831868    3.48247302\n  expRating[116]    1.56613521 0.53987685    0.56943291    1.17942262\n  expRating[117]    3.75888120 0.53648461    2.79892155    3.37270780\n  expRating[118]    2.66305584 0.52044171    1.74196384    2.29583548\n  expRating[119]    0.21426049 0.51126679   -0.68613276   -0.15697610\n  expRating[120]    3.40867775 0.51941496    2.49288524    3.00536199\n  expRating[121]    8.49502630 0.57958856    7.26670100    8.11633912\n  expRating[122]    6.04623095 0.58217158    4.88511187    5.62124728\n  expRating[123]    7.84789717 0.58599745    6.70250891    7.43913765\n  expRating[124]    7.45051158 0.51522017    6.43929303    7.10869116\n  expRating[125]    6.42948482 0.52413444    5.39841939    6.08818974\n  expRating[126]    8.62223081 0.55242135    7.50892415    8.25631893\n  expRating[127]    6.84385318 0.55780077    5.73653958    6.46973928\n  expRating[128]    9.00955853 0.54885170    7.89772009    8.66116798\n  expRating[129]    8.64551940 0.54061365    7.55466639    8.27036349\n  expRating[130]    5.97727743 0.58742534    4.82156098    5.57645572\n  expRating[131]    9.17169469 0.57609464    7.98083785    8.80034512\n  expRating[132]    7.77894364 0.58261582    6.57665782    7.41685272\n  expRating[133]    7.51861314 0.54738604    6.52786846    7.14595746\n  expRating[134]    5.22117029 0.56799611    4.10899881    4.82576825\n  expRating[135]    6.38520436 0.56813580    5.34620276    5.97057104\n  expRating[136]    2.03614833 0.59205480    0.87744063    1.64131537\n  expRating[137]    2.09536414 0.60467159    0.88099911    1.69193118\n  expRating[138]    0.17439677 0.61575572   -1.05685882   -0.26129551\n  expRating[139]    9.49593643 0.50459941    8.48333808    9.16053603\n  expRating[140]    8.47490967 0.49879437    7.42152631    8.18713779\n  expRating[141]    9.63894374 0.50116431    8.54757520    9.31462331\n  expRating[142]    7.20495206 0.56164714    6.08500832    6.83258623\n  expRating[143]    9.39769805 0.54549362    8.28580033    9.01379945\n  expRating[144]    8.00494700 0.55490115    6.86774292    7.66980356\n  expRating[145]    4.48783542 0.57386080    3.32093863    4.11536276\n  expRating[146]    0.32864101 0.55970804   -0.74362372   -0.06463365\n  expRating[147]    2.99038751 0.54287143    1.92335141    2.63399146\n  expRating[148]    4.93606459 0.56723502    3.85708526    4.56051656\n  expRating[149]    3.07431302 0.57146049    1.95799678    2.71210232\n  expRating[150]    7.12881057 0.57195476    6.01251415    6.73719214\n  expRating[151]    1.95645244 0.43236889    1.17813645    1.66636393\n  expRating[152]    2.38254976 0.44456668    1.57486007    2.05915039\n  expRating[153]    3.12817167 0.43464203    2.35277759    2.82278598\n  expRating[154]    0.91341596 0.53495255   -0.12479301    0.55682842\n  expRating[155]    1.97430304 0.54668847    0.91616271    1.59597894\n  expRating[156]    4.10783322 0.53640564    3.08257694    3.75322765\n  expRating[157]    5.89912353 0.58832313    4.69766408    5.50117246\n  expRating[158]    4.03737196 0.57344913    2.91952625    3.64826583\n  expRating[159]    8.09186952 0.59298003    6.93031639    7.69058398\n  expRating[160]    3.96281049 0.55267212    2.77108049    3.62226615\n  expRating[161]    1.51401514 0.57096325    0.33443335    1.15617792\n  expRating[162]    2.51568642 0.55495056    1.35623561    2.17566114\n  expRating[163]    3.70172597 0.58550364    2.54683685    3.30492995\n  expRating[164]    2.68069921 0.55925965    1.62257968    2.29761186\n  expRating[165]    4.87344519 0.57450549    3.75444583    4.48667287\n  expRating[166]    3.85037838 0.56360207    2.81666322    3.45642394\n  expRating[167]   -0.30881603 0.54828325   -1.36201006   -0.69396576\n  expRating[168]    2.71696961 0.56032627    1.64597359    2.33317859\n  expRating[169]    3.19732921 0.59717586    1.96348788    2.83259917\n  expRating[170]    2.33724891 0.59794897    1.13277725    1.95887920\n  expRating[171]    4.99899542 0.59974410    3.75415788    4.62502145\n  expRating[172]    7.10804743 0.53728960    6.10875814    6.71662003\n  expRating[173]    8.10971871 0.54319661    7.05242582    7.72098378\n  expRating[174]    8.90971364 0.52507486    7.87949462    8.55303487\n  expRating[175]    3.65207114 0.59677049    2.46975345    3.24023458\n  expRating[176]    1.73110376 0.60602427    0.59271269    1.30252648\n  expRating[177]    4.39285027 0.60765644    3.24003142    3.96864262\n  expRating[178]    6.49149871 0.57701416    5.49388171    6.11549574\n  expRating[179]    9.79061283 0.60014676    8.69536576    9.39768458\n  expRating[180]    9.68591597 0.59814938    8.61673405    9.24681690\n  expRating[181]    2.76408096 0.53434412    1.68220552    2.40782742\n  expRating[182]    1.80227001 0.54136645    0.77151701    1.43755967\n  expRating[183]    2.90708827 0.53862748    1.83903349    2.54502561\n  expRating[184]    9.46898784 0.50185079    8.47575213    9.12190530\n  expRating[185]    9.89508516 0.50686666    8.89371713    9.55398458\n  expRating[186]    9.61199516 0.50847144    8.60532527    9.28052710\n  expRating[187]   10.66429162 0.51801414    9.57209774   10.31337545\n  expRating[188]    8.36684878 0.51502280    7.25716787    8.05075711\n  expRating[189]   10.55959476 0.50834540    9.50519147   10.21132658\n  expRating[190]    2.41006099 0.53959842    1.46061899    2.02353177\n  expRating[191]    1.44825004 0.53707855    0.43659369    1.07512164\n  expRating[192]    3.58178022 0.51949901    2.65426788    3.20196075\n  expRating[193]    6.89977891 0.56551010    5.80285061    6.53176853\n  expRating[194]    7.32587622 0.56685228    6.26885777    6.93828848\n  expRating[195]    8.17619500 0.58004496    7.08593945    7.77140092\n  expRating[196]    3.83277113 0.52908748    2.82945700    3.45114586\n  expRating[197]    2.81174438 0.52430980    1.77251609    2.46107919\n  expRating[198]    0.94999281 0.52771341   -0.07047049    0.59492921\n  expRating[199]    0.79586951 0.55881422   -0.34214273    0.42289758\n  expRating[200]    4.09498363 0.55909254    2.98592238    3.76199895\n  expRating[201]   -0.06421078 0.55627589   -1.23933636   -0.40676042\n  expRating[202]    3.73654411 0.54976215    2.63035396    3.35982643\n  expRating[203]    4.58686289 0.55802022    3.42264668    4.22535612\n  expRating[204]    0.42766847 0.56671140   -0.73147204    0.04850106\n  expRating[205]    9.58379554 0.47163012    8.56009256    9.28299790\n  expRating[206]   10.00989285 0.46157017    9.02434826    9.69912716\n  expRating[207]    9.72680285 0.46414664    8.79363542    9.41058003\n  expRating[208]    7.08863187 0.58918259    5.97379130    6.66448918\n  expRating[209]    4.85040483 0.58866031    3.76378974    4.45734749\n  expRating[210]    5.59118396 0.56630610    4.48377160    5.18906231\n  expRating[211]    5.05960955 0.55523460    3.99075256    4.67294327\n  expRating[212]    5.48570686 0.56826854    4.42781661    5.06955191\n  expRating[213]    3.03691152 0.57084443    1.95360722    2.64427167\n  expRating[214]    4.96788567 0.60546831    3.78118928    4.55202555\n  expRating[215]    5.96955695 0.58900357    4.79807704    5.56660538\n  expRating[216]    6.76955189 0.60310390    5.54838265    6.35929943\n  expRating[217]    5.03733411 0.57531523    3.94758339    4.64802568\n  expRating[218]    2.73989126 0.57440911    1.59227245    2.34311065\n  expRating[219]    3.53988620 0.56675280    2.36527084    3.15781821\n  expRating[220]    8.35201532 0.57492564    7.24523590    7.98951458\n  expRating[221]    5.04313968 0.58007334    3.91544964    4.65409557\n  expRating[222]    7.70488619 0.57416905    6.61255775    7.31520123\n  expRating[223]    9.44301659 0.55825334    8.34251730    9.08708514\n  expRating[224]    6.99422125 0.55010454    5.92735090    6.64663431\n  expRating[225]    9.15992659 0.54157478    8.14034538    8.79199441\n  expRating[226]    5.35266726 0.59194445    4.16535898    4.96694835\n  expRating[227]    3.32996923 0.58482554    2.15351112    2.93412770\n  expRating[228]    4.33164051 0.59365849    3.18280550    3.93699058\n  expRating[229]    1.90539468 0.57069031    0.76954726    1.53281541\n  expRating[230]    2.90706596 0.55773484    1.83831194    2.52983693\n  expRating[231]    1.04531439 0.56456694   -0.07294547    0.65586383\n  expRating[232]    4.67060690 0.61862523    3.41138593    4.24430303\n  expRating[233]    5.94702299 0.60289195    4.79057545    5.52898446\n  expRating[234]    1.78782857 0.62553895    0.57800416    1.36275376\n  expRating[235]    7.09040964 0.55437348    6.01261529    6.71813577\n  expRating[236]    5.06771161 0.58281122    3.94698334    4.67708075\n  expRating[237]    6.06938288 0.56789066    4.93891718    5.70658475\n  expRating[238]    2.64992480 0.59615764    1.49121935    2.27045046\n  expRating[239]    4.81563015 0.58747426    3.65338485    4.42119524\n  expRating[240]    4.45159101 0.58825486    3.23137485    4.06962830\n  expRating[241]    4.71441390 0.55468762    3.60534333    4.33156072\n  expRating[242]    3.85433360 0.55529915    2.77874893    3.45071849\n  expRating[243]    6.88011924 0.55164702    5.84019055    6.47893168\n  expRating[244]    3.41000502 0.57956802    2.29529445    2.99077618\n  expRating[245]    2.38897827 0.57567641    1.22837408    1.98677395\n  expRating[246]    3.18897321 0.57405889    2.14407718    2.78486897\n  expRating[247]    5.61494970 0.56703241    4.50429130    5.22001513\n  expRating[248]    4.65313875 0.57827389    3.47523518    4.27932054\n  expRating[249]    5.39391788 0.56433292    4.32269408    5.02148320\n  expRating[250]    1.93263587 0.45214132    1.13628162    1.62541890\n  expRating[251]    2.35873318 0.44951598    1.59580749    2.03061758\n  expRating[252]   -0.09006217 0.45484373   -0.88615532   -0.42024691\n  expRating[253]    3.08017562 0.59888037    1.91952056    2.68847790\n  expRating[254]    6.10596126 0.61832363    4.90428940    5.70894511\n  expRating[255]    5.74192213 0.59010749    4.60365443    5.32425612\n  expRating[256]    6.50623039 0.56165439    5.39162023    6.14335989\n  expRating[257]    5.64615010 0.54653278    4.53378792    5.28634826\n  expRating[258]    8.30789661 0.53953374    7.26821139    7.94820985\n  expRating[259]    2.65796037 0.54850192    1.63322799    2.27562118\n  expRating[260]   -0.22481796 0.54634094   -1.26457577   -0.62592159\n  expRating[261]    2.43692855 0.54381241    1.44899785    2.05583980\n  expRating[262]    7.80841260 0.53169043    6.72500188    7.45596701\n  expRating[263]    7.86762841 0.52626832    6.81574352    7.50480649\n  expRating[264]    5.94666103 0.52465382    4.85848220    5.58991468\n  expRating[265]    5.94851183 0.55488107    4.86462225    5.57577517\n  expRating[266]    4.02754446 0.56076287    2.93856637    3.64942824\n  expRating[267]    6.68929096 0.56946008    5.52672706    6.31077803\n  expRating[268]    7.71563341 0.54760537    6.67167471    7.34439398\n  expRating[269]    5.85388184 0.53996384    4.77747338    5.47479531\n  expRating[270]    8.87966748 0.54854134    7.76334205    8.51161795\n  expRating[271]    3.90765655 0.55554424    2.80465050    3.53364046\n  expRating[272]    2.46053248 0.56801937    1.40607151    2.06773759\n  expRating[273]    2.51974829 0.57013946    1.48464480    2.12379136\n  expRating[274]    3.68048185 0.58926512    2.56663941    3.25655492\n  expRating[275]    4.10657917 0.58842578    3.02187686    3.70603423\n  expRating[276]    3.82348917 0.58218022    2.66309512    3.41261626\n  expRating[277]    4.73349480 0.58218904    3.53519454    4.35764016\n  expRating[278]    5.58381358 0.59661770    4.41077691    5.18934817\n  expRating[279]    4.45040480 0.57754506    3.31776450    4.08607138\n  expRating[280]    3.17016204 0.54101084    2.18051199    2.79896221\n  expRating[281]    4.44657813 0.53756556    3.43267696    4.07874073\n  expRating[282]    2.20835109 0.52519071    1.21891472    1.86371559\n  expRating[283]    8.48492621 0.51534315    7.51433119    8.14352763\n  expRating[284]    9.33524499 0.53183875    8.32970476    8.95679479\n  expRating[285]    8.20183621 0.54074258    7.11395124    7.84574147\n  expRating[286]    5.07761971 0.54139930    4.05810753    4.69834007\n  expRating[287]    3.68971145 0.54539084    2.69352792    3.29751126\n  expRating[288]    4.79452971 0.54124406    3.75545056    4.43298939\n  expRating[289]    6.23471075 0.54210632    5.13928958    5.85079579\n  expRating[290]    4.84680249 0.54541969    3.75168615    4.48211551\n  expRating[291]    5.95162075 0.55311989    4.89954545    5.56657289\n  expRating[292]    7.24081074 0.57776823    6.06981239    6.84938821\n  expRating[293]    5.85290248 0.56008883    4.72002725    5.47126240\n  expRating[294]    6.95772074 0.57715816    5.80088820    6.57846969\n  expRating[295]    5.14890154 0.54936497    4.07204245    4.79561902\n  expRating[296]    3.76099328 0.53024420    2.74587049    3.39939860\n  expRating[297]    4.86581154 0.54247234    3.77492685    4.50634750\n  expRating[298]    4.24948474 0.61440431    3.04257071    3.83002122\n  expRating[299]    5.31037182 0.58983308    4.07656864    4.92939256\n  expRating[300]    7.44390200 0.61915808    6.19660769    7.02995020\n  lp__           -103.17504100 9.60607847 -122.76659295 -109.65217623\n                stats\nparameter                  50%         75%       97.5%\n  ability[1]        5.62799458   5.9889135   6.7875045\n  ability[2]        8.06393711   8.4300159   9.1993755\n  ability[3]        6.80837732   7.2317559   8.0863096\n  ability[4]        7.14569644   7.5273359   8.2545672\n  ability[5]        3.15185755   3.4922580   4.2686250\n  ability[6]        4.58543777   4.9781811   5.7576969\n  ability[7]        8.98922918   9.3389438   9.8331098\n  ability[8]        4.18292617   4.6388750   5.4167653\n  ability[9]        2.85129585   3.2664930   4.0665621\n  ability[10]       8.86927173   9.2246751   9.8181963\n  ability[11]       5.72164179   6.1186694   6.8888627\n  ability[12]       6.72311268   7.1073212   7.9550598\n  ability[13]       1.61546592   1.9915958   2.6174647\n  ability[14]       9.38845398   9.6648635   9.9603482\n  ability[15]       5.82567296   6.2080806   6.9604760\n  ability[16]       6.64353899   7.0747466   7.7050669\n  ability[17]       5.24352903   5.6055082   6.4457014\n  ability[18]       1.89364252   2.3124797   3.0307776\n  ability[19]       2.65049934   3.0658305   3.8347963\n  ability[20]       4.86173056   5.2420734   6.0490163\n  ability[21]       6.88452230   7.2910385   8.1391472\n  ability[22]       7.68813667   8.0733344   8.8398106\n  ability[23]       3.80515824   4.2084591   5.0548310\n  ability[24]       8.75634619   9.1122800   9.7487622\n  ability[25]       2.09218337   2.5180752   3.2443379\n  ability[26]       5.95947944   6.3380356   7.1023949\n  ability[27]       2.61587668   3.0751412   3.8604750\n  ability[28]       2.15093290   2.5334203   3.3441912\n  ability[29]       5.09291549   5.4623938   6.2194881\n  ability[30]       8.40606525   8.7851438   9.4451887\n  ability[31]       8.85330246   9.2233037   9.8014642\n  ability[32]       2.37373062   2.7880511   3.4918553\n  ability[33]       9.06968859   9.4420709   9.8896546\n  ability[34]       7.34443643   7.7424342   8.4630307\n  ability[35]       5.54723657   5.8902618   6.5484976\n  ability[36]       5.91263407   6.2867978   7.1887487\n  ability[37]       4.01293713   4.4609842   5.2517783\n  ability[38]       2.03190190   2.4274204   3.2069646\n  ability[39]       2.27520668   2.6686208   3.3509948\n  ability[40]       1.90744797   2.2796649   3.0119187\n  ability[41]       7.80314445   8.1976473   8.9235733\n  ability[42]       7.15574820   7.5151938   8.2101580\n  ability[43]       8.56360397   8.9731227   9.6295149\n  ability[44]       7.73988796   8.1153502   8.8050960\n  ability[45]       5.94246808   6.3620164   7.0160894\n  ability[46]       2.76976968   3.2189505   4.0166512\n  ability[47]       9.26054625   9.5930783   9.9461111\n  ability[48]       7.93752078   8.3615695   8.9655993\n  ability[49]       2.91733791   3.3129588   4.1025632\n  ability[50]       5.68395486   6.0772806   6.7660021\n  ability[51]       1.61265226   1.9098245   2.6105736\n  ability[52]       2.66619760   3.0098979   3.7388829\n  ability[53]       6.63768957   7.0380225   7.7814695\n  ability[54]       3.26985620   3.6161742   4.3171593\n  ability[55]       3.42362509   3.8035342   4.5543087\n  ability[56]       2.26594783   2.6495679   3.4237672\n  ability[57]       4.95610343   5.3430742   6.2379028\n  ability[58]       8.85615879   9.2215724   9.8841885\n  ability[59]       4.31010444   4.7472560   5.5169924\n  ability[60]       8.18817534   8.6127683   9.5323984\n  ability[61]       2.47484824   2.8556591   3.5248061\n  ability[62]       9.22125327   9.5721670   9.9304841\n  ability[63]       9.13705256   9.4880086   9.9030082\n  ability[64]       2.08444243   2.4761836   3.2950442\n  ability[65]       6.60564599   7.0256730   7.7855598\n  ability[66]       3.52542456   3.9041173   4.6227736\n  ability[67]       2.52598695   2.9248877   3.6444539\n  ability[68]       3.02785739   3.3920350   4.1204429\n  ability[69]       9.34750010   9.6440165   9.9475489\n  ability[70]       5.50977061   5.9518058   6.7044416\n  ability[71]       4.76256850   5.1799511   5.8718387\n  ability[72]       6.69855185   7.1204086   7.9259758\n  ability[73]       3.47376723   3.8483411   4.6593291\n  ability[74]       7.64085146   8.0231899   8.7962950\n  ability[75]       8.74278454   9.1227012   9.7917818\n  ability[76]       5.07777563   5.4620407   6.1789276\n  ability[77]       3.65530468   4.0282168   4.8076509\n  ability[78]       4.35365692   4.7998946   5.6382422\n  ability[79]       6.79100354   7.1944530   7.9480131\n  ability[80]       4.35662703   4.7749727   5.6307309\n  ability[81]       6.45416881   6.8444291   7.5269245\n  ability[82]       3.11307015   3.5377207   4.2416519\n  ability[83]       5.30859627   5.7418309   6.5000180\n  ability[84]       1.57687140   1.9063332   2.6332064\n  ability[85]       5.66973692   6.0865286   6.8461779\n  ability[86]       8.23095609   8.6303132   9.3679576\n  ability[87]       2.32915080   2.7348863   3.5304056\n  ability[88]       8.53223895   8.9444202   9.5356921\n  ability[89]       6.60279026   6.9948516   7.7521681\n  ability[90]       8.44300951   8.8407612   9.4547016\n  ability[91]       3.18285129   3.5627528   4.4132524\n  ability[92]       3.37318058   3.8068915   4.5426883\n  ability[93]       4.01401128   4.4459474   5.1753825\n  ability[94]       2.86592521   3.2658819   3.9543584\n  ability[95]       7.77190339   8.1479601   8.7708191\n  ability[96]       4.36955437   4.7397105   5.4724673\n  ability[97]       5.50999316   5.9142522   6.5645508\n  ability[98]       6.52766281   6.9123173   7.6803632\n  ability[99]       4.43003377   4.8215846   5.4973122\n  ability[100]      5.98308667   6.3737025   7.2482509\n  severity[1]       0.30267236   0.4912375   0.8136419\n  severity[2]       0.72516462   0.8962323   1.2345320\n  severity[3]      -1.72449888  -1.5556586  -1.1353579\n  severity[4]       1.56344653   1.7699022   2.1372439\n  severity[5]      -0.72748442  -0.5327175  -0.1865232\n  severity[6]      -0.67541609  -0.4865462  -0.1457052\n  severity[7]      -2.59225149  -2.3933161  -2.0414509\n  severity[8]       0.43437174   0.6125920   0.9326485\n  severity[9]       1.46662937   1.6407227   2.0226799\n  severity[10]      0.08105666   0.2505973   0.6440907\n  sigma             0.93827954   0.9740191   1.0391643\n  tau               1.40219875   1.7069041   2.6180198\n  expRating[1]      3.90852244   4.2679057   4.9145121\n  expRating[2]      7.20270881   7.5506569   8.2765382\n  expRating[3]      4.97024485   5.3187339   6.0212989\n  expRating[4]      8.32312678   8.7323243   9.4427754\n  expRating[5]      9.59662699  10.0095158  10.7917483\n  expRating[6]      7.37541682   7.7519392   8.4952835\n  expRating[7]      6.15379677   6.5477851   7.3921424\n  expRating[8]      7.26685688   7.6600521   8.3857835\n  expRating[9]      6.90108041   7.2832958   8.0607665\n  expRating[10]     7.84698459   8.2502193   8.9889630\n  expRating[11]     6.45849774   6.8548813   7.5844034\n  expRating[12]     4.51873075   4.9381685   5.6092625\n  expRating[13]     3.86800178   4.2182858   4.9716077\n  expRating[14]     4.71587876   5.0536892   5.8079158\n  expRating[15]     2.42792510   2.7699363   3.5458846\n  expRating[16]     6.14589867   6.5321509   7.2875640\n  expRating[17]     3.83695957   4.2394414   4.9897707\n  expRating[18]     4.66547293   5.0389538   5.7214170\n  expRating[19]     9.27057623   9.6250225  10.1924747\n  expRating[20]     7.25634573   7.5892220   8.1505643\n  expRating[21]     9.41087579   9.7728479  10.3127113\n  expRating[22]     2.51519944   2.8872647   3.5760525\n  expRating[23]     3.57269492   3.9624902   4.6686491\n  expRating[24]     5.68683302   6.0726311   6.7851078\n  expRating[25]     3.15280056   3.5494833   4.2663513\n  expRating[26]     1.13554104   1.5231002   2.2838943\n  expRating[27]     0.26683549   0.6925224   1.3925358\n  expRating[28]    10.42589240  10.8170066  11.3657252\n  expRating[29]     9.27023183   9.6707335  10.3036863\n  expRating[30]    10.30168521  10.6887877  11.3361930\n  expRating[31]     7.29216787   7.6766597   8.4191691\n  expRating[32]     7.18717961   7.5655013   8.3547609\n  expRating[33]     5.79922605   6.1877730   6.9146035\n  expRating[34]     7.40175655   7.8151800   8.6789806\n  expRating[35]     8.25053534   8.6439460   9.4455194\n  expRating[36]     5.96141246   6.3807715   7.1262371\n  expRating[37]     2.37262618   2.6882383   3.4424882\n  expRating[38]    -0.07669701   0.2653697   0.9302161\n  expRating[39]     2.07546473   2.4098947   3.0848042\n  expRating[40]     9.64966388   9.9476005  10.4009851\n  expRating[41]     8.68842465   8.9809998   9.4772683\n  expRating[42]     9.77129511  10.0828200  10.5945728\n  expRating[43]     6.11880214   6.5141544   7.1753073\n  expRating[44]     3.21937279   3.6106282   4.3143649\n  expRating[45]     7.29120666   7.6636149   8.3782866\n  expRating[46]     8.25936428   8.6175780   9.2192737\n  expRating[47]     5.99198774   6.3658474   7.0324570\n  expRating[48]     4.08249916   4.4526737   5.1026793\n  expRating[49]     5.96751564   6.3404463   7.1788658\n  expRating[50]     2.67289172   3.0293234   3.8622182\n  expRating[51]     5.31049935   5.6823908   6.4698963\n  expRating[52]     1.17631024   1.5749841   2.2525198\n  expRating[53]     3.38262055   3.7380685   4.4486033\n  expRating[54]     1.97021800   2.3956900   3.0493610\n  expRating[55]     4.22557147   4.6209131   5.2514030\n  expRating[56]     1.96533501   2.3971194   3.0441303\n  expRating[57]     0.06161556   0.4762520   1.1650032\n  expRating[58]     4.13484580   4.5016728   5.2743833\n  expRating[59]     4.19314878   4.5678762   5.3714792\n  expRating[60]     6.32949288   6.7090711   7.5076717\n  expRating[61]     7.62121434   7.9939222   8.8558518\n  expRating[62]     6.22371931   6.6414798   7.4333431\n  expRating[63]     7.34149088   7.7192932   8.5322051\n  expRating[64]     7.99409163   8.3752502   9.1339266\n  expRating[65]     9.24474515   9.6644167  10.3383956\n  expRating[66]     5.06555102   5.4882425   6.2262852\n  expRating[67]     2.08674489   2.4670927   3.2997932\n  expRating[68]     5.37224216   5.7894294   6.4748544\n  expRating[69]     4.25735637   4.6681379   5.4162538\n  expRating[70]     9.03438878   9.3891445  10.0331156\n  expRating[71]     7.02027621   7.3954935   8.0620038\n  expRating[72]    10.21987781  10.5908577  11.1400709\n  expRating[73]     2.81074235   3.2088850   3.9320247\n  expRating[74]     2.53705905   2.9296776   3.6237951\n  expRating[75]     3.57332955   3.9595636   4.7623571\n  expRating[76]     7.54020280   7.9219500   8.5926948\n  expRating[77]     5.27765854   5.6733551   6.3348093\n  expRating[78]     6.39299209   6.7637199   7.4647217\n  expRating[79]     0.89348110   1.3273053   2.0176911\n  expRating[80]     4.20198693   4.6254356   5.3866768\n  expRating[81]     4.11258565   4.5078825   5.2546051\n  expRating[82]     2.86661183   3.2656582   3.9605017\n  expRating[83]     2.56544230   2.9565137   3.7245247\n  expRating[84]     2.20523276   2.6092755   3.3276201\n  expRating[85]     4.35111655   4.7103165   5.4611667\n  expRating[86]     6.54337724   6.8939137   7.6254105\n  expRating[87]     5.15741396   5.5056907   6.1998702\n  expRating[88]     8.69410886   9.0694146   9.7364981\n  expRating[89]     7.74091090   8.0910680   8.7480481\n  expRating[90]     9.86179354  10.2464670  10.9171729\n  expRating[91]    10.40192218  10.7887392  11.4437357\n  expRating[92]    10.31728248  10.6642311  11.2864480\n  expRating[93]     8.90818312   9.2876716   9.8687369\n  expRating[94]     0.66390356   1.0368913   1.8117187\n  expRating[95]     1.72027835   2.0623450   2.8313254\n  expRating[96]     2.46158129   2.8116220   3.5755812\n  expRating[97]    10.60267683  10.9754863  11.6149128\n  expRating[98]     6.46417491   6.8412437   7.4406250\n  expRating[99]    10.51913097  10.8800522  11.4668595\n  expRating[100]    8.05791346   8.4627357   9.1512835\n  expRating[101]    7.78112807   8.1478932   8.8959346\n  expRating[102]    7.42001697   7.8185016   8.5075046\n  expRating[103]    4.80564639   5.1542421   5.8461509\n  expRating[104]    7.00327369   7.3735566   8.0174351\n  expRating[105]    5.60111963   5.9662043   6.6364046\n  expRating[106]    6.64795385   6.9969880   7.7598479\n  expRating[107]    5.23319851   5.6288894   6.3607268\n  expRating[108]    7.38858528   7.7550775   8.4989182\n  expRating[109]    4.33193520   4.7143771   5.4301143\n  expRating[110]    1.42659495   1.8308666   2.5874619\n  expRating[111]    5.48670084   5.8920871   6.7010046\n  expRating[112]    2.32562392   2.7057732   3.4554124\n  expRating[113]    1.29449199   1.6965672   2.4859756\n  expRating[114]   -0.55968063  -0.1717688   0.5906395\n  expRating[115]    3.83215356   4.2128071   5.0264472\n  expRating[116]    1.55925767   1.9331083   2.6729902\n  expRating[117]    3.74887397   4.1412542   4.8256283\n  expRating[118]    2.62517225   3.0196170   3.7356957\n  expRating[119]    0.19804099   0.5326331   1.2723293\n  expRating[120]    3.38257444   3.7637247   4.4251521\n  expRating[121]    8.52939089   8.8937247   9.6036770\n  expRating[122]    6.06601011   6.4790698   7.1125268\n  expRating[123]    7.85963208   8.2524725   8.9544851\n  expRating[124]    7.44765156   7.7982863   8.4348007\n  expRating[125]    6.42579151   6.8029279   7.4199867\n  expRating[126]    8.62106440   9.0056287   9.6867209\n  expRating[127]    6.84667616   7.2268360   7.8860614\n  expRating[128]    9.02353761   9.3857469  10.0204733\n  expRating[129]    8.64983623   9.0281616   9.6682152\n  expRating[130]    5.99414860   6.4124159   7.0368827\n  expRating[131]    9.17754728   9.5444620  10.3093661\n  expRating[132]    7.79658977   8.1516222   8.9528935\n  expRating[133]    7.50430373   7.8867208   8.6010000\n  expRating[134]    5.22914228   5.6014382   6.3309751\n  expRating[135]    6.36376519   6.7646641   7.4838260\n  expRating[136]    2.02624769   2.4171330   3.2279299\n  expRating[137]    2.11229708   2.5035696   3.2436545\n  expRating[138]    0.18389582   0.5906184   1.3517325\n  expRating[139]    9.52968855   9.8716056  10.3350492\n  expRating[140]    8.48419075   8.8550236   9.3099260\n  expRating[141]    9.68105146  10.0091053  10.4607528\n  expRating[142]    7.23436201   7.5631901   8.2329451\n  expRating[143]    9.43128004   9.7832367  10.3795916\n  expRating[144]    8.02673500   8.3749182   9.0828981\n  expRating[145]    4.50205716   4.8732837   5.6164550\n  expRating[146]    0.32963952   0.7120546   1.4036926\n  expRating[147]    2.98013006   3.3646075   4.0746636\n  expRating[148]    4.94019714   5.3243555   6.0843663\n  expRating[149]    3.06309367   3.4567744   4.2372953\n  expRating[150]    7.13352982   7.5221496   8.1756140\n  expRating[151]    1.92970692   2.2294227   2.8594836\n  expRating[152]    2.36092181   2.6753306   3.2855793\n  expRating[153]    3.09943833   3.4061935   4.0921946\n  expRating[154]    0.91558954   1.2783502   1.9834827\n  expRating[155]    1.97865719   2.3539584   3.0280090\n  expRating[156]    4.10759292   4.4779422   5.1412360\n  expRating[157]    5.91771115   6.2973767   7.0023126\n  expRating[158]    4.04146408   4.4277622   5.1310061\n  expRating[159]    8.12411297   8.4913842   9.2269531\n  expRating[160]    3.96383467   4.3105284   5.0152224\n  expRating[161]    1.50307442   1.8942424   2.6045878\n  expRating[162]    2.51122324   2.8547772   3.6412358\n  expRating[163]    3.70176787   4.0964571   4.8485252\n  expRating[164]    2.68571778   3.0847911   3.7501394\n  expRating[165]    4.87466796   5.2772700   5.9814112\n  expRating[166]    3.84207373   4.2164643   4.9884088\n  expRating[167]   -0.29944979   0.0386570   0.8067337\n  expRating[168]    2.71839446   3.1050865   3.8165120\n  expRating[169]    3.20717258   3.5974416   4.3645076\n  expRating[170]    2.35859253   2.7271898   3.5423576\n  expRating[171]    5.02925180   5.3695306   6.1394771\n  expRating[172]    7.10534773   7.4914037   8.1956409\n  expRating[173]    8.11027995   8.4873838   9.1883353\n  expRating[174]    8.89369613   9.2583346   9.9409286\n  expRating[175]    3.64529194   4.0616690   4.7789069\n  expRating[176]    1.73575693   2.1089748   2.9218310\n  expRating[177]    4.39104876   4.8112788   5.5896133\n  expRating[178]    6.46301902   6.8541594   7.7747633\n  expRating[179]    9.77595899  10.1845705  11.1213774\n  expRating[180]    9.66303570  10.0708884  10.8831985\n  expRating[181]    2.76307221   3.1189266   3.8466596\n  expRating[182]    1.81629912   2.1685967   2.8969335\n  expRating[183]    2.91957467   3.2489592   4.0095935\n  expRating[184]    9.51372787   9.8466521  10.3031366\n  expRating[185]    9.93048024  10.2670955  10.7771193\n  expRating[186]    9.65600296   9.9901315  10.4273786\n  expRating[187]   10.69538063  11.0377165  11.5452793\n  expRating[188]    8.40392061   8.7199297   9.2546835\n  expRating[189]   10.58221725  10.9288448  11.4413974\n  expRating[190]    2.38585353   2.7500090   3.5279136\n  expRating[191]    1.41406771   1.8131787   2.5409794\n  expRating[192]    3.54576930   3.9422795   4.6337069\n  expRating[193]    6.86885620   7.2796622   8.0012363\n  expRating[194]    7.32317282   7.6926979   8.4533478\n  expRating[195]    8.16411794   8.5896746   9.2900641\n  expRating[196]    3.83095063   4.1955940   4.8370442\n  expRating[197]    2.81659542   3.1496771   3.8065286\n  expRating[198]    0.95210955   1.2991102   1.9665800\n  expRating[199]    0.80953719   1.1714932   1.8273959\n  expRating[200]    4.10190279   4.4733644   5.1368460\n  expRating[201]   -0.06308846   0.3358324   0.9307322\n  expRating[202]    3.73900666   4.0941854   4.7353885\n  expRating[203]    4.58395825   4.9551109   5.6792848\n  expRating[204]    0.44226469   0.7929109   1.5049230\n  expRating[205]    9.60867365   9.9326176  10.4321678\n  expRating[206]   10.04133358  10.3506520  10.8044199\n  expRating[207]    9.75253451  10.0611535  10.5598908\n  expRating[208]    7.09488210   7.4972410   8.2682935\n  expRating[209]    4.83150487   5.2598140   5.9891350\n  expRating[210]    5.58313856   5.9804806   6.7497609\n  expRating[211]    5.07580047   5.4287396   6.1403589\n  expRating[212]    5.49378957   5.8866795   6.5680854\n  expRating[213]    3.06022611   3.4328256   4.1469375\n  expRating[214]    4.94781519   5.3744615   6.1144946\n  expRating[215]    5.96662510   6.3703462   7.0832017\n  expRating[216]    6.78165510   7.1686373   7.9221339\n  expRating[217]    5.04306542   5.4186959   6.1450348\n  expRating[218]    2.74131206   3.1281293   3.8934515\n  expRating[219]    3.55059613   3.9339291   4.6097520\n  expRating[220]    8.34801483   8.7521903   9.4536344\n  expRating[221]    5.03910388   5.4097976   6.1466579\n  expRating[222]    7.72219140   8.0843260   8.8329269\n  expRating[223]    9.45853274   9.8390190  10.4697358\n  expRating[224]    7.01531942   7.3600338   8.0350492\n  expRating[225]    9.17076402   9.5373813  10.2290669\n  expRating[226]    5.34878062   5.7361767   6.5230950\n  expRating[227]    3.32233177   3.7325717   4.4514132\n  expRating[228]    4.32581378   4.7182031   5.5376692\n  expRating[229]    1.90632984   2.2674084   3.0501537\n  expRating[230]    2.91108355   3.2902071   4.0046781\n  expRating[231]    1.03474104   1.4434367   2.1539615\n  expRating[232]    4.67002973   5.0954354   5.9102301\n  expRating[233]    5.93421325   6.3506631   7.1000441\n  expRating[234]    1.78003228   2.1962413   3.0616396\n  expRating[235]    7.07751427   7.4608548   8.1852380\n  expRating[236]    5.06754324   5.4606203   6.1744810\n  expRating[237]    6.06632218   6.4456246   7.1617932\n  expRating[238]    2.62280942   3.0524246   3.8211308\n  expRating[239]    4.79686867   5.1960092   6.0151052\n  expRating[240]    4.43550651   4.8598931   5.6114200\n  expRating[241]    4.71687382   5.1068231   5.8062922\n  expRating[242]    3.85589314   4.2356710   4.9179224\n  expRating[243]    6.87147256   7.2665680   7.9368914\n  expRating[244]    3.39045900   3.8250596   4.5632031\n  expRating[245]    2.37598104   2.7946745   3.4888238\n  expRating[246]    3.16917009   3.5921508   4.3137684\n  expRating[247]    5.61907656   5.9982164   6.7184963\n  expRating[248]    4.65757124   5.0238964   5.7868164\n  expRating[249]    5.38194099   5.7690876   6.5021972\n  expRating[250]    1.89108935   2.2385050   2.8738024\n  expRating[251]    2.31582073   2.6538120   3.3226678\n  expRating[252]   -0.12009999   0.1865097   0.8944910\n  expRating[253]    3.07557334   3.4911409   4.2219467\n  expRating[254]    6.10222386   6.5084921   7.3001892\n  expRating[255]    5.74396881   6.1480759   6.8940369\n  expRating[256]    6.51164195   6.8705851   7.6091568\n  expRating[257]    5.65576315   6.0121372   6.6863310\n  expRating[258]    8.30563602   8.6818561   9.3864953\n  expRating[259]    2.65100318   3.0251334   3.7764533\n  expRating[260]   -0.22714633   0.1400230   0.8533824\n  expRating[261]    2.43501211   2.8158438   3.5016270\n  expRating[262]    7.82885459   8.1892355   8.7113460\n  expRating[263]    7.87855636   8.2425629   8.8615559\n  expRating[264]    5.95296925   6.3272474   6.8944598\n  expRating[265]    5.94326187   6.3096615   7.0118103\n  expRating[266]    4.04222070   4.3988125   5.1253135\n  expRating[267]    6.68546036   7.0514276   7.7992201\n  expRating[268]    7.72937968   8.0972616   8.7497396\n  expRating[269]    5.86257823   6.2358856   6.8524065\n  expRating[270]    8.88149737   9.2650650   9.9081266\n  expRating[271]    3.90596815   4.2883683   5.0168693\n  expRating[272]    2.47980897   2.8475218   3.5385852\n  expRating[273]    2.53304639   2.8897080   3.7188805\n  expRating[274]    3.66972623   4.1080838   4.7914118\n  expRating[275]    4.09575118   4.5217198   5.2279386\n  expRating[276]    3.82918435   4.2220654   4.9440332\n  expRating[277]    4.74665048   5.1123441   5.8282608\n  expRating[278]    5.58505519   5.9648246   6.8009428\n  expRating[279]    4.45967904   4.8434875   5.5685927\n  expRating[280]    3.16036990   3.5428461   4.2787818\n  expRating[281]    4.42569357   4.8140768   5.5175715\n  expRating[282]    2.16835759   2.5675686   3.2829428\n  expRating[283]    8.46546761   8.8475501   9.4302500\n  expRating[284]    9.36237396   9.7104331  10.3309773\n  expRating[285]    8.19937692   8.5663383   9.3158214\n  expRating[286]    5.08846083   5.4485226   6.1512300\n  expRating[287]    3.69073753   4.0653351   4.8359219\n  expRating[288]    4.79612294   5.1547427   5.8288050\n  expRating[289]    6.25996894   6.6107691   7.2201632\n  expRating[290]    4.86548534   5.2616457   5.8756343\n  expRating[291]    5.96403535   6.3518232   6.9336252\n  expRating[292]    7.23621092   7.6302820   8.3562660\n  expRating[293]    5.87803551   6.2275031   6.9531606\n  expRating[294]    6.96745605   7.3516168   8.0681823\n  expRating[295]    5.15893812   5.5028374   6.2408198\n  expRating[296]    3.74284043   4.1250946   4.7784993\n  expRating[297]    4.88116942   5.2317392   5.8837321\n  expRating[298]    4.24322192   4.6642416   5.4307434\n  expRating[299]    5.31894743   5.6908116   6.4320228\n  expRating[300]    7.46234763   7.8465820   8.6284093\n  lp__           -103.00387413 -96.2698186 -84.8146750\n\n, , chains = chain:4\n\n                stats\nparameter                 mean          sd          2.5%           25%\n  ability[1]        5.56312570  0.57784117    4.35496269  5.191107e+00\n  ability[2]        7.97408542  0.60484595    6.84440421  7.568746e+00\n  ability[3]        6.76540892  0.57498216    5.63391998  6.375404e+00\n  ability[4]        7.05603846  0.59406309    5.90539809  6.666684e+00\n  ability[5]        3.06181051  0.56930148    1.89418370  2.671489e+00\n  ability[6]        4.52951290  0.60130372    3.35775007  4.144931e+00\n  ability[7]        8.92585893  0.54641136    7.78124362  8.572645e+00\n  ability[8]        4.16377722  0.56720269    3.08221208  3.763168e+00\n  ability[9]        2.80910292  0.58678966    1.69695126  2.381485e+00\n  ability[10]       8.78658537  0.60537387    7.52356038  8.385318e+00\n  ability[11]       5.64228143  0.60571517    4.40609362  5.249161e+00\n  ability[12]       6.61441596  0.59074799    5.47337694  6.204655e+00\n  ability[13]       1.66369969  0.43142337    1.03717004  1.343025e+00\n  ability[14]       9.28750169  0.43488663    8.31328720  8.987085e+00\n  ability[15]       5.78359231  0.60567127    4.56551508  5.391327e+00\n  ability[16]       6.64343224  0.61033365    5.42901301  6.239608e+00\n  ability[17]       5.16336428  0.57411806    4.02292697  4.801179e+00\n  ability[18]       1.92271869  0.51780574    1.09538407  1.530725e+00\n  ability[19]       2.58835278  0.56705176    1.53604405  2.207580e+00\n  ability[20]       4.78879018  0.57631144    3.65527330  4.413395e+00\n  ability[21]       6.79853163  0.59368321    5.59944219  6.384048e+00\n  ability[22]       7.58305025  0.61213500    6.38125453  7.161555e+00\n  ability[23]       3.75369978  0.59969863    2.62523401  3.309385e+00\n  ability[24]       8.67251088  0.57708925    7.42815460  8.315292e+00\n  ability[25]       2.10937743  0.54215960    1.12204668  1.721201e+00\n  ability[26]       5.89765242  0.57887169    4.71308149  5.500381e+00\n  ability[27]       2.60164906  0.56441540    1.54228614  2.175466e+00\n  ability[28]       2.12184659  0.54445702    1.16852239  1.718239e+00\n  ability[29]       5.04099112  0.58948214    3.91131283  4.642688e+00\n  ability[30]       8.31894181  0.59495033    7.19515508  7.910008e+00\n  ability[31]       8.73047775  0.56590682    7.54041614  8.354863e+00\n  ability[32]       2.32891435  0.56874727    1.27627955  1.931827e+00\n  ability[33]       8.99855421  0.55311812    7.89710231  8.586144e+00\n  ability[34]       7.28505167  0.63349671    6.09889571  6.875478e+00\n  ability[35]       5.45840303  0.59967260    4.34076504  5.041651e+00\n  ability[36]       5.85235022  0.61535897    4.59689555  5.463612e+00\n  ability[37]       3.96366438  0.56990344    2.76357795  3.584870e+00\n  ability[38]       2.02618142  0.53102161    1.14639040  1.609319e+00\n  ability[39]       2.23057694  0.57067132    1.18661411  1.824027e+00\n  ability[40]       1.91235654  0.51234102    1.06556515  1.526540e+00\n  ability[41]       7.71979498  0.59929061    6.60446423  7.296831e+00\n  ability[42]       7.09287238  0.57887416    5.99325025  6.720694e+00\n  ability[43]       8.50820486  0.56185375    7.35805126  8.156953e+00\n  ability[44]       7.64827320  0.56674844    6.46655113  7.249566e+00\n  ability[45]       5.87109969  0.57888116    4.72750194  5.484413e+00\n  ability[46]       2.71776952  0.56607007    1.69714417  2.319909e+00\n  ability[47]       9.17563530  0.47156682    8.14530255  8.851574e+00\n  ability[48]       7.86033831  0.56398817    6.75715735  7.482531e+00\n  ability[49]       2.83335531  0.57379825    1.67493958  2.467317e+00\n  ability[50]       5.61872483  0.58432239    4.52997521  5.230634e+00\n  ability[51]       1.63198559  0.42781702    1.03379601  1.297792e+00\n  ability[52]       2.56083812  0.58201621    1.38749545  2.153082e+00\n  ability[53]       6.58948348  0.61181418    5.39879302  6.186878e+00\n  ability[54]       3.16430129  0.57678675    2.08471778  2.762301e+00\n  ability[55]       3.35909886  0.63280906    2.07933821  2.957116e+00\n  ability[56]       2.23847495  0.54329256    1.24194279  1.826089e+00\n  ability[57]       4.85163040  0.59776034    3.74621830  4.424888e+00\n  ability[58]       8.76898741  0.54205355    7.68141406  8.392413e+00\n  ability[59]       4.22199656  0.59710844    3.10784362  3.781055e+00\n  ability[60]       8.13625579  0.57007593    6.99804203  7.759768e+00\n  ability[61]       2.40604252  0.55290105    1.40419990  2.002850e+00\n  ability[62]       9.12362168  0.49478436    8.03289250  8.811447e+00\n  ability[63]       9.02014142  0.50334583    7.99293883  8.692925e+00\n  ability[64]       2.05823980  0.54311432    1.09911734  1.652368e+00\n  ability[65]       6.58020594  0.58802438    5.45036859  6.172300e+00\n  ability[66]       3.48264712  0.60610451    2.31189744  3.077628e+00\n  ability[67]       2.44913905  0.57121385    1.39282389  2.042551e+00\n  ability[68]       2.93078436  0.59551093    1.79905696  2.540742e+00\n  ability[69]       9.25293302  0.47804505    8.21279638  8.932056e+00\n  ability[70]       5.45680502  0.59334773    4.30608667  5.068758e+00\n  ability[71]       4.71744617  0.60162158    3.54731528  4.323963e+00\n  ability[72]       6.62950273  0.57512548    5.50060656  6.253683e+00\n  ability[73]       3.39783185  0.61502207    2.13959173  2.995008e+00\n  ability[74]       7.57636878  0.60757985    6.35834472  7.188205e+00\n  ability[75]       8.69171844  0.59457762    7.51878374  8.292340e+00\n  ability[76]       4.98267721  0.58824489    3.83352217  4.599491e+00\n  ability[77]       3.53535707  0.62955794    2.35831133  3.106574e+00\n  ability[78]       4.32657235  0.62174964    3.01893134  3.912975e+00\n  ability[79]       6.73560001  0.55664901    5.65572544  6.363241e+00\n  ability[80]       4.31963557  0.56314905    3.19545408  3.951125e+00\n  ability[81]       6.39458589  0.59093416    5.26717948  5.993677e+00\n  ability[82]       3.02820306  0.60354696    1.90362984  2.611155e+00\n  ability[83]       5.24004612  0.58378408    4.12605288  4.836428e+00\n  ability[84]       1.59742148  0.42021655    1.01454097  1.259148e+00\n  ability[85]       5.64462050  0.59800086    4.51619272  5.207632e+00\n  ability[86]       8.17197256  0.56628518    7.08332284  7.797890e+00\n  ability[87]       2.28805302  0.57809578    1.18410324  1.861137e+00\n  ability[88]       8.47546075  0.58875031    7.26192817  8.090172e+00\n  ability[89]       6.55961295  0.62732202    5.31683326  6.122258e+00\n  ability[90]       8.41238508  0.55901924    7.33648844  8.035055e+00\n  ability[91]       3.11193430  0.60167501    1.93550633  2.710757e+00\n  ability[92]       3.34710043  0.63972148    2.13290576  2.923580e+00\n  ability[93]       3.99436235  0.58452787    2.79080141  3.612967e+00\n  ability[94]       2.80652395  0.60111366    1.57445659  2.412267e+00\n  ability[95]       7.68702860  0.56161837    6.59792710  7.302583e+00\n  ability[96]       4.27962944  0.58351535    3.12744820  3.875066e+00\n  ability[97]       5.45226722  0.63979695    4.22476857  5.034572e+00\n  ability[98]       6.46658159  0.58587517    5.31141933  6.063201e+00\n  ability[99]       4.38479391  0.61669031    3.09554403  3.968307e+00\n  ability[100]      5.92295970  0.56064310    4.84011218  5.530284e+00\n  severity[1]       0.34668338  0.28437507   -0.16895821  1.566655e-01\n  severity[2]       0.77961453  0.27172328    0.27873116  5.856348e-01\n  severity[3]      -1.67483732  0.27843599   -2.22012571 -1.869559e+00\n  severity[4]       1.62543877  0.28995338    1.05560583  1.450720e+00\n  severity[5]      -0.66019454  0.29031166   -1.20451740 -8.730856e-01\n  severity[6]      -0.59500196  0.28750680   -1.14074149 -7.925307e-01\n  severity[7]      -2.53057726  0.28586683   -3.09181435 -2.718938e+00\n  severity[8]       0.47779332  0.28170479   -0.09065722  2.864107e-01\n  severity[9]       1.51341933  0.29265378    0.93685366  1.304434e+00\n  severity[10]      0.13541638  0.28453430   -0.42731345 -5.276928e-02\n  sigma             0.93701524  0.04717708    0.85123372  9.052943e-01\n  tau               1.48437473  0.42686053    0.94094015  1.198277e+00\n  expRating[1]      3.88828838  0.54999826    2.78623639  3.511452e+00\n  expRating[2]      7.18856447  0.55603686    6.04502045  6.817944e+00\n  expRating[3]      4.96812373  0.54107833    3.93096740  4.612788e+00\n  expRating[4]      8.32076880  0.59321772    7.15126006  7.913192e+00\n  expRating[5]      9.59952420  0.56553542    8.54016520  9.205960e+00\n  expRating[6]      7.37908346  0.57202529    6.26360173  6.983284e+00\n  expRating[7]      6.17040696  0.56543460    5.08230824  5.804382e+00\n  expRating[8]      7.24320224  0.55688487    6.16314093  6.885890e+00\n  expRating[9]      6.90082531  0.55469462    5.78528069  6.530031e+00\n  expRating[10]     7.83565299  0.57695367    6.74766614  7.438659e+00\n  expRating[11]     6.46103649  0.58002344    5.31394709  6.088023e+00\n  expRating[12]     4.52546119  0.57062278    3.41536554  4.137992e+00\n  expRating[13]     3.84142505  0.55114530    2.75207147  3.486307e+00\n  expRating[14]     4.68724929  0.54318320    3.61408732  4.337921e+00\n  expRating[15]     2.40161597  0.54353499    1.30086676  2.019974e+00\n  expRating[16]     6.15495167  0.57094177    5.05768928  5.768213e+00\n  expRating[17]     3.86931836  0.56309703    2.79634645  3.505957e+00\n  expRating[18]     4.66492928  0.56638926    3.54053215  4.306669e+00\n  expRating[19]     9.27254231  0.53815968    8.12560809  8.940336e+00\n  expRating[20]     7.25102161  0.53941751    6.10309124  6.899120e+00\n  expRating[21]     9.40365225  0.53032019    8.28398738  9.069536e+00\n  expRating[22]     2.48893990  0.54565732    1.46899126  2.099640e+00\n  expRating[23]     3.56877525  0.55931301    2.51621779  3.186054e+00\n  expRating[24]     5.67719655  0.55519738    4.63124167  5.284910e+00\n  expRating[25]     3.15578630  0.57489353    2.05970833  2.729653e+00\n  expRating[26]     1.13426560  0.56999183    0.07834318  7.186022e-01\n  expRating[27]     0.27852566  0.56209074   -0.75234576 -1.257740e-01\n  expRating[28]    10.41202415  0.58311549    9.24897082  1.000715e+01\n  expRating[29]     9.26437869  0.57415398    8.09611730  8.862907e+00\n  expRating[30]    10.30000470  0.58596682    9.17135306  9.897738e+00\n  expRating[31]     7.26772021  0.57551251    6.05083428  6.891023e+00\n  expRating[32]     7.15570077  0.57896071    5.94362668  6.774037e+00\n  expRating[33]     5.77769782  0.58551856    4.61553925  5.385063e+00\n  expRating[34]     7.39403049  0.56565467    6.33617010  6.995489e+00\n  expRating[35]     8.23985474  0.55212231    7.17354185  7.848484e+00\n  expRating[36]     5.95422142  0.55275972    4.91350851  5.559165e+00\n  expRating[37]     2.44331422  0.44561073    1.69811340  2.112979e+00\n  expRating[38]    -0.01113763  0.45939725   -0.78611787 -3.443577e-01\n  expRating[39]     2.14149301  0.45046909    1.36382284  1.822786e+00\n  expRating[40]     9.63418507  0.43854775    8.70283551  9.343683e+00\n  expRating[41]     8.69249972  0.44354723    7.75985316  8.412513e+00\n  expRating[42]     9.76529501  0.45813948    8.81031252  9.470233e+00\n  expRating[43]     6.13027569  0.57409107    4.99879091  5.762546e+00\n  expRating[44]     3.25301504  0.57234802    2.13013511  2.880307e+00\n  expRating[45]     7.29701164  0.57224806    6.18689909  6.927949e+00\n  expRating[46]     8.26887102  0.59038077    7.12000643  7.878110e+00\n  expRating[47]     6.04843028  0.57269419    4.97779844  5.666081e+00\n  expRating[48]     4.11285498  0.58003754    2.95407979  3.716997e+00\n  expRating[49]     5.94297881  0.56200066    4.79115498  5.561804e+00\n  expRating[50]     2.63278702  0.54409148    1.55299740  2.286663e+00\n  expRating[51]     5.29878066  0.54503164    4.22745449  4.941842e+00\n  expRating[52]     1.26252415  0.49881084    0.34240578  9.067848e-01\n  expRating[53]     3.43613802  0.49163971    2.52787220  3.079494e+00\n  expRating[54]     2.05813508  0.48994232    1.18386898  1.700618e+00\n  expRating[55]     4.21379155  0.54446522    3.14161744  3.834971e+00\n  expRating[56]     1.99335081  0.53787416    0.91425295  1.639755e+00\n  expRating[57]     0.05777551  0.54696910   -1.05995192 -2.979264e-01\n  expRating[58]     4.12859564  0.53005187    3.10293298  3.763304e+00\n  expRating[59]     4.19378822  0.54366469    3.07225302  3.842325e+00\n  expRating[60]     6.30220951  0.54846790    5.24061972  5.938945e+00\n  expRating[61]     7.57814616  0.56114457    6.44101343  7.195124e+00\n  expRating[62]     6.20352967  0.57064978    5.06795862  5.802387e+00\n  expRating[63]     7.27632495  0.56542022    6.09747387  6.900636e+00\n  expRating[64]     7.92973363  0.57441382    6.78866093  7.548815e+00\n  expRating[65]     9.20848903  0.58804335    8.10326829  8.807035e+00\n  expRating[66]     5.05247299  0.57658381    3.92666254  4.673421e+00\n  expRating[67]     2.07886246  0.57696298    1.00925607  1.678570e+00\n  expRating[68]     5.37913856  0.56940665    4.33002641  4.981305e+00\n  expRating[69]     4.23149310  0.58038611    3.15812481  3.830362e+00\n  expRating[70]     9.01919426  0.55146018    7.91129351  8.678090e+00\n  expRating[71]     6.99767355  0.55636648    5.90078208  6.628150e+00\n  expRating[72]    10.18593021  0.56037501    9.03811747  9.829104e+00\n  expRating[73]     2.88899196  0.52310243    1.93183664  2.502171e+00\n  expRating[74]     2.58717074  0.53344083    1.63338449  2.198531e+00\n  expRating[75]     3.62279676  0.53613659    2.62018873  3.250785e+00\n  expRating[76]     7.52309120  0.55127099    6.47641026  7.143433e+00\n  expRating[77]     5.30265046  0.56356908    4.21131537  4.919737e+00\n  expRating[78]     6.37544574  0.54566621    5.34130527  6.008331e+00\n  expRating[79]     0.92681174  0.53670863   -0.07567285  5.471849e-01\n  expRating[80]     4.22708783  0.54034946    3.23281887  3.845521e+00\n  expRating[81]     4.11506839  0.53709107    3.13112432  3.759549e+00\n  expRating[82]     2.90146113  0.53505792    1.91473643  2.535942e+00\n  expRating[83]     2.59963991  0.54577022    1.59016013  2.223055e+00\n  expRating[84]     2.25726298  0.54124996    1.21711042  1.884058e+00\n  expRating[85]     4.38079658  0.55604565    3.28083378  4.016124e+00\n  expRating[86]     6.55441045  0.55400867    5.45227558  6.193608e+00\n  expRating[87]     5.17640750  0.54577483    4.11481736  4.797093e+00\n  expRating[88]     8.66562519  0.55924298    7.61265440  8.276867e+00\n  expRating[89]     7.72393985  0.57770947    6.65162598  7.330116e+00\n  expRating[90]     9.83236114  0.57097411    8.73789739  9.438776e+00\n  expRating[91]    10.35591653  0.54393482    9.27023010  1.000470e+01\n  expRating[92]    10.24389708  0.54444696    9.16718857  9.875766e+00\n  expRating[93]     8.86589414  0.53163298    7.77255650  8.542111e+00\n  expRating[94]     0.65407703  0.54371631   -0.38263048  2.780853e-01\n  expRating[95]     1.73391238  0.54650830    0.63357165  1.368028e+00\n  expRating[96]     2.46433073  0.54218664    1.43983980  2.083279e+00\n  expRating[97]    10.62399299  0.54545814    9.51476779  1.026447e+01\n  expRating[98]     6.46797695  0.52756160    5.41006738  6.133852e+00\n  expRating[99]    10.51197355  0.52629970    9.41963015  1.017880e+01\n  expRating[100]    8.06466620  0.60404258    6.89972225  7.668472e+00\n  expRating[101]    7.76284499  0.60081821    6.67748238  7.356500e+00\n  expRating[102]    7.42046805  0.59810832    6.28442038  7.002903e+00\n  expRating[103]    4.79820849  0.56760326    3.65706415  4.394816e+00\n  expRating[104]    6.97182236  0.56419426    5.84162795  6.581243e+00\n  expRating[105]    5.59381942  0.58046696    4.46170038  5.198843e+00\n  expRating[106]    6.63196475  0.57997813    5.48587072  6.273716e+00\n  expRating[107]    5.25734826  0.58806018    4.10821864  4.872479e+00\n  expRating[108]    7.36576955  0.59417042    6.21097597  7.003811e+00\n  expRating[109]    4.31034776  0.54958974    3.27282655  3.936862e+00\n  expRating[110]    1.43308712  0.53480189    0.36412699  1.094816e+00\n  expRating[111]    5.47708371  0.53429709    4.43876772  5.127263e+00\n  expRating[112]    2.37286480  0.51213736    1.40084486  2.012438e+00\n  expRating[113]    1.36598688  0.52079974    0.44287918  9.948878e-01\n  expRating[114]   -0.50439584  0.51314789   -1.41041526 -8.690742e-01\n  expRating[115]    3.85601572  0.54862140    2.82257520  3.460659e+00\n  expRating[116]    1.57038240  0.54370830    0.57747418  1.157033e+00\n  expRating[117]    3.74399628  0.53581639    2.74497436  3.346950e+00\n  expRating[118]    2.69197108  0.50550783    1.77304667  2.329485e+00\n  expRating[119]    0.23751922  0.52123399   -0.65855224 -1.469294e-01\n  expRating[120]    3.42577587  0.52088757    2.50032415  3.044684e+00\n  expRating[121]    8.49940951  0.57804791    7.39462697  8.094840e+00\n  expRating[122]    6.04495765  0.57430372    4.92951933  5.656925e+00\n  expRating[123]    7.85521136  0.57117504    6.77791889  7.445120e+00\n  expRating[124]    7.43955576  0.55271570    6.31564454  7.099224e+00\n  expRating[125]    6.43267784  0.53596533    5.36941731  6.080475e+00\n  expRating[126]    8.60629171  0.54639090    7.46960435  8.248633e+00\n  expRating[127]    6.83336754  0.53840373    5.76562736  6.469897e+00\n  expRating[128]    8.98599818  0.54228808    7.94164204  8.608380e+00\n  expRating[129]    8.64362124  0.54665843    7.53092540  8.265438e+00\n  expRating[130]    5.97343588  0.54262137    4.88077280  5.628392e+00\n  expRating[131]    9.16169253  0.54556672    8.03194430  8.801357e+00\n  expRating[132]    7.78368958  0.54422322    6.72154362  7.400658e+00\n  expRating[133]    7.49653846  0.53847823    6.40884292  7.151318e+00\n  expRating[134]    5.21090514  0.53716285    4.14611857  4.835060e+00\n  expRating[135]    6.34889300  0.56084850    5.20562928  5.985044e+00\n  expRating[136]    2.05757498  0.55100664    1.09245792  1.655019e+00\n  expRating[137]    2.12276755  0.55069859    1.02858374  1.752062e+00\n  expRating[138]    0.18719225  0.53428167   -0.79358089 -1.826612e-01\n  expRating[139]    9.52231869  0.46062315    8.63165692  9.187073e+00\n  expRating[140]    8.51544076  0.47103914    7.53701803  8.200129e+00\n  expRating[141]    9.65342862  0.48364621    8.67914051  9.328367e+00\n  expRating[142]    7.20014377  0.53171069    6.18892457  6.821053e+00\n  expRating[143]    9.37375764  0.54675617    8.30523901  8.982879e+00\n  expRating[144]    7.99575469  0.53912089    6.95371164  7.599174e+00\n  expRating[145]    4.45879409  0.54346251    3.34285694  4.098030e+00\n  expRating[146]    0.30277805  0.54746435   -0.83642277 -6.410329e-02\n  expRating[147]    2.96877169  0.53656570    1.83229450  2.618129e+00\n  expRating[148]    4.95853029  0.54265936    3.91655756  4.602541e+00\n  expRating[149]    3.08814757  0.54904062    2.05698201  2.707585e+00\n  expRating[150]    7.13214416  0.55231032    6.01452095  6.772441e+00\n  expRating[151]    1.97866897  0.46480638    1.16399294  1.668691e+00\n  expRating[152]    2.41160012  0.45800527    1.61486876  2.089068e+00\n  expRating[153]    3.14540492  0.46655233    2.29061434  2.826414e+00\n  expRating[154]    0.88600080  0.54273754   -0.19338002  5.246536e-01\n  expRating[155]    1.96583615  0.55095195    0.86955898  1.603554e+00\n  expRating[156]    4.07425745  0.54990176    2.97974925  3.707084e+00\n  expRating[157]    5.92928894  0.57732240    4.78109849  5.541198e+00\n  expRating[158]    4.05890622  0.57517380    2.91901672  3.674605e+00\n  expRating[159]    8.10290281  0.57558395    7.00087279  7.727224e+00\n  expRating[160]    3.94391582  0.54521398    2.91811054  3.574021e+00\n  expRating[161]    1.48946397  0.54825110    0.45128088  1.111869e+00\n  expRating[162]    2.50410675  0.54711103    1.44883328  2.124781e+00\n  expRating[163]    3.70578224  0.59592177    2.48809354  3.310089e+00\n  expRating[164]    2.69890432  0.59023248    1.44226884  2.308023e+00\n  expRating[165]    4.87251819  0.60697772    3.65337232  4.466282e+00\n  expRating[166]    3.86391372  0.53104778    2.95303625  3.463359e+00\n  expRating[167]   -0.29210232  0.53683374   -1.31225921 -6.706805e-01\n  expRating[168]    2.71626827  0.51610551    1.73048909  2.349628e+00\n  expRating[169]    3.17679308  0.57540854    2.09053506  2.773974e+00\n  expRating[170]    2.32105313  0.56155080    1.25990383  1.937969e+00\n  expRating[171]    4.98704678  0.56832358    3.89509783  4.589148e+00\n  expRating[172]    7.09415008  0.52292056    6.04359121  6.745437e+00\n  expRating[173]    8.10879286  0.52900707    7.03095989  7.758161e+00\n  expRating[174]    8.90440379  0.52969959    7.85427839  8.541963e+00\n  expRating[175]    3.62699460  0.57269245    2.51126919  3.229384e+00\n  expRating[176]    1.69141930  0.56013997    0.60409497  1.304650e+00\n  expRating[177]    4.35741295  0.55728785    3.28338727  3.978708e+00\n  expRating[178]    6.46141847  0.54854284    5.33585242  6.120224e+00\n  expRating[179]    9.76169456  0.56102344    8.62116193  9.399037e+00\n  expRating[180]    9.64967512  0.54347341    8.55220007  9.294565e+00\n  expRating[181]    2.75272590  0.55492139    1.73793854  2.348751e+00\n  expRating[182]    1.81104056  0.53355901    0.78993490  1.448655e+00\n  expRating[183]    2.88383584  0.54456042    1.85152145  2.508235e+00\n  expRating[184]    9.47030506  0.48619680    8.49553111  9.138593e+00\n  expRating[185]    9.90323622  0.49835517    8.86294914  9.562957e+00\n  expRating[186]    9.60141500  0.49725366    8.60249893  9.270182e+00\n  expRating[187]   10.64558019  0.47743511    9.73496937  1.030771e+01\n  expRating[188]    8.35994688  0.49067713    7.40595066  8.032104e+00\n  expRating[189]   10.53356075  0.50432489    9.55483364  1.019594e+01\n  expRating[190]    2.40492318  0.53040874    1.37400461  2.058216e+00\n  expRating[191]    1.46323783  0.53493412    0.47244475  1.081565e+00\n  expRating[192]    3.57165913  0.52137481    2.56041603  3.229002e+00\n  expRating[193]    6.92688932  0.55971431    5.81484279  6.572028e+00\n  expRating[194]    7.35982047  0.55452500    6.28606658  7.016429e+00\n  expRating[195]    8.20564471  0.55699001    7.15772564  7.829488e+00\n  expRating[196]    3.82933050  0.57440251    2.74839526  3.439790e+00\n  expRating[197]    2.82245258  0.57382811    1.72570431  2.424861e+00\n  expRating[198]    0.95206986  0.56197665   -0.15298934  5.705330e-01\n  expRating[199]    0.77430173  0.55606079   -0.27927221  3.971125e-01\n  expRating[200]    4.07457782  0.56479936    3.02786340  3.675678e+00\n  expRating[201]   -0.08143821  0.55812878   -1.11903566 -4.714514e-01\n  expRating[202]    3.71039890  0.58892487    2.57087751  3.336663e+00\n  expRating[203]    4.55622314  0.57720896    3.48530508  4.151568e+00\n  expRating[204]    0.40020710  0.59256382   -0.74028942 -7.123773e-03\n  expRating[205]    9.59961640  0.47477956    8.58389137  9.301968e+00\n  expRating[206]   10.03254755  0.46647346    9.06008723  9.707201e+00\n  expRating[207]    9.73072634  0.47053817    8.74756755  9.404103e+00\n  expRating[208]    7.08224379  0.57837632    5.97673902  6.696453e+00\n  expRating[209]    4.86180306  0.55431053    3.75909265  4.492443e+00\n  expRating[210]    5.59222140  0.56914971    4.52277997  5.223841e+00\n  expRating[211]    5.06412955  0.56467897    3.96892962  4.677226e+00\n  expRating[212]    5.49706071  0.57372486    4.33687996  5.095195e+00\n  expRating[213]    3.04260885  0.58123626    1.97631622  2.631368e+00\n  expRating[214]    4.95466541  0.54485728    3.87652185  4.577175e+00\n  expRating[215]    5.96930819  0.55322088    4.91141671  5.585446e+00\n  expRating[216]    6.76491911  0.54031662    5.72804476  6.409751e+00\n  expRating[217]    5.02327062  0.58418759    3.87246657  4.642391e+00\n  expRating[218]    2.73763731  0.59303414    1.59359947  2.324713e+00\n  expRating[219]    3.53324823  0.59796319    2.34953789  3.155517e+00\n  expRating[220]    8.35598332  0.57529283    7.19377665  7.997494e+00\n  expRating[221]    5.04579152  0.58213977    3.90000432  4.642199e+00\n  expRating[222]    7.71178517  0.59322477    6.57911650  7.312623e+00\n  expRating[223]    9.47133297  0.58310828    8.29702414  9.101006e+00\n  expRating[224]    7.01688112  0.57726305    5.90883430  6.628701e+00\n  expRating[225]    9.16951176  0.57728056    8.09557148  8.769414e+00\n  expRating[226]    5.32936059  0.56433341    4.25205945  4.939584e+00\n  expRating[227]    3.30783989  0.55192517    2.28491593  2.952482e+00\n  expRating[228]    4.32248267  0.54973560    3.26844131  3.964277e+00\n  expRating[229]    1.86051975  0.61667144    0.67602997  1.431059e+00\n  expRating[230]    2.87516253  0.60719306    1.78816194  2.438405e+00\n  expRating[231]    1.00477980  0.60372505   -0.08874054  5.719995e-01\n  expRating[232]    4.67325573  0.61427923    3.44971807  4.255594e+00\n  expRating[233]    5.95201112  0.59920882    4.72811258  5.562740e+00\n  expRating[234]    1.79599508  0.59128370    0.61138129  1.419580e+00\n  expRating[235]    7.08228339  0.53550701    6.03936618  6.700272e+00\n  expRating[236]    5.06076269  0.52824787    4.04781178  4.707085e+00\n  expRating[237]    6.07540547  0.53728600    5.03797061  5.721134e+00\n  expRating[238]    2.64479825  0.54363540    1.59714447  2.283252e+00\n  expRating[239]    4.79742889  0.54578062    3.63221896  4.437505e+00\n  expRating[240]    4.45505195  0.53906044    3.36722407  4.110769e+00\n  expRating[241]    4.71974857  0.56424538    3.61603597  4.326040e+00\n  expRating[242]    3.86400863  0.56620252    2.79389419  3.495235e+00\n  expRating[243]    6.87237921  0.55659212    5.75465267  6.516640e+00\n  expRating[244]    3.37488644  0.57911934    2.28641479  2.984963e+00\n  expRating[245]    2.36800852  0.58477753    1.29506491  1.962243e+00\n  expRating[246]    3.16361945  0.58880140    2.01387051  2.754596e+00\n  expRating[247]    5.58672950  0.57057751    4.46453302  5.180763e+00\n  expRating[248]    4.64504416  0.54831097    3.59450507  4.263576e+00\n  expRating[249]    5.37546250  0.55706052    4.26817664  5.010444e+00\n  expRating[250]    1.94410486  0.45578801    1.14317721  1.639320e+00\n  expRating[251]    2.37703602  0.46064225    1.63154530  2.031448e+00\n  expRating[252]   -0.07741584  0.45754911   -0.88923241 -3.982382e-01\n  expRating[253]    3.11404324  0.58160805    1.96446766  2.734156e+00\n  expRating[254]    6.12241382  0.56837701    4.96540945  5.747201e+00\n  expRating[255]    5.78003688  0.57447177    4.65940960  5.390610e+00\n  expRating[256]    6.49713524  0.53979978    5.47767670  6.135034e+00\n  expRating[257]    5.64139530  0.52938574    4.61855931  5.302862e+00\n  expRating[258]    8.30738894  0.54959038    7.23025211  7.920910e+00\n  expRating[259]    2.63473640  0.55608494    1.54573429  2.245070e+00\n  expRating[260]   -0.24252425  0.54437339   -1.25319006 -6.091567e-01\n  expRating[261]    2.42346940  0.56036484    1.37179605  2.050664e+00\n  expRating[262]    7.81526621  0.54252927    6.75110277  7.455152e+00\n  expRating[263]    7.88045878  0.55667400    6.72085406  7.534796e+00\n  expRating[264]    5.94488348  0.56397286    4.77225867  5.582047e+00\n  expRating[265]    5.96461099  0.58772392    4.78321191  5.577923e+00\n  expRating[266]    4.02903569  0.59279667    2.87710744  3.643001e+00\n  expRating[267]    6.69502934  0.59172128    5.55140622  6.302588e+00\n  expRating[268]    7.75219054  0.53301615    6.73736382  7.392772e+00\n  expRating[269]    5.88180782  0.53270380    4.83921435  5.505006e+00\n  expRating[270]    8.89017840  0.53394953    7.85959343  8.523262e+00\n  expRating[271]    3.89154884  0.58304556    2.76446760  3.513021e+00\n  expRating[272]    2.45173976  0.57287470    1.29456647  2.081387e+00\n  expRating[273]    2.51693234  0.56986581    1.40559733  2.150046e+00\n  expRating[274]    3.69378381  0.60789181    2.47393067  3.276450e+00\n  expRating[275]    4.12671496  0.60487974    2.96442689  3.721809e+00\n  expRating[276]    3.82489375  0.60109404    2.63906902  3.443846e+00\n  expRating[277]    4.77397688  0.56131490    3.66037359  4.389784e+00\n  expRating[278]    5.61980112  0.56717032    4.51907068  5.243286e+00\n  expRating[279]    4.47215567  0.56189507    3.32389091  4.105738e+00\n  expRating[280]    3.15320733  0.56949465    2.08553075  2.762700e+00\n  expRating[281]    4.43196272  0.56179545    3.35133365  4.000414e+00\n  expRating[282]    2.21152198  0.56327042    1.13179324  1.833400e+00\n  expRating[283]    8.46664313  0.53994488    7.40513679  8.095734e+00\n  expRating[284]    9.31246737  0.53798808    8.25186724  8.954892e+00\n  expRating[285]    8.16482191  0.53198853    7.09953683  7.810382e+00\n  expRating[286]    5.05924397  0.56825619    3.93220199  4.701037e+00\n  expRating[287]    3.68462747  0.55709208    2.58415832  3.299269e+00\n  expRating[288]    4.75742276  0.54887301    3.69439562  4.396953e+00\n  expRating[289]    6.23188175  0.59962216    5.08049037  5.823055e+00\n  expRating[290]    4.85726526  0.60460805    3.75280736  4.438375e+00\n  expRating[291]    5.93006054  0.62648811    4.72228930  5.525851e+00\n  expRating[292]    7.24619613  0.57519801    6.11355443  6.877445e+00\n  expRating[293]    5.87157963  0.56265057    4.64935156  5.510101e+00\n  expRating[294]    6.94437491  0.56080803    5.80549044  6.572620e+00\n  expRating[295]    5.16440844  0.58807239    3.95494321  4.775752e+00\n  expRating[296]    3.78979195  0.57925992    2.61252903  3.412077e+00\n  expRating[297]    4.86258723  0.59582135    3.64810474  4.490518e+00\n  expRating[298]    4.24812238  0.53963837    3.26966413  3.856877e+00\n  expRating[299]    5.32795773  0.53077306    4.31877737  4.943704e+00\n  expRating[300]    7.43637903  0.53722959    6.44883195  7.047319e+00\n  lp__           -102.34610784 10.03386086 -123.17080183 -1.087276e+02\n                stats\nparameter                  50%          75%        97.5%\n  ability[1]        5.55869300   5.95473389   6.60615927\n  ability[2]        7.96873734   8.36476733   9.15449952\n  ability[3]        6.78230706   7.13709757   7.89992529\n  ability[4]        7.05436756   7.46043534   8.24492070\n  ability[5]        3.06277038   3.44960001   4.17652697\n  ability[6]        4.53106980   4.95160015   5.70832678\n  ability[7]        8.97685064   9.31039752   9.89884236\n  ability[8]        4.15020128   4.58447594   5.25036933\n  ability[9]        2.79225893   3.20057637   3.98151826\n  ability[10]       8.80148664   9.23187410   9.87896916\n  ability[11]       5.63309814   6.03695290   6.84476224\n  ability[12]       6.61967622   7.01168908   7.78194737\n  ability[13]       1.60774281   1.93416918   2.60651677\n  ability[14]       9.34061179   9.63394766   9.95024000\n  ability[15]       5.79476944   6.19182222   6.92648807\n  ability[16]       6.67450433   7.05033148   7.78752823\n  ability[17]       5.18339646   5.53754897   6.27986808\n  ability[18]       1.90541146   2.27132248   2.99365431\n  ability[19]       2.58074313   2.97860240   3.74061318\n  ability[20]       4.77506452   5.17344183   5.94008910\n  ability[21]       6.84154607   7.17539072   7.97885148\n  ability[22]       7.58745017   7.98116371   8.79910058\n  ability[23]       3.74625889   4.16966401   4.89386900\n  ability[24]       8.68386554   9.07409415   9.74052299\n  ability[25]       2.08578747   2.48463084   3.22449045\n  ability[26]       5.90658176   6.29126006   6.99128469\n  ability[27]       2.58007881   3.01732939   3.69038353\n  ability[28]       2.10453007   2.47612851   3.22917112\n  ability[29]       5.06743551   5.42966774   6.21015362\n  ability[30]       8.30093651   8.72354215   9.50552252\n  ability[31]       8.73697605   9.15046855   9.76354578\n  ability[32]       2.32023659   2.72385847   3.43848763\n  ability[33]       9.03669250   9.41865375   9.92885873\n  ability[34]       7.25949167   7.68241659   8.57155175\n  ability[35]       5.46193704   5.86202309   6.61276015\n  ability[36]       5.85889038   6.24922762   7.03060593\n  ability[37]       3.97745178   4.33856367   5.05183339\n  ability[38]       1.98409588   2.37557860   3.16178190\n  ability[39]       2.22529132   2.62813364   3.36952986\n  ability[40]       1.87744741   2.27230461   3.01344953\n  ability[41]       7.71987786   8.16172531   8.83944309\n  ability[42]       7.06857711   7.46600952   8.29893015\n  ability[43]       8.50774748   8.90518078   9.55474137\n  ability[44]       7.65681475   8.04869205   8.69601452\n  ability[45]       5.89415045   6.25289633   6.98592181\n  ability[46]       2.69417500   3.08818934   3.87807907\n  ability[47]       9.21515830   9.54853139   9.90975312\n  ability[48]       7.86708219   8.24946065   8.94673465\n  ability[49]       2.83642537   3.22603880   3.89031510\n  ability[50]       5.61368531   5.96724196   6.74917635\n  ability[51]       1.56183098   1.92484604   2.59640122\n  ability[52]       2.58011485   2.94090400   3.72346049\n  ability[53]       6.57683607   7.00107239   7.75430152\n  ability[54]       3.14860377   3.55266179   4.24241632\n  ability[55]       3.35264950   3.77938305   4.59188321\n  ability[56]       2.23326977   2.61465198   3.36177174\n  ability[57]       4.85427027   5.27567433   5.99126980\n  ability[58]       8.78299002   9.14987218   9.74891199\n  ability[59]       4.21689419   4.60081514   5.47586300\n  ability[60]       8.12828020   8.51036001   9.27287952\n  ability[61]       2.41775773   2.77325072   3.55918155\n  ability[62]       9.17110321   9.48214797   9.91686639\n  ability[63]       9.04575984   9.41955347   9.87172337\n  ability[64]       2.04798653   2.42193941   3.18126892\n  ability[65]       6.58459279   6.97557768   7.75822804\n  ability[66]       3.44545023   3.89216127   4.68796278\n  ability[67]       2.43818081   2.84443834   3.56678110\n  ability[68]       2.90750740   3.32959068   4.08820912\n  ability[69]       9.31603255   9.61489298   9.95615648\n  ability[70]       5.43401078   5.85076169   6.66197656\n  ability[71]       4.69901502   5.13213414   5.92579834\n  ability[72]       6.62020395   7.01474570   7.75036250\n  ability[73]       3.39341453   3.84590531   4.59624268\n  ability[74]       7.60040884   7.95860240   8.78188498\n  ability[75]       8.70869366   9.10871595   9.81823319\n  ability[76]       4.97234159   5.37679260   6.15552595\n  ability[77]       3.54688593   3.95604607   4.74347693\n  ability[78]       4.33622371   4.76027341   5.51509870\n  ability[79]       6.71322035   7.12423072   7.84459083\n  ability[80]       4.33310494   4.66747168   5.46111268\n  ability[81]       6.37083899   6.81164509   7.51074204\n  ability[82]       3.01508707   3.42698336   4.22017410\n  ability[83]       5.22656782   5.64371648   6.31023773\n  ability[84]       1.54919504   1.84597434   2.54846578\n  ability[85]       5.65591007   6.05202091   6.81464633\n  ability[86]       8.16574547   8.55045169   9.26169704\n  ability[87]       2.29084496   2.70422519   3.40123903\n  ability[88]       8.51484624   8.87329722   9.56214602\n  ability[89]       6.57549216   7.00499647   7.79966164\n  ability[90]       8.41088409   8.77809478   9.47840119\n  ability[91]       3.08324936   3.50907616   4.28513575\n  ability[92]       3.32626029   3.78459138   4.60783197\n  ability[93]       3.98855984   4.37595266   5.18075904\n  ability[94]       2.81873318   3.21527963   3.96810406\n  ability[95]       7.69539387   8.07630478   8.76543834\n  ability[96]       4.28515545   4.69150293   5.39455971\n  ability[97]       5.46463545   5.85916098   6.70161641\n  ability[98]       6.49250533   6.84487755   7.57913005\n  ability[99]       4.40665094   4.78367854   5.61193257\n  ability[100]      5.91915796   6.31847839   6.93904514\n  severity[1]       0.34007578   0.53442966   0.92351311\n  severity[2]       0.76685138   0.96312616   1.33693316\n  severity[3]      -1.68009621  -1.47636464  -1.12392400\n  severity[4]       1.61404119   1.81751429   2.21213747\n  severity[5]      -0.67246682  -0.46463006  -0.05772079\n  severity[6]      -0.60104592  -0.41812548   0.03022786\n  severity[7]      -2.53901886  -2.32277719  -1.97482114\n  severity[8]       0.47003862   0.66464291   1.02796501\n  severity[9]       1.51442199   1.70922223   2.07284406\n  severity[10]      0.12609998   0.31888242   0.69352037\n  sigma             0.93485502   0.96824050   1.03265350\n  tau               1.37406695   1.67436978   2.48247919\n  expRating[1]      3.87524000   4.27612868   4.94913926\n  expRating[2]      7.18022843   7.57165734   8.27127976\n  expRating[3]      4.96871021   5.32436826   6.13460506\n  expRating[4]      8.32393789   8.72700801   9.42933343\n  expRating[5]      9.59080439   9.95713254  10.72737335\n  expRating[6]      7.36992872   7.75872177   8.48174767\n  expRating[7]      6.16946697   6.53257849   7.27717173\n  expRating[8]      7.24184532   7.60942720   8.31072388\n  expRating[9]      6.90389178   7.27982332   7.96488011\n  expRating[10]     7.84205588   8.24058726   8.96713051\n  expRating[11]     6.46324939   6.84759903   7.58171455\n  expRating[12]     4.54052211   4.92161011   5.61377725\n  expRating[13]     3.83931948   4.20761041   4.91961915\n  expRating[14]     4.67136411   5.04522003   5.81571888\n  expRating[15]     2.42402571   2.77797537   3.42640775\n  expRating[16]     6.16207415   6.53650479   7.22473012\n  expRating[17]     3.86034372   4.26526941   4.99458420\n  expRating[18]     4.66656119   5.04001078   5.73096790\n  expRating[19]     9.31097303   9.61960994  10.27613448\n  expRating[20]     7.28480033   7.61013206   8.28016322\n  expRating[21]     9.42604779   9.77863233  10.40280147\n  expRating[22]     2.48469973   2.88042638   3.56353996\n  expRating[23]     3.54414781   3.95157311   4.65749704\n  expRating[24]     5.65463048   6.05583317   6.75618440\n  expRating[25]     3.16378269   3.56273524   4.20148272\n  expRating[26]     1.12768828   1.54058354   2.18436809\n  expRating[27]     0.28121128   0.66380344   1.34352561\n  expRating[28]    10.42465973  10.82317166  11.45546169\n  expRating[29]     9.25885094   9.67999683  10.30840521\n  expRating[30]    10.31019157  10.72102395  11.37920020\n  expRating[31]     7.27299489   7.65562511   8.40256065\n  expRating[32]     7.15900537   7.52958365   8.27669621\n  expRating[33]     5.76643628   6.18690544   6.89980811\n  expRating[34]     7.40554433   7.77111663   8.49129669\n  expRating[35]     8.24199924   8.62817046   9.31983245\n  expRating[36]     5.95781877   6.34380568   7.03306180\n  expRating[37]     2.41349158   2.72605130   3.41308355\n  expRating[38]    -0.04297005   0.27475831   0.94952641\n  expRating[39]     2.10742605   2.43051868   3.13282252\n  expRating[40]     9.65075980   9.94835432  10.42782926\n  expRating[41]     8.70472797   9.01344077   9.50689068\n  expRating[42]     9.77407756  10.08157676  10.60174244\n  expRating[43]     6.10457937   6.49959856   7.26850547\n  expRating[44]     3.25804153   3.62078523   4.37568569\n  expRating[45]     7.30856376   7.65327911   8.39729856\n  expRating[46]     8.27473545   8.65689089   9.43348860\n  expRating[47]     6.05139160   6.42211426   7.23070648\n  expRating[48]     4.10703767   4.49013982   5.27710504\n  expRating[49]     5.95780520   6.31135466   7.04768073\n  expRating[50]     2.64050870   2.98896977   3.68263005\n  expRating[51]     5.30546043   5.66139999   6.36253199\n  expRating[52]     1.26650503   1.60892393   2.24949439\n  expRating[53]     3.42874597   3.75975359   4.42731782\n  expRating[54]     2.04768486   2.38999788   3.06337363\n  expRating[55]     4.24372563   4.57210652   5.25654983\n  expRating[56]     2.00892059   2.37799875   2.99141880\n  expRating[57]     0.08482467   0.42578991   1.07011183\n  expRating[58]     4.13622749   4.49876260   5.14170646\n  expRating[59]     4.20328983   4.56007050   5.22092039\n  expRating[60]     6.30383325   6.66097939   7.39735456\n  expRating[61]     7.58002987   7.93534580   8.71497330\n  expRating[62]     6.20960796   6.57823081   7.30916216\n  expRating[63]     7.29740695   7.65175372   8.26833270\n  expRating[64]     7.92156551   8.29927090   9.10407521\n  expRating[65]     9.20149150   9.62515025  10.36811686\n  expRating[66]     5.06275197   5.45950508   6.17994300\n  expRating[67]     2.06300910   2.45817922   3.19798675\n  expRating[68]     5.35946624   5.75312017   6.47203586\n  expRating[69]     4.22703408   4.60490074   5.38926290\n  expRating[70]     9.02382890   9.41252383  10.03815932\n  expRating[71]     7.02034326   7.37919723   8.09467112\n  expRating[72]    10.20181143  10.57056228  11.23417431\n  expRating[73]     2.86731235   3.25055588   3.95622810\n  expRating[74]     2.56929669   2.95616580   3.63679071\n  expRating[75]     3.60163590   3.97952364   4.70388681\n  expRating[76]     7.53169696   7.91751515   8.56917574\n  expRating[77]     5.30045340   5.69707721   6.43395010\n  expRating[78]     6.37194261   6.75902500   7.45168488\n  expRating[79]     0.92289143   1.31132028   1.98118378\n  expRating[80]     4.21455174   4.61890366   5.28072625\n  expRating[81]     4.09372431   4.47296012   5.22354393\n  expRating[82]     2.90417064   3.25360773   3.97899918\n  expRating[83]     2.57813624   2.98282779   3.67220488\n  expRating[84]     2.25748825   2.61091619   3.28931400\n  expRating[85]     4.36530447   4.76220707   5.52232973\n  expRating[86]     6.55002844   6.90539271   7.67659754\n  expRating[87]     5.16763644   5.54121645   6.23644450\n  expRating[88]     8.67059816   9.04771796   9.73394177\n  expRating[89]     7.73110344   8.13681334   8.81002459\n  expRating[90]     9.85491465  10.20363792  10.98861892\n  expRating[91]    10.36396554  10.74943526  11.39092671\n  expRating[92]    10.25242444  10.63154271  11.20080178\n  expRating[93]     8.89915495   9.23417075   9.88028620\n  expRating[94]     0.66029474   1.01784769   1.70837114\n  expRating[95]     1.75087448   2.10409897   2.76425518\n  expRating[96]     2.47612879   2.82151298   3.47744460\n  expRating[97]    10.65719945  11.02460446  11.55110303\n  expRating[98]     6.47583752   6.84702747   7.45779777\n  expRating[99]    10.53486803  10.86062809  11.47892836\n  expRating[100]    8.05033212   8.46317640   9.27246408\n  expRating[101]    7.73848245   8.14454289   9.00820435\n  expRating[102]    7.39953466   7.82702058   8.57460543\n  expRating[103]    4.80687448   5.18870782   5.86435148\n  expRating[104]    6.96944581   7.35327658   8.04727313\n  expRating[105]    5.60476358   6.00355371   6.68451843\n  expRating[106]    6.63033403   7.02179243   7.74896415\n  expRating[107]    5.25515205   5.63154670   6.40264297\n  expRating[108]    7.36210699   7.76297427   8.52611179\n  expRating[109]    4.31949725   4.67392055   5.39883789\n  expRating[110]    1.42962133   1.76847167   2.46911992\n  expRating[111]    5.48385600   5.81946526   6.51161729\n  expRating[112]    2.33703635   2.72948186   3.41664012\n  expRating[113]    1.35276525   1.69723918   2.45985772\n  expRating[114]   -0.54357704  -0.14422922   0.53852639\n  expRating[115]    3.85127946   4.21956956   4.94606603\n  expRating[116]    1.57411997   1.96875288   2.64779978\n  expRating[117]    3.74204814   4.13273305   4.77469733\n  expRating[118]    2.66980469   3.04376853   3.77745698\n  expRating[119]    0.19073065   0.57464696   1.32752435\n  expRating[120]    3.40515150   3.78267046   4.51516812\n  expRating[121]    8.51655208   8.89637589   9.59411529\n  expRating[122]    6.04483064   6.46591300   7.11093342\n  expRating[123]    7.87000367   8.25487573   8.91922428\n  expRating[124]    7.45021710   7.76293757   8.53322681\n  expRating[125]    6.40922308   6.80136298   7.47580621\n  expRating[126]    8.61543135   8.96077938   9.67586185\n  expRating[127]    6.83891673   7.21782894   7.83146346\n  expRating[128]    9.01670816   9.36490448  10.00951065\n  expRating[129]    8.64884743   9.03559207   9.66398886\n  expRating[130]    5.98554045   6.35178015   7.00904649\n  expRating[131]    9.15896558   9.52309960  10.23035622\n  expRating[132]    7.79066017   8.14671459   8.82711966\n  expRating[133]    7.52796681   7.85986427   8.50551494\n  expRating[134]    5.21742814   5.55824547   6.21281149\n  expRating[135]    6.36357426   6.72202408   7.45236977\n  expRating[136]    2.04353776   2.45611138   3.15608022\n  expRating[137]    2.11560449   2.50292034   3.18695537\n  expRating[138]    0.17526326   0.54519365   1.19987131\n  expRating[139]    9.53317933   9.84204326  10.37373707\n  expRating[140]    8.52851348   8.84714085   9.37423534\n  expRating[141]    9.68375093   9.97449851  10.53790625\n  expRating[142]    7.21196916   7.58796932   8.16313336\n  expRating[143]    9.37835721   9.76081442  10.44050170\n  expRating[144]    8.01391051   8.36416951   9.07378388\n  expRating[145]    4.47554464   4.82215362   5.49710704\n  expRating[146]    0.31840852   0.66818627   1.33352151\n  expRating[147]    2.96703720   3.31712193   4.00691050\n  expRating[148]    4.93123707   5.33519479   6.02727362\n  expRating[149]    3.09032561   3.46821060   4.17902946\n  expRating[150]    7.14008080   7.48965223   8.21403700\n  expRating[151]    1.95206477   2.27490271   2.98539013\n  expRating[152]    2.37044817   2.69982081   3.40833417\n  expRating[153]    3.12262374   3.44916449   4.05930567\n  expRating[154]    0.88534009   1.26334724   1.89853321\n  expRating[155]    1.99537090   2.32299877   2.98324431\n  expRating[156]    4.07410444   4.44060882   5.10903071\n  expRating[157]    5.93552806   6.31045423   7.06917317\n  expRating[158]    4.06836603   4.46705514   5.12050464\n  expRating[159]    8.10297468   8.49743963   9.21429816\n  expRating[160]    3.95617631   4.29571122   5.06635853\n  expRating[161]    1.48873867   1.84729152   2.58284383\n  expRating[162]    2.50149974   2.88162538   3.59687068\n  expRating[163]    3.69571167   4.11139732   4.93750622\n  expRating[164]    2.70361038   3.08410379   3.93864231\n  expRating[165]    4.88836486   5.26081028   6.09944143\n  expRating[166]    3.85285723   4.21934165   4.95270800\n  expRating[167]   -0.30280045   0.06021099   0.76057601\n  expRating[168]    2.70798956   3.05411099   3.77481587\n  expRating[169]    3.16151992   3.57416295   4.27509523\n  expRating[170]    2.31718135   2.70699484   3.41975755\n  expRating[171]    4.99708961   5.37320965   6.05972945\n  expRating[172]    7.10267716   7.46899277   8.10871822\n  expRating[173]    8.13403706   8.48998540   9.04941359\n  expRating[174]    8.92357403   9.27918643   9.87255414\n  expRating[175]    3.62574521   3.99407321   4.80177037\n  expRating[176]    1.69226704   2.05830242   2.91996355\n  expRating[177]    4.36091030   4.73240296   5.45559634\n  expRating[178]    6.45904396   6.79751957   7.55545003\n  expRating[179]    9.76539490  10.14078671  10.86438303\n  expRating[180]    9.63503262   9.99943725  10.77116842\n  expRating[181]    2.74422032   3.13867432   3.86754577\n  expRating[182]    1.80359462   2.17101268   2.87628089\n  expRating[183]    2.88450488   3.25387142   4.00098294\n  expRating[184]    9.49301475   9.82042841  10.31554553\n  expRating[185]    9.92736851  10.25917895  10.79932623\n  expRating[186]    9.62196297   9.98177974  10.48800018\n  expRating[187]   10.66874699  10.97606098  11.52363454\n  expRating[188]    8.36537991   8.71550043   9.27036928\n  expRating[189]   10.54649661  10.87091114  11.51348971\n  expRating[190]    2.39858852   2.74990914   3.45701329\n  expRating[191]    1.47661377   1.80666767   2.58559122\n  expRating[192]    3.57969150   3.90912603   4.58227076\n  expRating[193]    6.91168925   7.28689680   7.99990615\n  expRating[194]    7.33695203   7.74777249   8.46387769\n  expRating[195]    8.18988862   8.54933265   9.33615786\n  expRating[196]    3.81807613   4.21355510   5.03781168\n  expRating[197]    2.79319067   3.21906211   3.94515716\n  expRating[198]    0.92216578   1.34200653   2.07998091\n  expRating[199]    0.76965044   1.15384579   1.84740258\n  expRating[200]    4.07177799   4.44565326   5.22878671\n  expRating[201]   -0.08328384   0.28035109   1.01084379\n  expRating[202]    3.71803345   4.10765975   4.75860465\n  expRating[203]    4.56130447   4.94857466   5.66028111\n  expRating[204]    0.39055214   0.81195094   1.49404651\n  expRating[205]    9.63686839   9.92276459  10.42312270\n  expRating[206]   10.06715745  10.37774987  10.84944200\n  expRating[207]    9.76208722  10.08545864  10.53344019\n  expRating[208]    7.06287768   7.49160584   8.22726766\n  expRating[209]    4.85046571   5.23624832   5.93646736\n  expRating[210]    5.58720485   5.95310078   6.70985330\n  expRating[211]    5.05523368   5.46420219   6.17000925\n  expRating[212]    5.49813724   5.89068386   6.60376698\n  expRating[213]    3.02186746   3.44468720   4.21855980\n  expRating[214]    4.94609158   5.31531722   6.04174543\n  expRating[215]    5.95288185   6.32471400   7.07832766\n  expRating[216]    6.75559979   7.12948247   7.86073012\n  expRating[217]    5.02721490   5.41927140   6.16458684\n  expRating[218]    2.74312091   3.16843337   3.91298088\n  expRating[219]    3.54212748   3.93730449   4.70051595\n  expRating[220]    8.36100039   8.73587646   9.49620073\n  expRating[221]    5.06518927   5.45070319   6.11426716\n  expRating[222]    7.71126013   8.10163694   8.86448840\n  expRating[223]    9.47499458   9.84849137  10.62990688\n  expRating[224]    7.04014456   7.40904459   8.10411649\n  expRating[225]    9.17113033   9.56881076  10.25853962\n  expRating[226]    5.32512910   5.71516917   6.49352860\n  expRating[227]    3.32140749   3.65409402   4.48307125\n  expRating[228]    4.32401012   4.67708478   5.45157915\n  expRating[229]    1.86060831   2.29377388   3.08697070\n  expRating[230]    2.85176665   3.29160337   4.06768097\n  expRating[231]    0.98262586   1.43569175   2.16295008\n  expRating[232]    4.68071295   5.07530652   5.89531552\n  expRating[233]    5.92898492   6.36205259   7.05029600\n  expRating[234]    1.80208070   2.19136736   2.99235016\n  expRating[235]    7.08476881   7.43384058   8.18433898\n  expRating[236]    5.02546790   5.42321305   6.14058722\n  expRating[237]    6.05489252   6.43793217   7.14830850\n  expRating[238]    2.63401432   3.00585121   3.80794859\n  expRating[239]    4.79387737   5.15588476   5.88451099\n  expRating[240]    4.43448833   4.81113662   5.56186014\n  expRating[241]    4.73279288   5.08522850   5.83923608\n  expRating[242]    3.86842644   4.24212316   4.99634914\n  expRating[243]    6.88616957   7.24231582   7.94646149\n  expRating[244]    3.36097018   3.75097044   4.50888467\n  expRating[245]    2.36213254   2.77005542   3.52500100\n  expRating[246]    3.16282630   3.55495536   4.34812330\n  expRating[247]    5.58475626   5.97472638   6.65588723\n  expRating[248]    4.63869570   5.02067561   5.72284793\n  expRating[249]    5.35863567   5.77299307   6.43125299\n  expRating[250]    1.91579694   2.21821095   2.90346263\n  expRating[251]    2.33776209   2.68257443   3.33251589\n  expRating[252]   -0.10914638   0.22828130   0.88923559\n  expRating[253]    3.11851937   3.50188699   4.23543386\n  expRating[254]    6.12399545   6.52187456   7.19944687\n  expRating[255]    5.78947786   6.15354326   6.90307835\n  expRating[256]    6.48748799   6.85333643   7.58431247\n  expRating[257]    5.62850365   5.99817709   6.60405397\n  expRating[258]    8.30100198   8.67896149   9.40989790\n  expRating[259]    2.63499366   3.00737593   3.72408084\n  expRating[260]   -0.24145773   0.12563415   0.84591763\n  expRating[261]    2.40053028   2.81928401   3.52430250\n  expRating[262]    7.83267774   8.17390174   8.87666969\n  expRating[263]    7.89654964   8.26098195   8.94596020\n  expRating[264]    5.95934959   6.30886716   7.11759526\n  expRating[265]    5.96217294   6.34519408   7.16994059\n  expRating[266]    4.02963384   4.41308973   5.17028204\n  expRating[267]    6.67898910   7.09813799   7.87707800\n  expRating[268]    7.74435546   8.13240603   8.78197318\n  expRating[269]    5.89433057   6.23783771   6.90306033\n  expRating[270]    8.90166260   9.25146158   9.91695585\n  expRating[271]    3.88333722   4.32191867   4.98415364\n  expRating[272]    2.45075925   2.82683080   3.55407090\n  expRating[273]    2.50429283   2.90098104   3.58393983\n  expRating[274]    3.68189983   4.09734588   4.91562786\n  expRating[275]    4.11453520   4.53232539   5.30726152\n  expRating[276]    3.80467510   4.23496358   5.02136889\n  expRating[277]    4.78192968   5.16075440   5.87807945\n  expRating[278]    5.60337497   5.99656093   6.71655403\n  expRating[279]    4.46572384   4.85943157   5.56391874\n  expRating[280]    3.14374189   3.54386476   4.26225355\n  expRating[281]    4.43401944   4.83534635   5.49746873\n  expRating[282]    2.19082392   2.59621113   3.31505192\n  expRating[283]    8.47461783   8.82116704   9.49157957\n  expRating[284]    9.30715936   9.67755521  10.39531939\n  expRating[285]    8.17701172   8.51968297   9.19841356\n  expRating[286]    5.06452130   5.43314310   6.16450967\n  expRating[287]    3.67164260   4.07349600   4.78847064\n  expRating[288]    4.73966260   5.11142685   5.87975517\n  expRating[289]    6.24725803   6.63631237   7.39408537\n  expRating[290]    4.84163766   5.23727893   6.09896743\n  expRating[291]    5.94643467   6.34901739   7.20227187\n  expRating[292]    7.26116845   7.63096434   8.32843532\n  expRating[293]    5.88085986   6.23383088   6.99569912\n  expRating[294]    6.95754974   7.30725293   7.99832747\n  expRating[295]    5.17913807   5.56381837   6.32444225\n  expRating[296]    3.82101644   4.16496947   4.90898598\n  expRating[297]    4.86940920   5.25917107   5.98552137\n  expRating[298]    4.24063526   4.60675294   5.28379300\n  expRating[299]    5.31156694   5.68647592   6.38514470\n  expRating[300]    7.43678225   7.81082384   8.49831360\n  lp__           -101.69745714 -95.21571581 -85.16344394\n\n, , chains = chain:5\n\n                stats\nparameter                 mean         sd          2.5%           25%\n  ability[1]        5.63085498 0.62577440    4.45833616  5.191328e+00\n  ability[2]        8.00990457 0.62577631    6.82785158  7.581469e+00\n  ability[3]        6.81192366 0.59008474    5.63292561  6.418709e+00\n  ability[4]        7.10084421 0.60498849    5.90130567  6.693003e+00\n  ability[5]        3.11290333 0.59294768    1.98790558  2.719250e+00\n  ability[6]        4.57321669 0.61754056    3.34433658  4.162983e+00\n  ability[7]        8.93107872 0.56413373    7.73052630  8.545275e+00\n  ability[8]        4.23919936 0.56057303    3.08016803  3.863157e+00\n  ability[9]        2.85613291 0.57748081    1.80335277  2.461798e+00\n  ability[10]       8.80967403 0.51788705    7.79595326  8.439074e+00\n  ability[11]       5.66759736 0.56381818    4.58365313  5.280788e+00\n  ability[12]       6.66482188 0.60923242    5.41365299  6.248712e+00\n  ability[13]       1.67296927 0.45278662    1.04842098  1.312893e+00\n  ability[14]       9.31014018 0.45054919    8.33877795  9.042346e+00\n  ability[15]       5.79970075 0.59696348    4.71588499  5.375687e+00\n  ability[16]       6.65766049 0.56999087    5.57710321  6.274125e+00\n  ability[17]       5.20269807 0.60570041    4.01709901  4.833955e+00\n  ability[18]       1.94315661 0.52847129    1.07820807  1.552111e+00\n  ability[19]       2.65279741 0.60222533    1.46480322  2.236093e+00\n  ability[20]       4.83361212 0.60410889    3.71741597  4.417989e+00\n  ability[21]       6.86464568 0.61420608    5.69201763  6.467479e+00\n  ability[22]       7.63835207 0.58136550    6.49403214  7.232082e+00\n  ability[23]       3.76736472 0.60951751    2.55579405  3.335521e+00\n  ability[24]       8.71930026 0.55750246    7.60303996  8.373844e+00\n  ability[25]       2.13533294 0.53946259    1.17786695  1.754776e+00\n  ability[26]       5.94994502 0.56387139    4.80779071  5.569084e+00\n  ability[27]       2.60969622 0.63002361    1.41814679  2.166497e+00\n  ability[28]       2.16524887 0.50902043    1.25345544  1.785355e+00\n  ability[29]       5.06634375 0.62429219    3.82130081  4.696553e+00\n  ability[30]       8.39178478 0.59629533    7.22951626  7.989967e+00\n  ability[31]       8.80504691 0.56813525    7.64284382  8.441000e+00\n  ability[32]       2.36533442 0.55627043    1.30881137  1.972327e+00\n  ability[33]       9.01753194 0.54505777    7.80985562  8.649165e+00\n  ability[34]       7.31409800 0.58977274    6.10371218  6.931072e+00\n  ability[35]       5.50017868 0.60021618    4.37171416  5.081118e+00\n  ability[36]       5.87600952 0.58635167    4.71363726  5.510590e+00\n  ability[37]       4.00506444 0.58903711    2.84251270  3.615215e+00\n  ability[38]       2.02000285 0.55916692    1.04190536  1.620041e+00\n  ability[39]       2.26795296 0.58286538    1.20337887  1.850160e+00\n  ability[40]       1.96724142 0.49106091    1.09779660  1.612023e+00\n  ability[41]       7.75902518 0.54686954    6.68343315  7.406682e+00\n  ability[42]       7.12079054 0.58567301    5.96390444  6.728334e+00\n  ability[43]       8.52835432 0.55585176    7.37665092  8.163543e+00\n  ability[44]       7.70644208 0.59372743    6.48438530  7.292825e+00\n  ability[45]       5.91018851 0.55748178    4.79937387  5.541019e+00\n  ability[46]       2.76753634 0.59855888    1.59137728  2.364251e+00\n  ability[47]       9.20637316 0.48898595    8.14106724  8.880937e+00\n  ability[48]       7.92077999 0.60109653    6.75265066  7.516174e+00\n  ability[49]       2.91962984 0.58411393    1.75475244  2.542335e+00\n  ability[50]       5.66314396 0.64435794    4.44352143  5.217142e+00\n  ability[51]       1.64969241 0.43328015    1.05183353  1.320828e+00\n  ability[52]       2.63090022 0.57213785    1.61191713  2.219353e+00\n  ability[53]       6.63866354 0.58398591    5.45401398  6.235250e+00\n  ability[54]       3.21129496 0.59473711    2.11914555  2.781751e+00\n  ability[55]       3.40030673 0.56841837    2.34138508  3.016553e+00\n  ability[56]       2.27770244 0.57489960    1.23921109  1.872064e+00\n  ability[57]       4.91553058 0.59833477    3.71556828  4.522145e+00\n  ability[58]       8.80685590 0.53961844    7.68086384  8.439182e+00\n  ability[59]       4.29326611 0.61298520    3.13531234  3.858923e+00\n  ability[60]       8.18737538 0.59501916    7.00716659  7.778925e+00\n  ability[61]       2.47649683 0.59226862    1.23468673  2.065900e+00\n  ability[62]       9.16550258 0.48310450    8.14292330  8.839614e+00\n  ability[63]       9.03587005 0.50446312    7.94448669  8.694203e+00\n  ability[64]       2.09397087 0.53667380    1.12264763  1.701072e+00\n  ability[65]       6.59516907 0.59334556    5.35646337  6.203679e+00\n  ability[66]       3.53128494 0.58706431    2.39306275  3.140438e+00\n  ability[67]       2.47634059 0.55964117    1.43754318  2.087941e+00\n  ability[68]       2.99843788 0.56046097    1.91663579  2.617984e+00\n  ability[69]       9.27509782 0.44224295    8.26867616  9.003061e+00\n  ability[70]       5.50604062 0.58771620    4.36311415  5.113160e+00\n  ability[71]       4.72836675 0.58950964    3.62199196  4.307458e+00\n  ability[72]       6.67566125 0.63616840    5.36530423  6.245417e+00\n  ability[73]       3.45801516 0.57822784    2.36861247  3.050435e+00\n  ability[74]       7.63851295 0.59628760    6.51225452  7.228412e+00\n  ability[75]       8.70610769 0.54788426    7.60566409  8.335094e+00\n  ability[76]       5.02466088 0.60251663    3.85107228  4.616649e+00\n  ability[77]       3.59165030 0.62289310    2.33875450  3.166484e+00\n  ability[78]       4.35623696 0.63021807    3.20094783  3.906824e+00\n  ability[79]       6.79365316 0.61933039    5.66613834  6.374055e+00\n  ability[80]       4.34464589 0.61090213    3.09045556  3.931410e+00\n  ability[81]       6.43223700 0.55586282    5.34595689  6.040444e+00\n  ability[82]       3.08377648 0.55467729    2.06896468  2.710262e+00\n  ability[83]       5.29986016 0.58439580    4.22436555  4.915147e+00\n  ability[84]       1.65401194 0.41711883    1.04873079  1.330198e+00\n  ability[85]       5.67244275 0.58530477    4.52568000  5.288169e+00\n  ability[86]       8.19653608 0.59326238    7.01567857  7.822483e+00\n  ability[87]       2.36159217 0.57280019    1.31721727  1.929164e+00\n  ability[88]       8.55048266 0.61410970    7.22559930  8.153546e+00\n  ability[89]       6.60974761 0.57444122    5.44249191  6.246730e+00\n  ability[90]       8.44802607 0.54996156    7.39178935  8.071959e+00\n  ability[91]       3.15715534 0.60804203    1.96619855  2.731558e+00\n  ability[92]       3.37828780 0.60872774    2.13794369  2.958523e+00\n  ability[93]       4.01574002 0.58129906    2.85144741  3.636372e+00\n  ability[94]       2.84423706 0.60913524    1.57948960  2.468225e+00\n  ability[95]       7.76880374 0.57119547    6.65709298  7.382618e+00\n  ability[96]       4.33264443 0.59826973    3.16311209  3.934734e+00\n  ability[97]       5.51031707 0.59265837    4.36016053  5.120288e+00\n  ability[98]       6.50208108 0.58498585    5.33595070  6.111840e+00\n  ability[99]       4.43455244 0.62711701    3.14858284  4.035323e+00\n  ability[100]      5.95932186 0.58489519    4.84260396  5.552279e+00\n  severity[1]       0.30433614 0.28530423   -0.23876180  9.268321e-02\n  severity[2]       0.74181713 0.27483178    0.23170285  5.577973e-01\n  severity[3]      -1.70426228 0.28764369   -2.26299264 -1.897556e+00\n  severity[4]       1.59130106 0.29476759    1.00514206  1.391979e+00\n  severity[5]      -0.71202375 0.29713523   -1.33150577 -9.070249e-01\n  severity[6]      -0.66001487 0.30760313   -1.27246837 -8.735068e-01\n  severity[7]      -2.56425188 0.30082622   -3.15362172 -2.777774e+00\n  severity[8]       0.44449684 0.28453416   -0.09262229  2.552341e-01\n  severity[9]       1.47202884 0.29495608    0.89416000  1.271143e+00\n  severity[10]      0.08561025 0.29265078   -0.47828136 -1.206183e-01\n  sigma             0.93474514 0.04827199    0.84507784  9.006255e-01\n  tau               1.48431579 0.40064607    0.91055402  1.203762e+00\n  expRating[1]      3.92659270 0.58638508    2.73466380  3.528002e+00\n  expRating[2]      7.22215603 0.58849203    6.05928929  6.815568e+00\n  expRating[3]      4.97084011 0.59149000    3.83340420  4.547495e+00\n  expRating[4]      8.31424071 0.58512224    7.15146530  7.956947e+00\n  expRating[5]      9.60120563 0.59416907    8.43779985  9.196497e+00\n  expRating[6]      7.34988970 0.58880936    6.15059125  6.967304e+00\n  expRating[7]      6.15190879 0.56212710    5.04402366  5.761418e+00\n  expRating[8]      7.25642051 0.55649688    6.19237526  6.904712e+00\n  expRating[9]      6.89753392 0.57959707    5.80554369  6.510298e+00\n  expRating[10]     7.84266133 0.58065613    6.67407478  7.463079e+00\n  expRating[11]     6.44082934 0.57846523    5.26245873  6.052421e+00\n  expRating[12]     4.53659232 0.57641421    3.42933480  4.169703e+00\n  expRating[13]     3.85472046 0.54711369    2.80456656  3.503552e+00\n  expRating[14]     4.70420439 0.55878965    3.58041244  4.343424e+00\n  expRating[15]     2.40087958 0.55737607    1.33917598  2.036444e+00\n  expRating[16]     6.16451775 0.57986476    5.07732968  5.755788e+00\n  expRating[17]     3.86119294 0.57040801    2.78738530  3.463957e+00\n  expRating[18]     4.65882694 0.57789575    3.56820852  4.242032e+00\n  expRating[19]     9.23541485 0.56884331    8.10474259  8.820692e+00\n  expRating[20]     7.22681644 0.56250879    6.02970992  6.865190e+00\n  expRating[21]     9.37557556 0.55099932    8.22112643  9.010146e+00\n  expRating[22]     2.53493708 0.52959145    1.45689446  2.190871e+00\n  expRating[23]     3.57918449 0.53145978    2.54582069  3.237536e+00\n  expRating[24]     5.71122820 0.53786436    4.67081977  5.354737e+00\n  expRating[25]     3.16046904 0.53871429    2.13137985  2.804064e+00\n  expRating[26]     1.15187063 0.56259929    0.06877140  7.604125e-01\n  expRating[27]     0.29188102 0.55103555   -0.75637354 -8.065111e-02\n  expRating[28]    10.40097508 0.51363636    9.43600015  1.005046e+01\n  expRating[29]     9.25417087 0.50340346    8.27248001  8.904652e+00\n  expRating[30]    10.28170287 0.51866274    9.28508229  9.929934e+00\n  expRating[31]     7.25889841 0.55184321    6.20213531  6.892589e+00\n  expRating[32]     7.13962620 0.53252080    6.07230899  6.787129e+00\n  expRating[33]     5.75320761 0.54045488    4.65037861  5.387953e+00\n  expRating[34]     7.40663901 0.56358401    6.25402507  7.034219e+00\n  expRating[35]     8.25612294 0.56779533    7.11976182  7.876606e+00\n  expRating[36]     5.95279813 0.56988787    4.74112465  5.584315e+00\n  expRating[37]     2.41478640 0.46719427    1.60839076  2.071991e+00\n  expRating[38]    -0.03129300 0.47264199   -0.85549910 -3.601023e-01\n  expRating[39]     2.11746612 0.47571827    1.25234664  1.774766e+00\n  expRating[40]     9.61447631 0.46053697    8.65426657  9.339369e+00\n  expRating[41]     8.65012531 0.47764441    7.63881771  8.349779e+00\n  expRating[42]     9.75463702 0.45789350    8.76069475  9.477034e+00\n  expRating[43]     6.10403688 0.56852655    5.03760814  5.699471e+00\n  expRating[44]     3.23544886 0.56411347    2.19829189  2.831506e+00\n  expRating[45]     7.27172959 0.55998710    6.18088477  6.898305e+00\n  expRating[46]     8.24896155 0.54268432    7.17131510  7.861483e+00\n  expRating[47]     5.99764562 0.52788851    4.96686613  5.614005e+00\n  expRating[48]     4.09340861 0.54473458    3.00817806  3.717858e+00\n  expRating[49]     5.94451519 0.56901920    4.85896514  5.560165e+00\n  expRating[50]     2.63844618 0.57964406    1.50785464  2.257993e+00\n  expRating[51]     5.28830832 0.57245925    4.15875743  4.918124e+00\n  expRating[52]     1.23113286 0.51548645    0.32373644  8.525085e-01\n  expRating[53]     3.41518546 0.52445459    2.52215751  3.049964e+00\n  expRating[54]     2.02876687 0.51354551    1.13462363  1.654614e+00\n  expRating[55]     4.24409847 0.55732626    3.16860472  3.878033e+00\n  expRating[56]     1.99278254 0.58068066    0.87311836  1.609020e+00\n  expRating[57]     0.08854553 0.56752273   -1.04929537 -2.707547e-01\n  expRating[58]     4.12158837 0.57043287    2.97022410  3.729166e+00\n  expRating[59]     4.17359725 0.56185985    3.09415779  3.812914e+00\n  expRating[60]     6.30564097 0.57056184    5.16661724  5.928259e+00\n  expRating[61]     7.60646281 0.57620152    6.46958709  7.230067e+00\n  expRating[62]     6.20463081 0.58850191    5.06963632  5.838210e+00\n  expRating[63]     7.30914253 0.58548945    6.13466848  6.924479e+00\n  expRating[64]     7.94268821 0.55328280    6.87851912  7.571122e+00\n  expRating[65]     9.22965313 0.54560251    8.15203650  8.861375e+00\n  expRating[66]     5.07410019 0.53895395    4.06901531  4.707338e+00\n  expRating[67]     2.06310244 0.59407163    0.93623326  1.645860e+00\n  expRating[68]     5.35866578 0.58517753    4.22853187  4.939868e+00\n  expRating[69]     4.21186156 0.58483929    3.03927659  3.806766e+00\n  expRating[70]     9.02363640 0.54783753    7.94073660  8.643973e+00\n  expRating[71]     7.01503798 0.54058806    5.94034462  6.635062e+00\n  expRating[72]    10.19132910 0.54513262    9.07073718  9.806885e+00\n  expRating[73]     2.87715007 0.52235820    1.93604662  2.514140e+00\n  expRating[74]     2.57982978 0.52607126    1.62058017  2.202895e+00\n  expRating[75]     3.60736178 0.54151680    2.60815579  3.224991e+00\n  expRating[76]     7.54124608 0.54164744    6.44974644  7.190574e+00\n  expRating[77]     5.28993015 0.53768803    4.20732106  4.942321e+00\n  expRating[78]     6.39444186 0.55151988    5.34193321  6.016358e+00\n  expRating[79]     0.90543394 0.59030438   -0.19621975  5.102362e-01\n  expRating[80]     4.20099728 0.58125781    3.10222002  3.808119e+00\n  expRating[81]     4.08172506 0.58610473    2.97815363  3.698303e+00\n  expRating[82]     2.90706600 0.49995108    1.99816600  2.551139e+00\n  expRating[83]     2.60974572 0.50109939    1.62414737  2.254104e+00\n  expRating[84]     2.25085913 0.51221407    1.28287873  1.883468e+00\n  expRating[85]     4.35432000 0.58701873    3.17990620  3.971959e+00\n  expRating[86]     6.53837260 0.58971272    5.34443943  6.144914e+00\n  expRating[87]     5.15195401 0.58851024    4.02112400  4.763115e+00\n  expRating[88]     8.69612091 0.56189014    7.62007230  8.329864e+00\n  expRating[89]     7.73176991 0.55316868    6.63815821  7.352659e+00\n  expRating[90]     9.86381362 0.55724005    8.75907942  9.511830e+00\n  expRating[91]    10.39634797 0.54984707    9.29646882  1.003734e+01\n  expRating[92]    10.27707575 0.54239655    9.23582758  9.911943e+00\n  expRating[93]     8.89065716 0.54205407    7.76810225  8.549108e+00\n  expRating[94]     0.66107214 0.53133884   -0.34275624  2.798550e-01\n  expRating[95]     1.70531955 0.52597643    0.67249718  1.343965e+00\n  expRating[96]     2.45094467 0.52234375    1.48618969  2.079990e+00\n  expRating[97]    10.60883299 0.52561671    9.58625851  1.024946e+01\n  expRating[98]     6.45328006 0.52296344    5.37350732  6.117475e+00\n  expRating[99]    10.48956078 0.51830878    9.37022173  1.018288e+01\n  expRating[100]    8.05591513 0.55761329    6.93785636  7.675867e+00\n  expRating[101]    7.75859485 0.54201851    6.69556533  7.414994e+00\n  expRating[102]    7.39970826 0.55182416    6.32505931  7.022998e+00\n  expRating[103]    4.78815493 0.57948740    3.71058704  4.375102e+00\n  expRating[104]    6.97220753 0.58296780    5.84140483  6.567284e+00\n  expRating[105]    5.58578894 0.56196415    4.49695929  5.209752e+00\n  expRating[106]    6.61782665 0.55587701    5.51710520  6.276451e+00\n  expRating[107]    5.21599465 0.55889162    4.11882477  4.847904e+00\n  expRating[108]    7.34803836 0.55933693    6.25742681  6.990539e+00\n  expRating[109]    4.30940057 0.55714255    3.22513220  3.946552e+00\n  expRating[110]    1.44081256 0.55576462    0.38198088  1.053133e+00\n  expRating[111]    5.47709328 0.56629655    4.42008544  5.083747e+00\n  expRating[112]    2.32433899 0.53610836    1.34666169  1.923723e+00\n  expRating[113]    1.30797910 0.55314297    0.30630873  9.092246e-01\n  expRating[114]   -0.54424903 0.53762849   -1.51017537 -9.091030e-01\n  expRating[115]    3.85925401 0.56016347    2.79102510  3.476219e+00\n  expRating[116]    1.55592921 0.56680020    0.48068659  1.173370e+00\n  expRating[117]    3.73998180 0.55806658    2.69487488  3.354615e+00\n  expRating[118]    2.70905855 0.48610310    1.80729184  2.371657e+00\n  expRating[119]    0.26297914 0.49694784   -0.60638770 -1.071236e-01\n  expRating[120]    3.43927026 0.49180247    2.52017484  3.098138e+00\n  expRating[121]    8.50084230 0.52476229    7.50054584  8.138025e+00\n  expRating[122]    6.05476290 0.52723329    5.01449171  5.697064e+00\n  expRating[123]    7.84463543 0.52710271    6.79221400  7.487493e+00\n  expRating[124]    7.42512667 0.55470999    6.35988006  7.043429e+00\n  expRating[125]    6.40876679 0.54897351    5.32279330  6.040213e+00\n  expRating[126]    8.59281938 0.55569293    7.53799563  8.239798e+00\n  expRating[127]    6.82409204 0.53064916    5.75936436  6.457049e+00\n  expRating[128]    8.97285116 0.53001072    7.99153504  8.601173e+00\n  expRating[129]    8.61396457 0.52120276    7.58600237  8.240912e+00\n  expRating[130]    6.00217981 0.56123441    4.88855315  5.639226e+00\n  expRating[131]    9.17847092 0.56448176    8.04372295  8.789578e+00\n  expRating[132]    7.79205233 0.56281130    6.71322036  7.404001e+00\n  expRating[133]    7.50148957 0.53062584    6.45821696  7.155172e+00\n  expRating[134]    5.19816476 0.53339345    4.17547622  4.832105e+00\n  expRating[135]    6.35468535 0.52446049    5.30919369  6.013653e+00\n  expRating[136]    2.05551259 0.54959475    1.02445070  1.676937e+00\n  expRating[137]    2.10752147 0.56398499    1.06850422  1.714445e+00\n  expRating[138]    0.20328445 0.55447494   -0.86541266 -1.844695e-01\n  expRating[139]    9.51070930 0.49649858    8.47933152  9.167811e+00\n  expRating[140]    8.49434941 0.50458262    7.47956188  8.143455e+00\n  expRating[141]    9.65087000 0.49835788    8.58101656  9.309723e+00\n  expRating[142]    7.20875624 0.56467980    6.09538196  6.814154e+00\n  expRating[143]    9.39280883 0.56123402    8.28540017  9.018131e+00\n  expRating[144]    8.00639024 0.55340026    6.91442192  7.621721e+00\n  expRating[145]    4.51093089 0.55505848    3.40411624  4.156634e+00\n  expRating[146]    0.35537795 0.54667584   -0.75387806  9.053911e-03\n  expRating[147]    3.00524009 0.55121444    1.93500286  2.647314e+00\n  expRating[148]    4.95112021 0.61016889    3.79144303  4.525855e+00\n  expRating[149]    3.09889208 0.61338042    1.96050689  2.645309e+00\n  expRating[150]    7.13517280 0.60371481    5.96283717  6.725145e+00\n  expRating[151]    1.95402855 0.44251144    1.21200963  1.639999e+00\n  expRating[152]    2.39150954 0.45506267    1.59541217  2.073356e+00\n  expRating[153]    3.12172126 0.45535738    2.31534354  2.801816e+00\n  expRating[154]    0.92663794 0.53925448   -0.11534283  5.679006e-01\n  expRating[155]    1.97088535 0.56244699    0.84103920  1.597817e+00\n  expRating[156]    4.10292906 0.54593428    3.03051796  3.732579e+00\n  expRating[157]    5.92663978 0.55955302    4.83530476  5.576239e+00\n  expRating[158]    4.07441165 0.55292751    2.97420426  3.673915e+00\n  expRating[159]    8.11069238 0.56606777    7.00304404  7.721069e+00\n  expRating[160]    3.95311209 0.57059385    2.88290360  3.568154e+00\n  expRating[161]    1.50703268 0.56267487    0.37699340  1.144069e+00\n  expRating[162]    2.49927121 0.58987599    1.38428086  2.079404e+00\n  expRating[163]    3.70464287 0.54712818    2.66476084  3.324356e+00\n  expRating[164]    2.68828298 0.55052751    1.63787879  2.298123e+00\n  expRating[165]    4.87233558 0.56893981    3.78567780  4.473820e+00\n  expRating[166]    3.86900349 0.54815299    2.84525151  3.468762e+00\n  expRating[167]   -0.28654944 0.56195142   -1.35922260 -6.543814e-01\n  expRating[168]    2.72219928 0.54711215    1.69393228  2.333514e+00\n  expRating[169]    3.21126831 0.55581207    2.11925804  2.820104e+00\n  expRating[170]    2.35127870 0.55716628    1.31626313  1.947735e+00\n  expRating[171]    5.00114083 0.55424364    3.92118235  4.621075e+00\n  expRating[172]    7.10259363 0.51895237    6.07251926  6.774915e+00\n  expRating[173]    8.09483215 0.53479618    6.95685461  7.744479e+00\n  expRating[174]    8.89246616 0.51898867    7.85056175  8.569532e+00\n  expRating[175]    3.63325124 0.56742458    2.55024828  3.252703e+00\n  expRating[176]    1.72901423 0.57608918    0.65803646  1.324943e+00\n  expRating[177]    4.37887636 0.56241721    3.30375610  3.986972e+00\n  expRating[178]    6.48311310 0.55761335    5.40285002  6.111100e+00\n  expRating[179]    9.77867644 0.55451543    8.65722168  9.428170e+00\n  expRating[180]    9.65940422 0.55138805    8.58459877  9.258481e+00\n  expRating[181]    2.78083296 0.54610570    1.71343013  2.430522e+00\n  expRating[182]    1.81648196 0.55570763    0.68609506  1.439010e+00\n  expRating[183]    2.92099367 0.55405873    1.86222478  2.539677e+00\n  expRating[184]    9.46983872 0.48493009    8.44380247  9.157881e+00\n  expRating[185]    9.90731971 0.48619373    8.88958725  9.576610e+00\n  expRating[186]    9.60999942 0.49960827    8.55103319  9.271368e+00\n  expRating[187]   10.62717110 0.49297357    9.61230207  1.032513e+01\n  expRating[188]    8.32384630 0.49339757    7.29330930  8.019675e+00\n  expRating[189]   10.50789889 0.49782636    9.50160618  1.015532e+01\n  expRating[190]    2.39830701 0.52089744    1.41427414  2.037015e+00\n  expRating[191]    1.43395600 0.51133088    0.50545417  1.058227e+00\n  expRating[192]    3.56599971 0.52774219    2.59570347  3.190356e+00\n  expRating[193]    6.89950521 0.57865705    5.75207674  6.510941e+00\n  expRating[194]    7.33698620 0.57980958    6.15663377  6.960613e+00\n  expRating[195]    8.18647013 0.58416000    7.04852332  7.784186e+00\n  expRating[196]    3.83562108 0.55505078    2.72559005  3.477696e+00\n  expRating[197]    2.81926119 0.55684433    1.69523306  2.482151e+00\n  expRating[198]    0.96703306 0.54419724   -0.12215942  6.129028e-01\n  expRating[199]    0.77207831 0.54172819   -0.26736245  4.125944e-01\n  expRating[200]    4.06764165 0.53644023    3.04604029  3.683212e+00\n  expRating[201]   -0.08791129 0.53831994   -1.07849774 -4.580010e-01\n  expRating[202]    3.74025500 0.53992751    2.69551348  3.384239e+00\n  expRating[203]    4.58973893 0.53579155    3.53174847  4.224206e+00\n  expRating[204]    0.43418599 0.53396616   -0.61002424  7.965843e-02\n  expRating[205]    9.57943395 0.44808546    8.66502193  9.272684e+00\n  expRating[206]   10.01691495 0.44941930    9.06203354  9.728075e+00\n  expRating[207]    9.71959466 0.46318820    8.73054747  9.413590e+00\n  expRating[208]    7.09734167 0.57011659    5.97051412  6.703566e+00\n  expRating[209]    4.84602575 0.56843111    3.73149555  4.467233e+00\n  expRating[210]    5.59165087 0.56157473    4.51769611  5.223118e+00\n  expRating[211]    5.03270288 0.57603944    3.96162128  4.613491e+00\n  expRating[212]    5.47018388 0.56524967    4.41249187  5.070899e+00\n  expRating[213]    3.02410447 0.57315541    1.97197534  2.625321e+00\n  expRating[214]    4.97139897 0.59431868    3.81802852  4.573201e+00\n  expRating[215]    5.96363750 0.60276657    4.78790697  5.558334e+00\n  expRating[216]    6.76127150 0.59999825    5.57256237  6.354988e+00\n  expRating[217]    5.04931621 0.52761848    4.03671889  4.682562e+00\n  expRating[218]    2.74599141 0.53567640    1.75208775  2.359634e+00\n  expRating[219]    3.54362541 0.54830631    2.50247383  3.159951e+00\n  expRating[220]    8.38033007 0.57232672    7.29348875  8.002390e+00\n  expRating[221]    5.07426106 0.56053829    4.03415787  4.701024e+00\n  expRating[222]    7.72412320 0.56805099    6.65908689  7.345399e+00\n  expRating[223]    9.44792482 0.52622798    8.42113122  9.064203e+00\n  expRating[224]    7.00184542 0.53099659    5.92927217  6.636489e+00\n  expRating[225]    9.15060453 0.52329841    8.10008475  8.802544e+00\n  expRating[226]    5.32899702 0.58433821    4.22938290  4.949245e+00\n  expRating[227]    3.32039861 0.57672204    2.15850316  2.941642e+00\n  expRating[228]    4.31263713 0.58278258    3.21600458  3.921979e+00\n  expRating[229]    1.88738802 0.60267576    0.72380347  1.471520e+00\n  expRating[230]    2.87962655 0.59503759    1.75334842  2.462237e+00\n  expRating[231]    1.02739842 0.60283100   -0.08836433  5.900375e-01\n  expRating[232]    4.66057309 0.61043174    3.49514186  4.220862e+00\n  expRating[233]    5.94753801 0.59212300    4.81187802  5.533180e+00\n  expRating[234]    1.79198507 0.59127196    0.66118571  1.376827e+00\n  expRating[235]    7.09798930 0.59773260    5.89290022  6.713518e+00\n  expRating[236]    5.08939088 0.58091368    4.01836146  4.685395e+00\n  expRating[237]    6.08162941 0.60261409    4.92071287  5.678742e+00\n  expRating[238]    2.64038362 0.58803983    1.45344616  2.218453e+00\n  expRating[239]    4.78914273 0.57870055    3.64835288  4.389938e+00\n  expRating[240]    4.43025614 0.57687012    3.28151893  4.048247e+00\n  expRating[241]    4.72797472 0.53384915    3.68676812  4.358272e+00\n  expRating[242]    3.86798512 0.53329501    2.83440828  3.505460e+00\n  expRating[243]    6.87673384 0.53278347    5.87971097  6.508820e+00\n  expRating[244]    3.38811261 0.52669837    2.41090236  3.006579e+00\n  expRating[245]    2.37175273 0.54397290    1.35885362  1.985195e+00\n  expRating[246]    3.16938673 0.52462561    2.15849728  2.809456e+00\n  expRating[247]    5.60419630 0.54869086    4.52601570  5.236969e+00\n  expRating[248]    4.63984529 0.55161372    3.64089405  4.243258e+00\n  expRating[249]    5.38547041 0.55472157    4.32683627  5.019958e+00\n  expRating[250]    1.95834808 0.43473699    1.13754908  1.652249e+00\n  expRating[251]    2.39582907 0.43070212    1.60057092  2.090667e+00\n  expRating[252]   -0.05025034 0.43577643   -0.84430252 -3.701891e-01\n  expRating[253]    3.10819087 0.56337897    1.97032567  2.733557e+00\n  expRating[254]    6.11693960 0.56130435    5.07468678  5.705781e+00\n  expRating[255]    5.75805301 0.55553299    4.71741489  5.370378e+00\n  expRating[256]    6.49227381 0.56298569    5.37983300  6.116600e+00\n  expRating[257]    5.63228420 0.56869006    4.52342778  5.255245e+00\n  expRating[258]    8.28214633 0.56393284    7.17871327  7.906734e+00\n  expRating[259]    2.66592831 0.52565655    1.66479500  2.293666e+00\n  expRating[260]   -0.20265971 0.53945119   -1.21110660 -5.783533e-01\n  expRating[261]    2.44720242 0.52937745    1.45513103  2.075911e+00\n  expRating[262]    7.83845891 0.58299706    6.68422203  7.451881e+00\n  expRating[263]    7.89046779 0.58195907    6.71848592  7.497893e+00\n  expRating[264]    5.98623078 0.56334917    4.85252872  5.599277e+00\n  expRating[265]    5.94973274 0.56990303    4.78023796  5.568084e+00\n  expRating[266]    4.04549573 0.55249974    2.93904041  3.667664e+00\n  expRating[267]    6.69535786 0.55476700    5.52208789  6.346144e+00\n  expRating[268]    7.73600232 0.52285246    6.67155240  7.401019e+00\n  expRating[269]    5.88377419 0.53099252    4.84802937  5.554080e+00\n  expRating[270]    8.89252291 0.52151148    7.84714518  8.524531e+00\n  expRating[271]    3.89897247 0.56951959    2.74831090  3.514924e+00\n  expRating[272]    2.44513159 0.59183998    1.32182495  2.052889e+00\n  expRating[273]    2.49714047 0.58283103    1.32897485  2.133824e+00\n  expRating[274]    3.68262393 0.58337356    2.51972339  3.295399e+00\n  expRating[275]    4.12010492 0.57762275    2.96366564  3.742249e+00\n  expRating[276]    3.82278464 0.57293658    2.68578094  3.437224e+00\n  expRating[277]    4.75755714 0.55041566    3.66980339  4.400362e+00\n  expRating[278]    5.60704107 0.56697649    4.43709910  5.241210e+00\n  expRating[279]    4.46023686 0.56246446    3.41808915  4.093177e+00\n  expRating[280]    3.14857320 0.59840440    1.87108013  2.753968e+00\n  expRating[281]    4.43553812 0.58336757    3.28819799  4.063928e+00\n  expRating[282]    2.18422219 0.57258245    1.09255196  1.803898e+00\n  expRating[283]    8.51062087 0.53691413    7.46908650  8.146175e+00\n  expRating[284]    9.36010480 0.55226252    8.28380376  8.990239e+00\n  expRating[285]    8.21330059 0.54777890    7.08661945  7.854540e+00\n  expRating[286]    5.07446156 0.55592321    3.98816937  4.706500e+00\n  expRating[287]    3.67262956 0.58365980    2.47715175  3.279777e+00\n  expRating[288]    4.77714127 0.57460761    3.68963513  4.377571e+00\n  expRating[289]    6.25213420 0.54861984    5.20100100  5.877629e+00\n  expRating[290]    4.85030220 0.56743008    3.77815593  4.467742e+00\n  expRating[291]    5.95481391 0.56791542    4.85994627  5.572115e+00\n  expRating[292]    7.24389821 0.54673289    6.16380994  6.875429e+00\n  expRating[293]    5.84206621 0.56032940    4.75319233  5.457521e+00\n  expRating[294]    6.94657793 0.55217847    5.93091867  6.577966e+00\n  expRating[295]    5.17636956 0.58992573    3.99575114  4.769894e+00\n  expRating[296]    3.77453757 0.59427081    2.63320395  3.385037e+00\n  expRating[297]    4.87904928 0.58239899    3.69955790  4.477534e+00\n  expRating[298]    4.25505958 0.56111787    3.16576263  3.848341e+00\n  expRating[299]    5.29930699 0.55552218    4.20249265  4.916812e+00\n  expRating[300]    7.43135070 0.56224703    6.36897126  7.046802e+00\n  lp__           -101.68702903 9.92870261 -123.79649229 -1.078582e+02\n                stats\nparameter                  50%         75%        97.5%\n  ability[1]        5.62683371   6.0514640   6.81319053\n  ability[2]        8.00993993   8.4239620   9.27489042\n  ability[3]        6.81963185   7.1723965   8.03119327\n  ability[4]        7.12234627   7.4985588   8.25403573\n  ability[5]        3.08219572   3.5154940   4.27805738\n  ability[6]        4.56685432   4.9835772   5.78998524\n  ability[7]        8.97901197   9.3594994   9.87197459\n  ability[8]        4.22825728   4.6127650   5.27603113\n  ability[9]        2.84706728   3.2377484   3.97336488\n  ability[10]       8.82626343   9.2002430   9.73364672\n  ability[11]       5.65715241   6.0404624   6.80722623\n  ability[12]       6.65560399   7.0728905   7.85304268\n  ability[13]       1.61064246   1.9479210   2.68199847\n  ability[14]       9.37645680   9.6593435   9.96762571\n  ability[15]       5.81705192   6.2006094   7.00132089\n  ability[16]       6.65164184   7.0708962   7.73789438\n  ability[17]       5.19034045   5.5752997   6.36736402\n  ability[18]       1.88731674   2.2927771   3.03297646\n  ability[19]       2.65814917   3.0449070   3.85397555\n  ability[20]       4.83371715   5.2261651   6.06844698\n  ability[21]       6.86979767   7.2668772   8.07636604\n  ability[22]       7.64984085   8.0463087   8.72967126\n  ability[23]       3.77142960   4.1970575   4.90390597\n  ability[24]       8.72857072   9.1265180   9.71059716\n  ability[25]       2.10777952   2.5240295   3.28017670\n  ability[26]       5.96307653   6.3416435   7.02522188\n  ability[27]       2.61310447   3.0309027   3.81163612\n  ability[28]       2.15104234   2.5016702   3.23763903\n  ability[29]       5.09103399   5.4886091   6.24830411\n  ability[30]       8.36084366   8.8172755   9.62465604\n  ability[31]       8.82230696   9.2028379   9.87931921\n  ability[32]       2.37363575   2.7403680   3.49958548\n  ability[33]       9.04754768   9.4444036   9.92669073\n  ability[34]       7.33183683   7.7260518   8.43514473\n  ability[35]       5.48311050   5.8934620   6.71122651\n  ability[36]       5.86279370   6.2612011   7.05848101\n  ability[37]       4.00669497   4.3893566   5.15469970\n  ability[38]       1.99745086   2.3923893   3.23921928\n  ability[39]       2.25704757   2.6435673   3.46797218\n  ability[40]       1.95076562   2.2882895   2.99345530\n  ability[41]       7.76552523   8.1085202   8.79827201\n  ability[42]       7.12253688   7.4985083   8.25034423\n  ability[43]       8.53834344   8.9166813   9.60357858\n  ability[44]       7.72780847   8.1333030   8.79822219\n  ability[45]       5.89786818   6.2698649   6.97966002\n  ability[46]       2.77325365   3.1975770   3.93151308\n  ability[47]       9.27932150   9.5786162   9.95389598\n  ability[48]       7.93322568   8.3486826   9.03014007\n  ability[49]       2.91022597   3.2979776   4.06788074\n  ability[50]       5.66172161   6.0901469   6.92565107\n  ability[51]       1.55866058   1.9141721   2.64552559\n  ability[52]       2.61226583   3.0150332   3.81093331\n  ability[53]       6.65088505   7.0366406   7.71546054\n  ability[54]       3.21778360   3.5966836   4.39067452\n  ability[55]       3.40624696   3.7870346   4.50021607\n  ability[56]       2.26078293   2.6561657   3.41272894\n  ability[57]       4.89774055   5.3369634   6.05764349\n  ability[58]       8.83068670   9.1939780   9.76905279\n  ability[59]       4.28952507   4.6911537   5.51991676\n  ability[60]       8.17234899   8.5808515   9.34124535\n  ability[61]       2.48431254   2.8503103   3.66922705\n  ability[62]       9.20738853   9.5356881   9.93600893\n  ability[63]       9.07728610   9.4209001   9.85960899\n  ability[64]       2.07981889   2.4606618   3.16873117\n  ability[65]       6.62063978   7.0099980   7.64571894\n  ability[66]       3.52676742   3.9270934   4.78143655\n  ability[67]       2.48126002   2.8459387   3.62643669\n  ability[68]       2.99400836   3.3962172   4.09711105\n  ability[69]       9.31859852   9.6181769   9.95616126\n  ability[70]       5.51321209   5.8698613   6.77676579\n  ability[71]       4.73645783   5.1283726   5.89499939\n  ability[72]       6.66969508   7.0682464   7.90626306\n  ability[73]       3.43481280   3.8609094   4.56201304\n  ability[74]       7.63731450   8.0311661   8.80737787\n  ability[75]       8.72861598   9.1074335   9.73633686\n  ability[76]       5.02084776   5.4191469   6.24165261\n  ability[77]       3.58313778   4.0072515   4.83922658\n  ability[78]       4.34692780   4.7877564   5.63562544\n  ability[79]       6.81729073   7.2075298   8.00692768\n  ability[80]       4.36833033   4.7603653   5.52708651\n  ability[81]       6.42723203   6.8102929   7.57469801\n  ability[82]       3.08758567   3.4477887   4.17690174\n  ability[83]       5.29788300   5.7024103   6.43501102\n  ability[84]       1.60091688   1.9341875   2.59920509\n  ability[85]       5.66245708   6.0455010   6.85521544\n  ability[86]       8.21505973   8.5993256   9.32926488\n  ability[87]       2.33987056   2.7478099   3.48481106\n  ability[88]       8.56465706   8.9964640   9.68134152\n  ability[89]       6.60469530   6.9868464   7.70659654\n  ability[90]       8.45757220   8.8294563   9.54090209\n  ability[91]       3.15905603   3.5733566   4.35044972\n  ability[92]       3.38841434   3.7928093   4.53717033\n  ability[93]       4.01930707   4.4216649   5.13871814\n  ability[94]       2.84965849   3.2386505   4.00206052\n  ability[95]       7.77160564   8.1558231   8.87934787\n  ability[96]       4.31256512   4.7318454   5.52448251\n  ability[97]       5.51015828   5.8811399   6.70532565\n  ability[98]       6.49190302   6.9075297   7.62905123\n  ability[99]       4.43156026   4.8562398   5.66162558\n  ability[100]      5.95533115   6.3520763   7.16826412\n  severity[1]       0.30943210   0.4928847   0.87204633\n  severity[2]       0.73341428   0.9194668   1.30856695\n  severity[3]      -1.71173896  -1.5116698  -1.12856952\n  severity[4]       1.59489513   1.7828456   2.18847295\n  severity[5]      -0.71073145  -0.5048824  -0.16294122\n  severity[6]      -0.65789109  -0.4480303  -0.07792639\n  severity[7]      -2.55887246  -2.3688972  -1.96611457\n  severity[8]       0.44288731   0.6257459   1.01171437\n  severity[9]       1.47983630   1.6676018   2.04003872\n  severity[10]      0.08836151   0.2861737   0.66709365\n  sigma             0.93333913   0.9654271   1.03699567\n  tau               1.41626407   1.6614532   2.52469972\n  expRating[1]      3.90821563   4.3560256   5.05719946\n  expRating[2]      7.24411768   7.6127053   8.37437120\n  expRating[3]      4.98410993   5.3775951   6.12760385\n  expRating[4]      8.30798395   8.7146953   9.46930472\n  expRating[5]      9.60697869   9.9958426  10.76671983\n  expRating[6]      7.33884081   7.7184678   8.54993700\n  expRating[7]      6.15583768   6.5693996   7.25827853\n  expRating[8]      7.25566744   7.6199196   8.34224207\n  expRating[9]      6.89034237   7.3065135   8.06661299\n  expRating[10]     7.85176170   8.2203579   8.94751190\n  expRating[11]     6.44650558   6.8159571   7.54899814\n  expRating[12]     4.53998688   4.9084452   5.68925654\n  expRating[13]     3.84927264   4.2084252   4.98221245\n  expRating[14]     4.69998777   5.0690656   5.78854328\n  expRating[15]     2.40275569   2.7823699   3.51030273\n  expRating[16]     6.16480078   6.5564132   7.30663228\n  expRating[17]     3.85504669   4.2424183   4.97960978\n  expRating[18]     4.67180282   5.0499622   5.78074512\n  expRating[19]     9.26994979   9.6498237  10.25788719\n  expRating[20]     7.26186102   7.6309379   8.24420647\n  expRating[21]     9.39894177   9.7804961  10.36060565\n  expRating[22]     2.52708393   2.8817901   3.54034175\n  expRating[23]     3.57711867   3.9467605   4.61323005\n  expRating[24]     5.70111461   6.0648974   6.71529842\n  expRating[25]     3.16288909   3.5147640   4.22341889\n  expRating[26]     1.14946215   1.5156961   2.29745080\n  expRating[27]     0.28701298   0.6316335   1.34712509\n  expRating[28]    10.41065193  10.7650997  11.39295294\n  expRating[29]     9.26843102   9.6089107  10.18596210\n  expRating[30]    10.29397595  10.6542075  11.25587144\n  expRating[31]     7.25860718   7.6192824   8.35526135\n  expRating[32]     7.13222312   7.5029487   8.18519595\n  expRating[33]     5.76123163   6.1185400   6.79026808\n  expRating[34]     7.40849515   7.7783265   8.54091824\n  expRating[35]     8.27001139   8.6447815   9.32953730\n  expRating[36]     5.96160435   6.3433455   7.02542686\n  expRating[37]     2.38213285   2.7224485   3.40919113\n  expRating[38]    -0.06669966   0.2698989   0.94083263\n  expRating[39]     2.07255904   2.4344943   3.11185459\n  expRating[40]     9.64830065   9.9371936  10.41521279\n  expRating[41]     8.67899031   8.9802555   9.50362780\n  expRating[42]     9.78943639  10.0931568  10.52132314\n  expRating[43]     6.09859308   6.4953998   7.23059501\n  expRating[44]     3.20724424   3.6089728   4.37712402\n  expRating[45]     7.27512113   7.6350865   8.41006925\n  expRating[46]     8.28140490   8.6203223   9.24885935\n  expRating[47]     6.02588311   6.3542488   6.96127326\n  expRating[48]     4.10404537   4.4940456   5.12729728\n  expRating[49]     5.92907872   6.3263303   7.03396950\n  expRating[50]     2.64622731   3.0050400   3.82886000\n  expRating[51]     5.30027457   5.6542531   6.48768446\n  expRating[52]     1.18944963   1.5785790   2.33183129\n  expRating[53]     3.36364311   3.7725927   4.52721406\n  expRating[54]     1.98344564   2.3860086   3.11123555\n  expRating[55]     4.25292464   4.6054223   5.33181833\n  expRating[56]     1.99834898   2.3799936   3.14452240\n  expRating[57]     0.08760877   0.4612211   1.16159865\n  expRating[58]     4.13691364   4.4898139   5.20624095\n  expRating[59]     4.18452229   4.5582446   5.25037133\n  expRating[60]     6.31465543   6.6927076   7.36561848\n  expRating[61]     7.60973493   7.9789474   8.75446552\n  expRating[62]     6.19065751   6.5900354   7.31328607\n  expRating[63]     7.30252260   7.7031766   8.42831021\n  expRating[64]     7.94885357   8.3125472   9.00799921\n  expRating[65]     9.22536242   9.6001397  10.27105984\n  expRating[66]     5.07386839   5.4581599   6.11093439\n  expRating[67]     2.06330460   2.5039528   3.17707255\n  expRating[68]     5.35183457   5.7794393   6.48061855\n  expRating[69]     4.23741465   4.5962238   5.36019241\n  expRating[70]     9.03097654   9.4095574  10.04491419\n  expRating[71]     7.01822817   7.4028279   8.01057134\n  expRating[72]    10.20262164  10.5900405  11.18158992\n  expRating[73]     2.83825254   3.2092691   3.92226401\n  expRating[74]     2.53823121   2.9496012   3.60040067\n  expRating[75]     3.59610287   3.9748991   4.69770313\n  expRating[76]     7.54733699   7.9130014   8.60345353\n  expRating[77]     5.28771244   5.6813019   6.30544083\n  expRating[78]     6.39221884   6.7737354   7.46662243\n  expRating[79]     0.89597846   1.2878522   2.05603906\n  expRating[80]     4.21542522   4.5774767   5.38731891\n  expRating[81]     4.09537408   4.4728450   5.21561240\n  expRating[82]     2.89942433   3.2537545   3.92014235\n  expRating[83]     2.60530277   2.9434614   3.62308286\n  expRating[84]     2.23636770   2.6036007   3.25824554\n  expRating[85]     4.35910057   4.7411785   5.49898746\n  expRating[86]     6.56104320   6.9274611   7.70148873\n  expRating[87]     5.13929448   5.5412824   6.32146237\n  expRating[88]     8.71410041   9.0949797   9.80664912\n  expRating[89]     7.72449537   8.1130309   8.82047593\n  expRating[90]     9.86555231  10.2329591  10.98568640\n  expRating[91]    10.40784886  10.7827327  11.43217889\n  expRating[92]    10.27993421  10.6673177  11.32056918\n  expRating[93]     8.89378723   9.2436152   9.90066914\n  expRating[94]     0.65598764   1.0228807   1.69818724\n  expRating[95]     1.70004711   2.0450757   2.78308876\n  expRating[96]     2.45077755   2.8033995   3.50198759\n  expRating[97]    10.62058760  10.9972842  11.53313993\n  expRating[98]     6.49462015   6.8215413   7.37806760\n  expRating[99]    10.50229803  10.8488760  11.41006545\n  expRating[100]    8.06744805   8.4371375   9.09304496\n  expRating[101]    7.77475308   8.1143974   8.78082745\n  expRating[102]    7.41072699   7.7783048   8.40302821\n  expRating[103]    4.78924483   5.1851612   5.93921424\n  expRating[104]    6.99040343   7.3713933   8.09425472\n  expRating[105]    5.57848935   5.9606243   6.72786546\n  expRating[106]    6.62631910   6.9642351   7.71656336\n  expRating[107]    5.20684849   5.5670871   6.34124647\n  expRating[108]    7.34492049   7.7186090   8.47577226\n  expRating[109]    4.31346223   4.6910757   5.41986623\n  expRating[110]    1.45865235   1.8025804   2.55481420\n  expRating[111]    5.47749272   5.8640607   6.61247854\n  expRating[112]    2.32089367   2.6818024   3.52563986\n  expRating[113]    1.27954612   1.6821953   2.43537788\n  expRating[114]   -0.56821983  -0.2053485   0.55489991\n  expRating[115]    3.86302977   4.2347467   4.92442442\n  expRating[116]    1.56075562   1.9201480   2.67839751\n  expRating[117]    3.73361432   4.1219483   4.94180649\n  expRating[118]    2.67736737   3.0236439   3.74867502\n  expRating[119]    0.23210634   0.6013882   1.28314221\n  expRating[120]    3.40764872   3.7621242   4.44632068\n  expRating[121]    8.50193212   8.8506665   9.49355806\n  expRating[122]    6.05677275   6.3965887   7.05956467\n  expRating[123]    7.84588458   8.2096938   8.89426384\n  expRating[124]    7.45629993   7.7844702   8.56846526\n  expRating[125]    6.39321658   6.7803566   7.52943439\n  expRating[126]    8.57400402   8.9710148   9.70356213\n  expRating[127]    6.82717339   7.1860231   7.87238892\n  expRating[128]    8.98325622   9.3364553   9.98959843\n  expRating[129]    8.63180625   8.9775075   9.56778541\n  expRating[130]    6.01457722   6.3738108   7.04665235\n  expRating[131]    9.20343760   9.5664116  10.23940285\n  expRating[132]    7.79186103   8.1796348   8.91057021\n  expRating[133]    7.51631739   7.8656590   8.49829271\n  expRating[134]    5.19435823   5.5777219   6.20211318\n  expRating[135]    6.34815787   6.7268415   7.37480134\n  expRating[136]    2.04833451   2.4244700   3.12692708\n  expRating[137]    2.08775297   2.5048816   3.20934979\n  expRating[138]    0.21658429   0.5623693   1.27463586\n  expRating[139]    9.55460808   9.8593471  10.40763166\n  expRating[140]    8.52499855   8.8547403   9.39116251\n  expRating[141]    9.69457186  10.0150420  10.48740569\n  expRating[142]    7.22372073   7.6133801   8.26441695\n  expRating[143]    9.40014423   9.7968581  10.45966189\n  expRating[144]    8.01922340   8.3854675   9.08264465\n  expRating[145]    4.51532744   4.8679280   5.67976081\n  expRating[146]    0.36513437   0.6874265   1.51125794\n  expRating[147]    2.99425839   3.3994531   4.15032214\n  expRating[148]    4.96120761   5.3802921   6.11270551\n  expRating[149]    3.08442026   3.5293025   4.22650207\n  expRating[150]    7.13359155   7.5538666   8.39708313\n  expRating[151]    1.91333356   2.2267620   2.89079495\n  expRating[152]    2.34524974   2.6842684   3.34601869\n  expRating[153]    3.09071878   3.4035261   4.13798545\n  expRating[154]    0.92414004   1.2716572   2.07539341\n  expRating[155]    1.97531301   2.3473803   3.08002079\n  expRating[156]    4.08491503   4.4361855   5.25051929\n  expRating[157]    5.90710479   6.3020454   7.00855528\n  expRating[158]    4.09264151   4.4766811   5.07332968\n  expRating[159]    8.10889509   8.5052377   9.16981757\n  expRating[160]    3.93524402   4.3380985   5.10337524\n  expRating[161]    1.50513320   1.8671564   2.65808383\n  expRating[162]    2.48182460   2.9126528   3.70170757\n  expRating[163]    3.71428204   4.0793032   4.78121173\n  expRating[164]    2.67109936   3.0700401   3.76810690\n  expRating[165]    4.87355563   5.2703124   5.97776793\n  expRating[166]    3.85011373   4.2513740   4.97045848\n  expRating[167]   -0.29610024   0.1024342   0.86498839\n  expRating[168]    2.71276794   3.0890880   3.76066877\n  expRating[169]    3.21344249   3.5944574   4.26190927\n  expRating[170]    2.36193013   2.7489585   3.41208102\n  expRating[171]    5.00073294   5.3741822   6.04254299\n  expRating[172]    7.13734483   7.4603354   8.01091839\n  expRating[173]    8.12162573   8.4330147   9.09323685\n  expRating[174]    8.91317792   9.2725799   9.80263898\n  expRating[175]    3.59101022   4.0150128   4.79648302\n  expRating[176]    1.72424419   2.1199632   2.88751663\n  expRating[177]    4.37388310   4.7628421   5.45341519\n  expRating[178]    6.48141214   6.8579405   7.54706941\n  expRating[179]    9.79364795  10.1460099  10.88154240\n  expRating[180]    9.68238904  10.0102744  10.75601712\n  expRating[181]    2.77708310   3.1460184   3.86884300\n  expRating[182]    1.80767201   2.1957529   2.86860363\n  expRating[183]    2.91347334   3.3100654   3.99259844\n  expRating[184]    9.51460457   9.8162630  10.30524839\n  expRating[185]    9.95479887  10.2664164  10.72838468\n  expRating[186]    9.63352925   9.9859661  10.48327349\n  expRating[187]   10.64237972  10.9813090  11.53005863\n  expRating[188]    8.34160050   8.6709370   9.23525677\n  expRating[189]   10.53455825  10.8694912  11.41638692\n  expRating[190]    2.41386712   2.7675818   3.39629808\n  expRating[191]    1.43827336   1.7858036   2.50289795\n  expRating[192]    3.55786879   3.9087949   4.67757282\n  expRating[193]    6.91507156   7.2838939   8.01552639\n  expRating[194]    7.36426657   7.7196359   8.43388623\n  expRating[195]    8.18992944   8.6099485   9.27802030\n  expRating[196]    3.83073733   4.2119520   4.95124214\n  expRating[197]    2.80544598   3.1998188   3.92377099\n  expRating[198]    0.98137551   1.3178259   2.03707249\n  expRating[199]    0.76385667   1.1134943   1.88006009\n  expRating[200]    4.05457995   4.4352172   5.10732149\n  expRating[201]   -0.10672727   0.2776176   0.98011179\n  expRating[202]    3.74000862   4.1077738   4.83043751\n  expRating[203]    4.59250645   4.9372680   5.70529171\n  expRating[204]    0.44620537   0.7911373   1.46404907\n  expRating[205]    9.59494552   9.8967003  10.40645183\n  expRating[206]   10.03739409  10.3517246  10.81402956\n  expRating[207]    9.73557434  10.0386472  10.54966087\n  expRating[208]    7.10341276   7.4908684   8.28254946\n  expRating[209]    4.84599862   5.2350760   6.02948317\n  expRating[210]    5.59409956   5.9579197   6.73197600\n  expRating[211]    5.00552689   5.4291669   6.19741084\n  expRating[212]    5.46714280   5.8745844   6.55708017\n  expRating[213]    3.00350567   3.4129283   4.15781231\n  expRating[214]    4.96450422   5.3358749   6.18871667\n  expRating[215]    5.93585172   6.3851794   7.11597176\n  expRating[216]    6.74172178   7.1592245   7.97562950\n  expRating[217]    5.02068453   5.4226227   6.07518494\n  expRating[218]    2.75032995   3.1042220   3.76832856\n  expRating[219]    3.55035015   3.9284240   4.57066115\n  expRating[220]    8.36322714   8.7465806   9.56754581\n  expRating[221]    5.07377194   5.4345926   6.20741239\n  expRating[222]    7.69150147   8.1013880   8.87350178\n  expRating[223]    9.47977788   9.8163699  10.45720405\n  expRating[224]    7.03202307   7.3987368   7.92496669\n  expRating[225]    9.16906343   9.5116766  10.12003449\n  expRating[226]    5.34719929   5.7001876   6.52085670\n  expRating[227]    3.33222901   3.6779803   4.49540761\n  expRating[228]    4.30662062   4.6885952   5.53283592\n  expRating[229]    1.88224293   2.2972192   3.06353397\n  expRating[230]    2.87461873   3.2727735   3.98063983\n  expRating[231]    1.02549171   1.4389828   2.24035748\n  expRating[232]    4.64650814   5.0835252   5.86544594\n  expRating[233]    5.92730634   6.3444801   7.06785779\n  expRating[234]    1.79453678   2.2003773   2.89398181\n  expRating[235]    7.09971855   7.4919865   8.28602530\n  expRating[236]    5.09065791   5.4599578   6.24740945\n  expRating[237]    6.06763516   6.4735617   7.28006398\n  expRating[238]    2.64806002   3.0435786   3.78236473\n  expRating[239]    4.79902332   5.1640149   5.91634290\n  expRating[240]    4.41184367   4.8296723   5.50912997\n  expRating[241]    4.72018470   5.0971777   5.77820091\n  expRating[242]    3.87065202   4.2189793   4.93516954\n  expRating[243]    6.89498655   7.2370311   7.93980954\n  expRating[244]    3.38847993   3.7619242   4.37548385\n  expRating[245]    2.38497518   2.7271840   3.47722332\n  expRating[246]    3.17240491   3.5273975   4.17214523\n  expRating[247]    5.61744852   5.9838773   6.60856166\n  expRating[248]    4.65210139   5.0289791   5.70560684\n  expRating[249]    5.38029669   5.7531792   6.43401319\n  expRating[250]    1.94750384   2.2508796   2.84360260\n  expRating[251]    2.36853824   2.6732546   3.29380626\n  expRating[252]   -0.06561523   0.2454642   0.81943450\n  expRating[253]    3.09358257   3.4752050   4.20560715\n  expRating[254]    6.10942869   6.5065744   7.23221338\n  expRating[255]    5.74566984   6.1415727   6.80191848\n  expRating[256]    6.49536086   6.8559601   7.59914667\n  expRating[257]    5.61377374   6.0046867   6.73489644\n  expRating[258]    8.28859795   8.6641312   9.36227247\n  expRating[259]    2.64460796   3.0278628   3.67400886\n  expRating[260]   -0.21629752   0.1602781   0.83048984\n  expRating[261]    2.43626037   2.8090556   3.48250306\n  expRating[262]    7.84412619   8.2781619   8.92430077\n  expRating[263]    7.89468733   8.3147096   8.93638877\n  expRating[264]    6.00132382   6.4257851   6.98374217\n  expRating[265]    5.96430095   6.3344211   7.07795528\n  expRating[266]    4.05536037   4.4205398   5.11340919\n  expRating[267]    6.70847839   7.0642582   7.77701614\n  expRating[268]    7.72765473   8.0946720   8.74114154\n  expRating[269]    5.88200845   6.2355707   6.95118017\n  expRating[270]    8.90289224   9.2545926   9.86828952\n  expRating[271]    3.88725664   4.2794927   4.97188647\n  expRating[272]    2.43602298   2.8365956   3.58118203\n  expRating[273]    2.48111478   2.8802690   3.65154752\n  expRating[274]    3.67168833   4.0988842   4.85603969\n  expRating[275]    4.14804861   4.5193347   5.22740136\n  expRating[276]    3.82227947   4.2199613   4.88579353\n  expRating[277]    4.74599761   5.1319043   5.84501941\n  expRating[278]    5.61562144   6.0008876   6.71674565\n  expRating[279]    4.45399984   4.8553913   5.57344509\n  expRating[280]    3.14828926   3.5514377   4.23412318\n  expRating[281]    4.43433626   4.7998067   5.55715925\n  expRating[282]    2.16997278   2.5579057   3.28367578\n  expRating[283]    8.52548248   8.8685936   9.56521003\n  expRating[284]    9.36684139   9.7290623  10.42193598\n  expRating[285]    8.20773984   8.5826206   9.23740516\n  expRating[286]    5.06395546   5.4342856   6.15397020\n  expRating[287]    3.68545741   4.0826618   4.78131168\n  expRating[288]    4.79410960   5.1774083   5.94031902\n  expRating[289]    6.25801842   6.5957719   7.33796511\n  expRating[290]    4.85702438   5.2293765   5.91284330\n  expRating[291]    5.95941009   6.3313870   7.09063074\n  expRating[292]    7.23940744   7.6040443   8.31597857\n  expRating[293]    5.85080885   6.2420118   6.90529759\n  expRating[294]    6.94471004   7.3165562   8.01108473\n  expRating[295]    5.18506209   5.5907122   6.32213169\n  expRating[296]    3.76080244   4.2014482   4.94627867\n  expRating[297]    4.87672108   5.2898752   6.01395284\n  expRating[298]    4.25601203   4.6341180   5.35545559\n  expRating[299]    5.29485773   5.6719295   6.40507638\n  expRating[300]    7.43600986   7.8179304   8.53768548\n  lp__           -101.05355642 -94.8578545 -84.05983975\n\n\n\nratings.fit1s &lt;-launch_shinystan(ratings.fit1)",
    "crumbs": [
      "Multiple Raters in Stan"
    ]
  },
  {
    "objectID": "Bayesian/Schools8Stan-improved.html",
    "href": "Bayesian/Schools8Stan-improved.html",
    "title": "8 Schools Stan Improved",
    "section": "",
    "text": "This is the classic eight schools example from Rubin(1981)1. (It is also found in Chapter 5 of Gelman et al., 2014 2.) The story is that 8 different schools experimented with an SAT coaching experiment. The performance gains of the coached students were compared to students on a weight list control. Separate estimates were obtained for each school, but because the size of the schools differed, the standard errors differed as well.\nHere are the data:\n\nSchools &lt;- data.frame(row.names=c(\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\"),\n                      effect = c(28.39,7.94,-2.75,6.82,-.64,.63,18.01,12.16),\n                      see = c(14.9, 10.2, 16.3, 11.0, 9.4, 11.4, 10.4, 17.6))\n\nSchools\n\n  effect  see\nA  28.39 14.9\nB   7.94 10.2\nC  -2.75 16.3\nD   6.82 11.0\nE  -0.64  9.4\nF   0.63 11.4\nG  18.01 10.4\nH  12.16 17.6\n\n\nLets start by calculating a weighted average effect. I’ll weight each case by the precision (one over the square of the see).\n\nSchools$w &lt;- 1/Schools$see^2\nSchools.mean &lt;- sum(Schools$w*Schools$effect)/sum(Schools$w)\nSchools.mean\n\n[1] 7.870546\n\n\nHere is a plot of the data.\n\nord &lt;- order(Schools$effect)\nplot(Schools$effect[ord[c(1,8)]]+c(-2,2)*Schools$see[ord[c(1,8)]],\n     c(nrow(Schools),1),main = \"8 Schools data.\",type=\"n\",yaxt=\"n\",\n     xlab=\"Effect Size\",ylab=\"School\")\npoints(Schools$effect[ord],nrow(Schools):1,pch=rownames(Schools)[ord])\nsegments(Schools$effect[ord]-2*Schools$see[ord],nrow(Schools):1,\n         Schools$effect[ord]+2*Schools$see[ord],nrow(Schools):1)\nabline(v=Schools.mean,col=\"blue\")\n\n\n\n\n\n\n\n\n\n\nFirst we need to load the packages. The rstan package runs stan from R. The shinystan package gives us a browser for the results.\n\nlibrary(rstan)\n\nLoading required package: StanHeaders\n\n\n\nrstan version 2.32.7 (Stan version 2.32.2)\n\n\nFor execution on a local, multicore CPU with excess RAM we recommend calling\noptions(mc.cores = parallel::detectCores()).\nTo avoid recompilation of unchanged Stan programs, we recommend calling\nrstan_options(auto_write = TRUE)\nFor within-chain threading using `reduce_sum()` or `map_rect()` Stan functions,\nchange `threads_per_chain` option:\nrstan_options(threads_per_chain = 1)\n\nlibrary(shinystan)\n\nLoading required package: shiny\n\n\n\nThis is shinystan version 2.6.0\n\nlibrary(parallel) # For using multiple chains\noptions(mc.cores=5)\n\nFirst we set up the Stan model, and put it into a variable called school8 (note the output.var=\"school8\" in stan block tag)\n\ndata {\n  int&lt;lower=0&gt; J; // number of schools\n  real y[J]; // estimated treatement effects\n  real&lt;lower=0&gt; sigma[J]; //s.e. of effects.\n}\nparameters {\n  real mu;\n  real&lt;lower=0&gt; tau;\n  vector[J] eta;\n} \ntransformed parameters {\n  vector [J] theta;\n  theta = mu + eta*tau;\n}\nmodel {\n  eta ~ normal(0,1);\n  //target += normal_lpdf(theta|mu,tau);\n  y ~ normal(theta,sigma);\n  //target += normal_lpdf(effect|theta,see);\n}\n\n\n\nAlmost all models will have a data, parameters and model block. They could have others as well (common ones are transformed data to do pre-calculations and transformed parameters to recast the model).\n\n\nIn a stan model, data refers to values that don’t change over the course of the MCMC loop. This tends to be one of three things: * The observed data values. * Fixed hyperparameters for prior distributions. * Structural hyperparamters (e.g., sample size, number of groups).\nIn stan (like C++) all variables must be declared. They are generally either int or real (here vector is shorthand for a vector of real values). The modifiers &lt;lower=XXX&gt; and &lt;upper=XXX&gt; can be used to constrain the inputs. In the data field this is just used to to type checking.\n\n\n\nThe parameters are the values that stan will try to estimate. These include both latent variables and ordinary parameters and hyperparameters.\nIn stan, parameters should all be real. Non-continuous parameters don’t work with the Hamiltonian Monte Carlo. (There are tricks for dealing with common cases like mixture models in the stan examples.) This is called lp__ (log pdf) in the output.\nNote carefully the use of the lower and upper bounds. It is important for stan to know when a parameter is restricted to say positive values (i.e., a scale parameter) as it needs to constrain the space for the sampler.\n\n\n\nThis section gives the distribution for all of the parameters. First I give the BUGS-like way of doing this. This is to use the ~ operator to give the distribution. The names are slightly different in stan and R, so the rstan function lookup can help you find the stan function corresponding to the R function.\n\nlookup(dnorm)\n\n          StanFunction\n415 normal_id_glm_lpdf\n418         normal_log\n419        normal_lpdf\n553    std_normal_lpdf\n                                                                       Arguments\n415 (real, matrix, real, vector, T);(vector, row_vector, vector, vector, vector)\n418                                     (real, real, T);(vector, vector, vector)\n419                                     (real, real, T);(vector, vector, vector)\n553                                                                 (T);(vector)\n    ReturnType\n415     T;real\n418     T;real\n419     T;real\n553     T;real\n\nlookup(dt)\n\n      StanFunction                                              Arguments\n562  student_t_log (real, real, real, T);(vector, vector, vector, vector)\n563 student_t_lpdf (real, real, real, T);(vector, vector, vector, vector)\n    ReturnType\n562     T;real\n563     T;real\n\n\nNote that what stan is actually doing in the model block is calculating the log p.d.f. Thus, the commented out expressions are an alternative to the ~ notation.\nFinally, note that this is an incomplete Bayesian model as there are no distributions for mu or tau. In stan this means we are implicitly putting a uniform prior on mu and log(tau); the latter is transformed so that it will always be positive. These are improper priors, but stan will be fine as long as the posterior is proper.\n\n\n\n\n\nBuild a list which contains the data as elements using the names in the data section of the model.\n\nschool8.dat &lt;- list(\n  J = nrow(Schools),\n  y = Schools$effect,\n  sigma=Schools$see)\n\n\n\n\nThere are two funcitons to start the sampling in stan. The first one is stan(file=XXX,data=YYY,...). This assumes that the stan model is in a file. However, with R markdown, we already saved the model in an object so we can use sampling(model,data=YYY,...). By the way, this same function can be used to make additional samples after we have sampled for a while.\nThe mc.cores should work well for Unix (Linux and Mac OS), I’m not so sure about Windows.\n\n#options(mc.cores = parallel::detectCores()-1)\nschool8.fit1 &lt;- sampling(\n  school8,       # The model\n  data = school8.dat, # The data\n  chains = 5,\n  warmup = 1000,\n  iter = 2000,\n  refresh=1000        # Show progress\n)\n\nWarning: There were 3 divergent transitions after warmup. See\nhttps://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\nto find out why this is a problem and how to eliminate them.\n\n\nWarning: Examine the pairs() plot to diagnose sampling problems\n\nsummary(school8.fit1)\n\n4775\ntau       9.1052115 19.7136121 2327.896 1.0015684\neta[1]    1.0315979  2.1702931 4867.594 1.0005030\neta[2]    0.6038292  1.7575377 5039.402 0.9995612\neta[3]    0.4435779  1.6478861 5245.409 0.9993708\neta[4]    0.5638652  1.6657116 5558.884 0.9997504\neta[5]    0.2340142  1.4861669 5305.889 0.9998699\neta[6]    0.3620663  1.5623228 4894.520 1.0001554\neta[7]    0.9115123  2.0291966 4784.759 1.0004121\neta[8]    0.6889746  1.8865260 5513.766 0.9996236\ntheta[1] 15.7714754 32.8230629 3370.790 1.0005775\ntheta[2] 11.9793957 20.3729747 5464.851 0.9997725\ntheta[3] 11.1034097 21.0252281 4373.520 1.0001817\ntheta[4] 11.8827749 21.1340201 5362.160 0.9998073\ntheta[5]  9.8017072 16.9570110 5526.486 1.0007979\ntheta[6] 10.6206478 18.4255261 5500.031 1.0009768\ntheta[7] 14.5116616 25.1983460 4325.928 1.0001939\ntheta[8] 12.9424748 25.9498102 4572.715 1.0002053\nlp__     -3.0313726 -0.5140309 1560.513 1.0032085\n\n$c_summary\n, , chains = chain:1\n\n          stats\nparameter          mean        sd        2.5%        25%         50%        75%\n  mu        8.108977919 4.5414579  -1.3735888  5.1920601  8.16485328 11.1048192\n  tau       6.196502314 5.0705968   0.3051799  2.4856176  4.99387040  8.4951222\n  eta[1]    0.377026768 0.9516892  -1.5356402 -0.2752460  0.37186417  1.0062744\n  eta[2]   -0.005839293 0.8685292  -1.7253996 -0.5446877 -0.02515838  0.5468288\n  eta[3]   -0.182964412 0.9571102  -2.0553740 -0.8061908 -0.19696588  0.4582568\n  eta[4]   -0.047212212 0.8975976  -1.7863226 -0.6436490 -0.03972658  0.5347721\n  eta[5]   -0.337766958 0.9182210  -2.1641677 -0.9070398 -0.33365592  0.2405289\n  eta[6]   -0.228016366 0.8637041  -1.8619810 -0.8047200 -0.22061596  0.3390450\n  eta[7]    0.298692959 0.9158723  -1.5002286 -0.2954845  0.29221986  0.8677758\n  eta[8]    0.044490389 0.9421667  -1.7426750 -0.6191457  0.03489879  0.6973535\n  theta[1] 11.527464459 8.4252323  -1.6984561  6.3422959 10.51978536 15.3487061\n  theta[2]  7.938971310 5.8333094  -3.5050637  4.3669240  8.01192729 11.5234189\n  theta[3]  6.547431266 7.8296833 -11.5778553  3.1088068  7.29252875 11.1371943\n  theta[4]  8.036214206 6.1828977  -3.7091480  4.2985883  7.96721128 11.4889555\n  theta[5]  5.792198654 6.4551752  -8.9274652  2.0787198  6.23225467  9.9841633\n  theta[6]  6.431389216 6.5422285  -9.6454064  2.7619612  7.16121278 10.5606060\n  theta[7] 10.543852816 6.4320421  -0.3579881  6.1798853 10.04096160 14.1887795\n  theta[8]  8.782358158 7.6219863  -7.1582848  4.4444597  8.57645560 12.7680413\n  lp__     -4.870801197 2.5488115 -10.9226984 -6.4380852 -4.68081643 -3.0915766\n          stats\nparameter       97.5%\n  mu       16.9712636\n  tau      18.5799679\n  eta[1]    2.1481498\n  eta[2]    1.6823520\n  eta[3]    1.7362172\n  eta[4]    1.6954624\n  eta[5]    1.4922502\n  eta[6]    1.5142767\n  eta[7]    2.1457115\n  eta[8]    1.8410109\n  theta[1] 31.9545933\n  theta[2] 20.2092511\n  theta[3] 19.5321853\n  theta[4] 21.3236209\n  theta[5] 17.2868457\n  theta[6] 18.1561487\n  theta[7] 24.8549275\n  theta[8] 25.5726002\n  lp__     -0.5535598\n\n, , chains = chain:2\n\n          stats\nparameter          mean        sd        2.5%        25%         50%        75%\n  mu        7.987985477 4.9485908  -1.7741590  4.5938001  7.70471754 11.2855018\n  tau       6.499956713 5.3508862   0.2214177  2.5013624  5.25135778  9.0830559\n  eta[1]    0.421067042 0.9525111  -1.6066398 -0.2255225  0.45065971  1.0714713\n  eta[2]    0.034622963 0.8693805  -1.7301475 -0.5639677  0.01826661  0.6301161\n  eta[3]   -0.194723788 0.9206019  -2.0368697 -0.7970403 -0.15026239  0.4635916\n  eta[4]   -0.001710459 0.8635883  -1.7109605 -0.5872694 -0.02683828  0.5775723\n  eta[5]   -0.310222863 0.9203612  -2.1063580 -0.9076318 -0.30167275  0.2938009\n  eta[6]   -0.210230107 0.8668066  -1.9233771 -0.8001534 -0.22322717  0.3614584\n  eta[7]    0.340137258 0.9504863  -1.5561022 -0.2761296  0.38080872  0.9456390\n  eta[8]    0.082262061 0.9285287  -1.7977866 -0.6002183  0.06180018  0.6998272\n  theta[1] 11.626050013 8.5608429  -3.5768229  6.2308734 10.54640986 15.7830646\n  theta[2]  8.231382568 6.2648629  -5.2720175  4.5376533  8.20970960 12.2993767\n  theta[3]  6.289476595 8.1186267 -13.0287501  2.3455710  6.91186800 11.2359355\n  theta[4]  7.798334347 6.2596044  -4.8754885  3.9974516  7.75565326 11.8550496\n  theta[5]  5.376882498 6.4380220  -8.5264248  1.5266789  6.04329046  9.7377413\n  theta[6]  6.194451030 6.8106011  -8.1767518  2.6580334  6.71880474 10.6567160\n  theta[7] 10.560320941 6.8518688  -2.3827422  6.0608562 10.33902352 14.4815048\n  theta[8]  8.677443402 7.3595251  -6.1942521  4.4943854  8.35334916 12.9665546\n  lp__     -4.859834219 2.6400012 -10.6963718 -6.4683610 -4.53241189 -3.0195196\n          stats\nparameter       97.5%\n  mu       17.6452037\n  tau      20.5068063\n  eta[1]    2.2748092\n  eta[2]    1.6874192\n  eta[3]    1.4905939\n  eta[4]    1.7217261\n  eta[5]    1.6425875\n  eta[6]    1.5324748\n  eta[7]    2.2551346\n  eta[8]    1.9195188\n  theta[1] 34.3108883\n  theta[2] 19.6477325\n  theta[3] 21.0252281\n  theta[4] 20.2212760\n  theta[5] 16.4239411\n  theta[6] 18.3513917\n  theta[7] 25.2776176\n  theta[8] 25.3181385\n  lp__     -0.2811735\n\n, , chains = chain:3\n\n          stats\nparameter          mean        sd        2.5%        25%           50%\n  mu        7.984119163 5.2850843  -2.6906032  4.4702459  7.9669988035\n  tau       6.780726194 5.3925022   0.3420370  2.6894762  5.3488883323\n  eta[1]    0.422059210 0.8786949  -1.4036719 -0.1566890  0.4748823818\n  eta[2]   -0.005168801 0.9233485  -1.8010965 -0.6185735 -0.0006456158\n  eta[3]   -0.197612781 0.9128971  -2.0036544 -0.7828097 -0.1697648829\n  eta[4]   -0.029129209 0.8274659  -1.6977804 -0.5534801 -0.0786573035\n  eta[5]   -0.339073873 0.8813990  -2.1100295 -0.9022798 -0.3226100845\n  eta[6]   -0.240538025 0.9570248  -2.0828795 -0.8767444 -0.3168690590\n  eta[7]    0.370962848 0.8522918  -1.3383904 -0.2069558  0.3670871187\n  eta[8]    0.057490341 0.8982614  -1.7181044 -0.5457274  0.0319265505\n  theta[1] 11.858118376 8.6589213  -2.2177205  6.3145163 10.5151562196\n  theta[2]  8.077483688 6.5248129  -4.5663661  3.9543494  7.7995007755\n  theta[3]  6.430177567 7.3508015 -10.7982961  2.6259719  6.7403931349\n  theta[4]  7.651956720 6.6603158  -6.1989833  3.1791978  7.8470966484\n  theta[5]  5.212598772 6.6982450 -10.0382583  1.0732341  5.6817361269\n  theta[6]  5.956522723 7.1653881  -8.9729190  1.9823107  6.1762180114\n  theta[7] 11.005251355 6.7836879  -0.8372271  6.2917144 10.5131491035\n  theta[8]  8.394853046 8.1271922  -7.2379880  3.6046370  8.2098218925\n  lp__     -4.681947470 2.5433275 -10.2939556 -6.2493706 -4.3799520615\n          stats\nparameter         75%      97.5%\n  mu       11.4501720 18.0367866\n  tau       9.5751040 19.4194335\n  eta[1]    1.0321554  2.0314540\n  eta[2]    0.5718271  1.8079509\n  eta[3]    0.4018604  1.5354504\n  eta[4]    0.5638104  1.5982821\n  eta[5]    0.2142210  1.4652254\n  eta[6]    0.3970379  1.6284067\n  eta[7]    0.9808142  1.9911270\n  eta[8]    0.6558605  1.8397262\n  theta[1] 16.0178470 32.7342324\n  theta[2] 12.2099101 20.9826191\n  theta[3] 10.8044350 19.9495177\n  theta[4] 11.9809052 20.4077498\n  theta[5]  9.7347633 16.8115437\n  theta[6] 10.4993680 19.4004811\n  theta[7] 15.2306949 25.2127880\n  theta[8] 13.1711533 27.4107701\n  lp__     -2.8813004 -0.4347474\n\n, , chains = chain:4\n\n          stats\nparameter         mean        sd        2.5%        25%         50%        75%\n  mu        8.44665379 5.3352691  -2.1109467  5.2235058  8.43858013 11.6889727\n  tau       6.36235338 5.1244939   0.2145530  2.3905578  5.18501654  9.3451497\n  eta[1]    0.39351569 0.9526534  -1.5048961 -0.2352222  0.37309645  1.0415627\n  eta[2]   -0.01394600 0.8948069  -1.7834092 -0.6606825 -0.02510561  0.5879319\n  eta[3]   -0.20981645 0.9618919  -2.0412251 -0.8562945 -0.25376539  0.4067044\n  eta[4]   -0.06472502 0.9085278  -1.8483946 -0.6858356 -0.08699758  0.5870187\n  eta[5]   -0.35049693 0.8598175  -2.0637544 -0.8785655 -0.36019478  0.1749668\n  eta[6]   -0.19764028 0.8677715  -1.7752442 -0.7741922 -0.21515830  0.3610380\n  eta[7]    0.28158021 0.8747492  -1.5085901 -0.2877497  0.27330443  0.8883679\n  eta[8]    0.06057562 0.9323394  -1.6899463 -0.5864536  0.03007708  0.7186805\n  theta[1] 11.93664296 8.8661074  -1.8322951  6.5261195 10.39641677 15.8024621\n  theta[2]  8.13858640 6.2720884  -5.2098070  4.5104608  8.05332736 12.0186045\n  theta[3]  6.40706617 7.7754159 -10.7513604  2.6023981  6.98826548 10.9046641\n  theta[4]  7.79463597 6.7970699  -6.2089815  3.9410127  7.71173461 11.8406025\n  theta[5]  5.74374317 6.4742331  -8.6747880  1.9901396  6.18363699  9.8017072\n  theta[6]  6.68675265 6.1641735  -6.5593358  2.9558672  7.11144483 10.7035672\n  theta[7] 10.60196469 6.6552632  -0.6750905  6.3221718  9.84516715 14.3359601\n  theta[8]  8.90212977 7.6528502  -6.2404706  4.4843181  8.58407628 13.2530573\n  lp__     -4.89105481 2.5517024 -10.5534623 -6.4349997 -4.61885489 -3.0983923\n          stats\nparameter       97.5%\n  mu       19.7533189\n  tau      18.6019300\n  eta[1]    2.2706853\n  eta[2]    1.6774047\n  eta[3]    1.6555280\n  eta[4]    1.6249366\n  eta[5]    1.4518943\n  eta[6]    1.5161620\n  eta[7]    1.9501146\n  eta[8]    1.9422304\n  theta[1] 34.6422306\n  theta[2] 20.3747862\n  theta[3] 22.0549434\n  theta[4] 21.7956785\n  theta[5] 17.1235708\n  theta[6] 17.8626937\n  theta[7] 25.6872509\n  theta[8] 25.8055607\n  lp__     -0.6692973\n\n, , chains = chain:5\n\n          stats\nparameter         mean        sd        2.5%        25%          50%        75%\n  mu        7.88789952 5.2968479  -3.1739389  4.7941353  7.936581174 11.0851131\n  tau       6.49968276 5.6275900   0.2933862  2.3355451  5.203383688  8.9535951\n  eta[1]    0.40926627 0.9273579  -1.6023623 -0.1483100  0.442381542  1.0190204\n  eta[2]    0.01174387 0.9496373  -1.7913149 -0.6635487 -0.001269867  0.6443615\n  eta[3]   -0.17059668 0.9561931  -2.0580794 -0.8297595 -0.195094958  0.4610610\n  eta[4]   -0.04583669 0.8920741  -1.7570111 -0.6370955 -0.052372765  0.5507579\n  eta[5]   -0.34070341 0.8832294  -2.0955712 -0.9400672 -0.336256555  0.2305865\n  eta[6]   -0.21940965 0.9197785  -2.0679843 -0.7843355 -0.221604668  0.3501175\n  eta[7]    0.26507534 0.8821249  -1.4431344 -0.3065250  0.274923652  0.8607893\n  eta[8]    0.05337650 0.9611311  -1.9539667 -0.6018330  0.031994549  0.6603284\n  theta[1] 11.42238212 8.0313287  -1.8515100  6.3042972 10.458319786 15.5981450\n  theta[2]  8.02661989 6.2991880  -4.5652951  4.3154461  8.067431618 12.0332342\n  theta[3]  6.44998187 8.2403939 -13.5917810  2.0755653  6.965794605 11.4371533\n  theta[4]  7.59030205 6.7866914  -6.7788374  3.4761028  7.768158189 12.0822328\n  theta[5]  5.27694502 6.9975740 -10.3148216  1.4341781  6.248980880  9.8514621\n  theta[6]  6.22942588 6.9814265 -10.4572939  2.4707043  6.665314539 10.7908328\n  theta[7] 10.38928427 6.7143272  -1.1111282  6.0216802  9.615524238 14.1214804\n  theta[8]  8.57333315 7.5871618  -6.4708957  4.3154441  8.513236219 12.4728154\n  lp__     -5.02582912 2.6971745 -11.2602533 -6.6779278 -4.650729965 -3.0834741\n          stats\nparameter      97.5%\n  mu       18.637779\n  tau      20.018943\n  eta[1]    2.170385\n  eta[2]    1.986170\n  eta[3]    1.704753\n  eta[4]    1.672707\n  eta[5]    1.395249\n  eta[6]    1.614642\n  eta[7]    1.935258\n  eta[8]    1.884664\n  theta[1] 30.400062\n  theta[2] 19.711725\n  theta[3] 21.530849\n  theta[4] 20.863305\n  theta[5] 17.008023\n  theta[6] 18.314343\n  theta[7] 24.953030\n  theta[8] 25.527128\n  lp__     -0.744831\n\n\nAlternate style using external file.\n\noptions(mc.cores = parallel::detectCores()-1)\nschool8.fit1 &lt;- stan(\n  file=\"school8.stan\",       # The model\n  data = school8.dat, # The data\n  chains = 5,\n  warmup = 1000,\n  iter = 2000,\n  refresh=100        # Show progress\n)\n\nWarning: There were 233 divergent transitions after warmup. See\nhttps://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\nto find out why this is a problem and how to eliminate them.\n\n\nWarning: Examine the pairs() plot to diagnose sampling problems\n\n\nWarning: The largest R-hat is 1.08, indicating chains have not mixed.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#r-hat\n\n\nWarning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#bulk-ess\n\n\nWarning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#tail-ess\n\nsummary(school8.fit1)\n\n2.444536   7.040953  10.401303\n  theta[4]   7.507625 6.252242  -5.411605   4.305377   7.164747  10.223777\n  theta[5]   5.352699 6.004565  -8.087215   1.868330   5.877882   8.461308\n  theta[6]   5.655399 6.537624  -9.533864   2.776726   5.715788   8.834715\n  theta[7]  10.233571 6.969352  -2.462557   5.948144   8.544680  14.348132\n  theta[8]   8.003704 7.355961  -6.278288   4.447453   8.093741  11.799609\n  lp__     -16.675897 6.007824 -27.120424 -21.387987 -17.224413 -11.477834\n          stats\nparameter     97.5%\n  mu       17.69822\n  tau      19.58059\n  theta[1] 28.99350\n  theta[2] 19.59113\n  theta[3] 21.22832\n  theta[4] 21.05145\n  theta[5] 17.40312\n  theta[6] 18.69120\n  theta[7] 26.93931\n  theta[8] 24.80068\n  lp__     -6.96373\n\n, , chains = chain:4\n\n          stats\nparameter        mean       sd        2.5%        25%        50%       75%\n  mu         7.807457 3.947565  -0.5354557   6.143475   7.827468  9.559913\n  tau        4.626939 4.227888   0.8404751   1.241994   3.348635  6.611204\n  theta[1]  10.140869 6.616249  -0.4700222   6.850292   8.512737 12.658141\n  theta[2]   7.760199 5.034573  -3.4875662   5.421400   8.366147  9.890144\n  theta[3]   6.841542 6.480835  -8.3835000   3.834625   7.298074  9.879202\n  theta[4]   7.464682 5.134219  -3.7308068   5.517495   7.021376  9.883241\n  theta[5]   6.072091 5.170162  -7.2718859   3.418545   7.714479  8.626607\n  theta[6]   6.730924 5.332127  -5.7074361   4.329503   7.761382  9.057138\n  theta[7]   9.716830 5.655594  -0.7444194   7.303205   8.810354 11.748011\n  theta[8]   7.924238 6.066356  -3.9011466   5.206314   7.317763 10.145656\n  lp__     -13.975460 6.687842 -25.2925621 -19.166760 -14.545492 -8.053060\n          stats\nparameter      97.5%\n  mu       15.845724\n  tau      15.627613\n  theta[1] 27.074990\n  theta[2] 18.550191\n  theta[3] 19.535936\n  theta[4] 18.183394\n  theta[5] 14.789921\n  theta[6] 16.210865\n  theta[7] 22.160988\n  theta[8] 21.712748\n  lp__     -4.169001\n\n, , chains = chain:5\n\n          stats\nparameter        mean       sd       2.5%         25%        50%        75%\n  mu         7.809227 5.753002  -3.540052   3.9092469   7.997191  11.524356\n  tau        7.741493 5.904724   1.182937   3.6291849   6.311963  10.454191\n  theta[1]  12.027258 9.546470  -3.429993   5.6852704  10.533546  17.041697\n  theta[2]   7.738394 6.784932  -5.469701   3.2753704   7.880612  11.921140\n  theta[3]   5.833821 7.886881 -11.564060   1.3506997   5.779110  10.608191\n  theta[4]   7.449897 6.997839  -6.370111   3.1381277   7.372843  11.542219\n  theta[5]   4.617703 6.773813  -8.477131   0.6694448   4.833342   9.073287\n  theta[6]   5.487537 7.362180 -11.166849   1.2774045   5.828258  10.462749\n  theta[7]  11.082788 7.347846  -1.566953   5.9998747  10.452288  14.912969\n  theta[8]   8.175263 8.591608  -8.094414   3.0607827   8.161392  13.135705\n  lp__     -18.775596 5.353685 -28.725127 -22.6867659 -18.983923 -15.256365\n          stats\nparameter      97.5%\n  mu       18.385645\n  tau      22.728494\n  theta[1] 34.352263\n  theta[2] 20.930151\n  theta[3] 21.262265\n  theta[4] 21.398070\n  theta[5] 17.499642\n  theta[6] 19.395661\n  theta[7] 27.724934\n  theta[8] 25.965795\n  lp__     -8.376766\n\n\n\n\n\nPrinting gives summaries of the posterior for the specified parameters. Use the pars argument to select what to print.\n\nprint(school8.fit1,pars=c(\"theta\",\"mu\",\"tau\",\"lp__\"), probs=c(.1,.5,.9))\n\nInference for Stan model: anon_model.\n5 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=5000.\n\n           mean se_mean   sd    10%    50%   90% n_eff Rhat\ntheta[1]  11.64    0.31 8.32   2.83   9.93 22.71   713 1.01\ntheta[2]   7.86    0.15 6.16   0.40   8.03 15.47  1670 1.00\ntheta[3]   6.10    0.19 7.96  -3.49   6.87 15.04  1693 1.00\ntheta[4]   7.55    0.15 6.45  -0.38   7.43 15.58  1765 1.00\ntheta[5]   5.23    0.20 6.36  -2.98   6.01 12.79  1020 1.00\ntheta[6]   5.89    0.20 6.79  -2.73   6.40 14.02  1194 1.00\ntheta[7]  10.77    0.24 6.80   3.12   9.78 19.59   791 1.01\ntheta[8]   8.27    0.18 7.68  -0.79   7.94 17.68  1887 1.00\nmu         7.92    0.16 5.08   1.77   7.83 14.19   988 1.00\ntau        6.78    0.46 5.46   1.38   5.42 13.77   140 1.04\nlp__     -17.29    0.84 6.03 -24.50 -18.13 -8.47    51 1.09\n\nSamples were drawn using NUTS(diag_e) at Sun Jan  4 20:11:41 2026.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\nThe plot function gives 50% and 95% intervals.\n\nplot(school8.fit1,pars=c(\"theta\",\"mu\",\"tau\"))\n\nci_level: 0.8 (80% intervals)\n\n\nouter_level: 0.95 (95% intervals)\n\n\n\n\n\n\n\n\n\nTraceplot shows convergence (note stan and coda have slightly different traceplot functions).\n\nrstan::traceplot(school8.fit1,pars=c(\"mu\",\"tau\",\"lp__\"), inc_warmup=TRUE, nrow=3)\n\n\n\n\n\n\n\n\nWhat we are looking for here is (a) white-noise like, and all chains plotting over top of each other.\nTypically variance and scale parameters will be positively skewed, and log-posterior is often not as well behaved as others.\nPair plot sometimes uncover problems with models.\n\n## This is not working, and I can't figure out why\n## pairs(school8.fit1,pars=c(\"mu\",\"eta[1]\",\"tau\"))\n\n\n\n\nShinystan opens a new shiny workspace which allows interactive browsing of the results.\n\nschool8.fit1s &lt;- launch_shinystan(school8.fit1)",
    "crumbs": [
      "8 Schools Stan Improved"
    ]
  },
  {
    "objectID": "Bayesian/Schools8Stan-improved.html#model-setup-in-stan.",
    "href": "Bayesian/Schools8Stan-improved.html#model-setup-in-stan.",
    "title": "8 Schools Stan Improved",
    "section": "",
    "text": "First we need to load the packages. The rstan package runs stan from R. The shinystan package gives us a browser for the results.\n\nlibrary(rstan)\n\nLoading required package: StanHeaders\n\n\n\nrstan version 2.32.7 (Stan version 2.32.2)\n\n\nFor execution on a local, multicore CPU with excess RAM we recommend calling\noptions(mc.cores = parallel::detectCores()).\nTo avoid recompilation of unchanged Stan programs, we recommend calling\nrstan_options(auto_write = TRUE)\nFor within-chain threading using `reduce_sum()` or `map_rect()` Stan functions,\nchange `threads_per_chain` option:\nrstan_options(threads_per_chain = 1)\n\nlibrary(shinystan)\n\nLoading required package: shiny\n\n\n\nThis is shinystan version 2.6.0\n\nlibrary(parallel) # For using multiple chains\noptions(mc.cores=5)\n\nFirst we set up the Stan model, and put it into a variable called school8 (note the output.var=\"school8\" in stan block tag)\n\ndata {\n  int&lt;lower=0&gt; J; // number of schools\n  real y[J]; // estimated treatement effects\n  real&lt;lower=0&gt; sigma[J]; //s.e. of effects.\n}\nparameters {\n  real mu;\n  real&lt;lower=0&gt; tau;\n  vector[J] eta;\n} \ntransformed parameters {\n  vector [J] theta;\n  theta = mu + eta*tau;\n}\nmodel {\n  eta ~ normal(0,1);\n  //target += normal_lpdf(theta|mu,tau);\n  y ~ normal(theta,sigma);\n  //target += normal_lpdf(effect|theta,see);\n}\n\n\n\nAlmost all models will have a data, parameters and model block. They could have others as well (common ones are transformed data to do pre-calculations and transformed parameters to recast the model).\n\n\nIn a stan model, data refers to values that don’t change over the course of the MCMC loop. This tends to be one of three things: * The observed data values. * Fixed hyperparameters for prior distributions. * Structural hyperparamters (e.g., sample size, number of groups).\nIn stan (like C++) all variables must be declared. They are generally either int or real (here vector is shorthand for a vector of real values). The modifiers &lt;lower=XXX&gt; and &lt;upper=XXX&gt; can be used to constrain the inputs. In the data field this is just used to to type checking.\n\n\n\nThe parameters are the values that stan will try to estimate. These include both latent variables and ordinary parameters and hyperparameters.\nIn stan, parameters should all be real. Non-continuous parameters don’t work with the Hamiltonian Monte Carlo. (There are tricks for dealing with common cases like mixture models in the stan examples.) This is called lp__ (log pdf) in the output.\nNote carefully the use of the lower and upper bounds. It is important for stan to know when a parameter is restricted to say positive values (i.e., a scale parameter) as it needs to constrain the space for the sampler.\n\n\n\nThis section gives the distribution for all of the parameters. First I give the BUGS-like way of doing this. This is to use the ~ operator to give the distribution. The names are slightly different in stan and R, so the rstan function lookup can help you find the stan function corresponding to the R function.\n\nlookup(dnorm)\n\n          StanFunction\n415 normal_id_glm_lpdf\n418         normal_log\n419        normal_lpdf\n553    std_normal_lpdf\n                                                                       Arguments\n415 (real, matrix, real, vector, T);(vector, row_vector, vector, vector, vector)\n418                                     (real, real, T);(vector, vector, vector)\n419                                     (real, real, T);(vector, vector, vector)\n553                                                                 (T);(vector)\n    ReturnType\n415     T;real\n418     T;real\n419     T;real\n553     T;real\n\nlookup(dt)\n\n      StanFunction                                              Arguments\n562  student_t_log (real, real, real, T);(vector, vector, vector, vector)\n563 student_t_lpdf (real, real, real, T);(vector, vector, vector, vector)\n    ReturnType\n562     T;real\n563     T;real\n\n\nNote that what stan is actually doing in the model block is calculating the log p.d.f. Thus, the commented out expressions are an alternative to the ~ notation.\nFinally, note that this is an incomplete Bayesian model as there are no distributions for mu or tau. In stan this means we are implicitly putting a uniform prior on mu and log(tau); the latter is transformed so that it will always be positive. These are improper priors, but stan will be fine as long as the posterior is proper.",
    "crumbs": [
      "8 Schools Stan Improved"
    ]
  },
  {
    "objectID": "Bayesian/Schools8Stan-improved.html#data-preparation",
    "href": "Bayesian/Schools8Stan-improved.html#data-preparation",
    "title": "8 Schools Stan Improved",
    "section": "",
    "text": "Build a list which contains the data as elements using the names in the data section of the model.\n\nschool8.dat &lt;- list(\n  J = nrow(Schools),\n  y = Schools$effect,\n  sigma=Schools$see)",
    "crumbs": [
      "8 Schools Stan Improved"
    ]
  },
  {
    "objectID": "Bayesian/Schools8Stan-improved.html#running-stan",
    "href": "Bayesian/Schools8Stan-improved.html#running-stan",
    "title": "8 Schools Stan Improved",
    "section": "",
    "text": "There are two funcitons to start the sampling in stan. The first one is stan(file=XXX,data=YYY,...). This assumes that the stan model is in a file. However, with R markdown, we already saved the model in an object so we can use sampling(model,data=YYY,...). By the way, this same function can be used to make additional samples after we have sampled for a while.\nThe mc.cores should work well for Unix (Linux and Mac OS), I’m not so sure about Windows.\n\n#options(mc.cores = parallel::detectCores()-1)\nschool8.fit1 &lt;- sampling(\n  school8,       # The model\n  data = school8.dat, # The data\n  chains = 5,\n  warmup = 1000,\n  iter = 2000,\n  refresh=1000        # Show progress\n)\n\nWarning: There were 3 divergent transitions after warmup. See\nhttps://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\nto find out why this is a problem and how to eliminate them.\n\n\nWarning: Examine the pairs() plot to diagnose sampling problems\n\nsummary(school8.fit1)\n\n4775\ntau       9.1052115 19.7136121 2327.896 1.0015684\neta[1]    1.0315979  2.1702931 4867.594 1.0005030\neta[2]    0.6038292  1.7575377 5039.402 0.9995612\neta[3]    0.4435779  1.6478861 5245.409 0.9993708\neta[4]    0.5638652  1.6657116 5558.884 0.9997504\neta[5]    0.2340142  1.4861669 5305.889 0.9998699\neta[6]    0.3620663  1.5623228 4894.520 1.0001554\neta[7]    0.9115123  2.0291966 4784.759 1.0004121\neta[8]    0.6889746  1.8865260 5513.766 0.9996236\ntheta[1] 15.7714754 32.8230629 3370.790 1.0005775\ntheta[2] 11.9793957 20.3729747 5464.851 0.9997725\ntheta[3] 11.1034097 21.0252281 4373.520 1.0001817\ntheta[4] 11.8827749 21.1340201 5362.160 0.9998073\ntheta[5]  9.8017072 16.9570110 5526.486 1.0007979\ntheta[6] 10.6206478 18.4255261 5500.031 1.0009768\ntheta[7] 14.5116616 25.1983460 4325.928 1.0001939\ntheta[8] 12.9424748 25.9498102 4572.715 1.0002053\nlp__     -3.0313726 -0.5140309 1560.513 1.0032085\n\n$c_summary\n, , chains = chain:1\n\n          stats\nparameter          mean        sd        2.5%        25%         50%        75%\n  mu        8.108977919 4.5414579  -1.3735888  5.1920601  8.16485328 11.1048192\n  tau       6.196502314 5.0705968   0.3051799  2.4856176  4.99387040  8.4951222\n  eta[1]    0.377026768 0.9516892  -1.5356402 -0.2752460  0.37186417  1.0062744\n  eta[2]   -0.005839293 0.8685292  -1.7253996 -0.5446877 -0.02515838  0.5468288\n  eta[3]   -0.182964412 0.9571102  -2.0553740 -0.8061908 -0.19696588  0.4582568\n  eta[4]   -0.047212212 0.8975976  -1.7863226 -0.6436490 -0.03972658  0.5347721\n  eta[5]   -0.337766958 0.9182210  -2.1641677 -0.9070398 -0.33365592  0.2405289\n  eta[6]   -0.228016366 0.8637041  -1.8619810 -0.8047200 -0.22061596  0.3390450\n  eta[7]    0.298692959 0.9158723  -1.5002286 -0.2954845  0.29221986  0.8677758\n  eta[8]    0.044490389 0.9421667  -1.7426750 -0.6191457  0.03489879  0.6973535\n  theta[1] 11.527464459 8.4252323  -1.6984561  6.3422959 10.51978536 15.3487061\n  theta[2]  7.938971310 5.8333094  -3.5050637  4.3669240  8.01192729 11.5234189\n  theta[3]  6.547431266 7.8296833 -11.5778553  3.1088068  7.29252875 11.1371943\n  theta[4]  8.036214206 6.1828977  -3.7091480  4.2985883  7.96721128 11.4889555\n  theta[5]  5.792198654 6.4551752  -8.9274652  2.0787198  6.23225467  9.9841633\n  theta[6]  6.431389216 6.5422285  -9.6454064  2.7619612  7.16121278 10.5606060\n  theta[7] 10.543852816 6.4320421  -0.3579881  6.1798853 10.04096160 14.1887795\n  theta[8]  8.782358158 7.6219863  -7.1582848  4.4444597  8.57645560 12.7680413\n  lp__     -4.870801197 2.5488115 -10.9226984 -6.4380852 -4.68081643 -3.0915766\n          stats\nparameter       97.5%\n  mu       16.9712636\n  tau      18.5799679\n  eta[1]    2.1481498\n  eta[2]    1.6823520\n  eta[3]    1.7362172\n  eta[4]    1.6954624\n  eta[5]    1.4922502\n  eta[6]    1.5142767\n  eta[7]    2.1457115\n  eta[8]    1.8410109\n  theta[1] 31.9545933\n  theta[2] 20.2092511\n  theta[3] 19.5321853\n  theta[4] 21.3236209\n  theta[5] 17.2868457\n  theta[6] 18.1561487\n  theta[7] 24.8549275\n  theta[8] 25.5726002\n  lp__     -0.5535598\n\n, , chains = chain:2\n\n          stats\nparameter          mean        sd        2.5%        25%         50%        75%\n  mu        7.987985477 4.9485908  -1.7741590  4.5938001  7.70471754 11.2855018\n  tau       6.499956713 5.3508862   0.2214177  2.5013624  5.25135778  9.0830559\n  eta[1]    0.421067042 0.9525111  -1.6066398 -0.2255225  0.45065971  1.0714713\n  eta[2]    0.034622963 0.8693805  -1.7301475 -0.5639677  0.01826661  0.6301161\n  eta[3]   -0.194723788 0.9206019  -2.0368697 -0.7970403 -0.15026239  0.4635916\n  eta[4]   -0.001710459 0.8635883  -1.7109605 -0.5872694 -0.02683828  0.5775723\n  eta[5]   -0.310222863 0.9203612  -2.1063580 -0.9076318 -0.30167275  0.2938009\n  eta[6]   -0.210230107 0.8668066  -1.9233771 -0.8001534 -0.22322717  0.3614584\n  eta[7]    0.340137258 0.9504863  -1.5561022 -0.2761296  0.38080872  0.9456390\n  eta[8]    0.082262061 0.9285287  -1.7977866 -0.6002183  0.06180018  0.6998272\n  theta[1] 11.626050013 8.5608429  -3.5768229  6.2308734 10.54640986 15.7830646\n  theta[2]  8.231382568 6.2648629  -5.2720175  4.5376533  8.20970960 12.2993767\n  theta[3]  6.289476595 8.1186267 -13.0287501  2.3455710  6.91186800 11.2359355\n  theta[4]  7.798334347 6.2596044  -4.8754885  3.9974516  7.75565326 11.8550496\n  theta[5]  5.376882498 6.4380220  -8.5264248  1.5266789  6.04329046  9.7377413\n  theta[6]  6.194451030 6.8106011  -8.1767518  2.6580334  6.71880474 10.6567160\n  theta[7] 10.560320941 6.8518688  -2.3827422  6.0608562 10.33902352 14.4815048\n  theta[8]  8.677443402 7.3595251  -6.1942521  4.4943854  8.35334916 12.9665546\n  lp__     -4.859834219 2.6400012 -10.6963718 -6.4683610 -4.53241189 -3.0195196\n          stats\nparameter       97.5%\n  mu       17.6452037\n  tau      20.5068063\n  eta[1]    2.2748092\n  eta[2]    1.6874192\n  eta[3]    1.4905939\n  eta[4]    1.7217261\n  eta[5]    1.6425875\n  eta[6]    1.5324748\n  eta[7]    2.2551346\n  eta[8]    1.9195188\n  theta[1] 34.3108883\n  theta[2] 19.6477325\n  theta[3] 21.0252281\n  theta[4] 20.2212760\n  theta[5] 16.4239411\n  theta[6] 18.3513917\n  theta[7] 25.2776176\n  theta[8] 25.3181385\n  lp__     -0.2811735\n\n, , chains = chain:3\n\n          stats\nparameter          mean        sd        2.5%        25%           50%\n  mu        7.984119163 5.2850843  -2.6906032  4.4702459  7.9669988035\n  tau       6.780726194 5.3925022   0.3420370  2.6894762  5.3488883323\n  eta[1]    0.422059210 0.8786949  -1.4036719 -0.1566890  0.4748823818\n  eta[2]   -0.005168801 0.9233485  -1.8010965 -0.6185735 -0.0006456158\n  eta[3]   -0.197612781 0.9128971  -2.0036544 -0.7828097 -0.1697648829\n  eta[4]   -0.029129209 0.8274659  -1.6977804 -0.5534801 -0.0786573035\n  eta[5]   -0.339073873 0.8813990  -2.1100295 -0.9022798 -0.3226100845\n  eta[6]   -0.240538025 0.9570248  -2.0828795 -0.8767444 -0.3168690590\n  eta[7]    0.370962848 0.8522918  -1.3383904 -0.2069558  0.3670871187\n  eta[8]    0.057490341 0.8982614  -1.7181044 -0.5457274  0.0319265505\n  theta[1] 11.858118376 8.6589213  -2.2177205  6.3145163 10.5151562196\n  theta[2]  8.077483688 6.5248129  -4.5663661  3.9543494  7.7995007755\n  theta[3]  6.430177567 7.3508015 -10.7982961  2.6259719  6.7403931349\n  theta[4]  7.651956720 6.6603158  -6.1989833  3.1791978  7.8470966484\n  theta[5]  5.212598772 6.6982450 -10.0382583  1.0732341  5.6817361269\n  theta[6]  5.956522723 7.1653881  -8.9729190  1.9823107  6.1762180114\n  theta[7] 11.005251355 6.7836879  -0.8372271  6.2917144 10.5131491035\n  theta[8]  8.394853046 8.1271922  -7.2379880  3.6046370  8.2098218925\n  lp__     -4.681947470 2.5433275 -10.2939556 -6.2493706 -4.3799520615\n          stats\nparameter         75%      97.5%\n  mu       11.4501720 18.0367866\n  tau       9.5751040 19.4194335\n  eta[1]    1.0321554  2.0314540\n  eta[2]    0.5718271  1.8079509\n  eta[3]    0.4018604  1.5354504\n  eta[4]    0.5638104  1.5982821\n  eta[5]    0.2142210  1.4652254\n  eta[6]    0.3970379  1.6284067\n  eta[7]    0.9808142  1.9911270\n  eta[8]    0.6558605  1.8397262\n  theta[1] 16.0178470 32.7342324\n  theta[2] 12.2099101 20.9826191\n  theta[3] 10.8044350 19.9495177\n  theta[4] 11.9809052 20.4077498\n  theta[5]  9.7347633 16.8115437\n  theta[6] 10.4993680 19.4004811\n  theta[7] 15.2306949 25.2127880\n  theta[8] 13.1711533 27.4107701\n  lp__     -2.8813004 -0.4347474\n\n, , chains = chain:4\n\n          stats\nparameter         mean        sd        2.5%        25%         50%        75%\n  mu        8.44665379 5.3352691  -2.1109467  5.2235058  8.43858013 11.6889727\n  tau       6.36235338 5.1244939   0.2145530  2.3905578  5.18501654  9.3451497\n  eta[1]    0.39351569 0.9526534  -1.5048961 -0.2352222  0.37309645  1.0415627\n  eta[2]   -0.01394600 0.8948069  -1.7834092 -0.6606825 -0.02510561  0.5879319\n  eta[3]   -0.20981645 0.9618919  -2.0412251 -0.8562945 -0.25376539  0.4067044\n  eta[4]   -0.06472502 0.9085278  -1.8483946 -0.6858356 -0.08699758  0.5870187\n  eta[5]   -0.35049693 0.8598175  -2.0637544 -0.8785655 -0.36019478  0.1749668\n  eta[6]   -0.19764028 0.8677715  -1.7752442 -0.7741922 -0.21515830  0.3610380\n  eta[7]    0.28158021 0.8747492  -1.5085901 -0.2877497  0.27330443  0.8883679\n  eta[8]    0.06057562 0.9323394  -1.6899463 -0.5864536  0.03007708  0.7186805\n  theta[1] 11.93664296 8.8661074  -1.8322951  6.5261195 10.39641677 15.8024621\n  theta[2]  8.13858640 6.2720884  -5.2098070  4.5104608  8.05332736 12.0186045\n  theta[3]  6.40706617 7.7754159 -10.7513604  2.6023981  6.98826548 10.9046641\n  theta[4]  7.79463597 6.7970699  -6.2089815  3.9410127  7.71173461 11.8406025\n  theta[5]  5.74374317 6.4742331  -8.6747880  1.9901396  6.18363699  9.8017072\n  theta[6]  6.68675265 6.1641735  -6.5593358  2.9558672  7.11144483 10.7035672\n  theta[7] 10.60196469 6.6552632  -0.6750905  6.3221718  9.84516715 14.3359601\n  theta[8]  8.90212977 7.6528502  -6.2404706  4.4843181  8.58407628 13.2530573\n  lp__     -4.89105481 2.5517024 -10.5534623 -6.4349997 -4.61885489 -3.0983923\n          stats\nparameter       97.5%\n  mu       19.7533189\n  tau      18.6019300\n  eta[1]    2.2706853\n  eta[2]    1.6774047\n  eta[3]    1.6555280\n  eta[4]    1.6249366\n  eta[5]    1.4518943\n  eta[6]    1.5161620\n  eta[7]    1.9501146\n  eta[8]    1.9422304\n  theta[1] 34.6422306\n  theta[2] 20.3747862\n  theta[3] 22.0549434\n  theta[4] 21.7956785\n  theta[5] 17.1235708\n  theta[6] 17.8626937\n  theta[7] 25.6872509\n  theta[8] 25.8055607\n  lp__     -0.6692973\n\n, , chains = chain:5\n\n          stats\nparameter         mean        sd        2.5%        25%          50%        75%\n  mu        7.88789952 5.2968479  -3.1739389  4.7941353  7.936581174 11.0851131\n  tau       6.49968276 5.6275900   0.2933862  2.3355451  5.203383688  8.9535951\n  eta[1]    0.40926627 0.9273579  -1.6023623 -0.1483100  0.442381542  1.0190204\n  eta[2]    0.01174387 0.9496373  -1.7913149 -0.6635487 -0.001269867  0.6443615\n  eta[3]   -0.17059668 0.9561931  -2.0580794 -0.8297595 -0.195094958  0.4610610\n  eta[4]   -0.04583669 0.8920741  -1.7570111 -0.6370955 -0.052372765  0.5507579\n  eta[5]   -0.34070341 0.8832294  -2.0955712 -0.9400672 -0.336256555  0.2305865\n  eta[6]   -0.21940965 0.9197785  -2.0679843 -0.7843355 -0.221604668  0.3501175\n  eta[7]    0.26507534 0.8821249  -1.4431344 -0.3065250  0.274923652  0.8607893\n  eta[8]    0.05337650 0.9611311  -1.9539667 -0.6018330  0.031994549  0.6603284\n  theta[1] 11.42238212 8.0313287  -1.8515100  6.3042972 10.458319786 15.5981450\n  theta[2]  8.02661989 6.2991880  -4.5652951  4.3154461  8.067431618 12.0332342\n  theta[3]  6.44998187 8.2403939 -13.5917810  2.0755653  6.965794605 11.4371533\n  theta[4]  7.59030205 6.7866914  -6.7788374  3.4761028  7.768158189 12.0822328\n  theta[5]  5.27694502 6.9975740 -10.3148216  1.4341781  6.248980880  9.8514621\n  theta[6]  6.22942588 6.9814265 -10.4572939  2.4707043  6.665314539 10.7908328\n  theta[7] 10.38928427 6.7143272  -1.1111282  6.0216802  9.615524238 14.1214804\n  theta[8]  8.57333315 7.5871618  -6.4708957  4.3154441  8.513236219 12.4728154\n  lp__     -5.02582912 2.6971745 -11.2602533 -6.6779278 -4.650729965 -3.0834741\n          stats\nparameter      97.5%\n  mu       18.637779\n  tau      20.018943\n  eta[1]    2.170385\n  eta[2]    1.986170\n  eta[3]    1.704753\n  eta[4]    1.672707\n  eta[5]    1.395249\n  eta[6]    1.614642\n  eta[7]    1.935258\n  eta[8]    1.884664\n  theta[1] 30.400062\n  theta[2] 19.711725\n  theta[3] 21.530849\n  theta[4] 20.863305\n  theta[5] 17.008023\n  theta[6] 18.314343\n  theta[7] 24.953030\n  theta[8] 25.527128\n  lp__     -0.744831\n\n\nAlternate style using external file.\n\noptions(mc.cores = parallel::detectCores()-1)\nschool8.fit1 &lt;- stan(\n  file=\"school8.stan\",       # The model\n  data = school8.dat, # The data\n  chains = 5,\n  warmup = 1000,\n  iter = 2000,\n  refresh=100        # Show progress\n)\n\nWarning: There were 233 divergent transitions after warmup. See\nhttps://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\nto find out why this is a problem and how to eliminate them.\n\n\nWarning: Examine the pairs() plot to diagnose sampling problems\n\n\nWarning: The largest R-hat is 1.08, indicating chains have not mixed.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#r-hat\n\n\nWarning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#bulk-ess\n\n\nWarning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#tail-ess\n\nsummary(school8.fit1)\n\n2.444536   7.040953  10.401303\n  theta[4]   7.507625 6.252242  -5.411605   4.305377   7.164747  10.223777\n  theta[5]   5.352699 6.004565  -8.087215   1.868330   5.877882   8.461308\n  theta[6]   5.655399 6.537624  -9.533864   2.776726   5.715788   8.834715\n  theta[7]  10.233571 6.969352  -2.462557   5.948144   8.544680  14.348132\n  theta[8]   8.003704 7.355961  -6.278288   4.447453   8.093741  11.799609\n  lp__     -16.675897 6.007824 -27.120424 -21.387987 -17.224413 -11.477834\n          stats\nparameter     97.5%\n  mu       17.69822\n  tau      19.58059\n  theta[1] 28.99350\n  theta[2] 19.59113\n  theta[3] 21.22832\n  theta[4] 21.05145\n  theta[5] 17.40312\n  theta[6] 18.69120\n  theta[7] 26.93931\n  theta[8] 24.80068\n  lp__     -6.96373\n\n, , chains = chain:4\n\n          stats\nparameter        mean       sd        2.5%        25%        50%       75%\n  mu         7.807457 3.947565  -0.5354557   6.143475   7.827468  9.559913\n  tau        4.626939 4.227888   0.8404751   1.241994   3.348635  6.611204\n  theta[1]  10.140869 6.616249  -0.4700222   6.850292   8.512737 12.658141\n  theta[2]   7.760199 5.034573  -3.4875662   5.421400   8.366147  9.890144\n  theta[3]   6.841542 6.480835  -8.3835000   3.834625   7.298074  9.879202\n  theta[4]   7.464682 5.134219  -3.7308068   5.517495   7.021376  9.883241\n  theta[5]   6.072091 5.170162  -7.2718859   3.418545   7.714479  8.626607\n  theta[6]   6.730924 5.332127  -5.7074361   4.329503   7.761382  9.057138\n  theta[7]   9.716830 5.655594  -0.7444194   7.303205   8.810354 11.748011\n  theta[8]   7.924238 6.066356  -3.9011466   5.206314   7.317763 10.145656\n  lp__     -13.975460 6.687842 -25.2925621 -19.166760 -14.545492 -8.053060\n          stats\nparameter      97.5%\n  mu       15.845724\n  tau      15.627613\n  theta[1] 27.074990\n  theta[2] 18.550191\n  theta[3] 19.535936\n  theta[4] 18.183394\n  theta[5] 14.789921\n  theta[6] 16.210865\n  theta[7] 22.160988\n  theta[8] 21.712748\n  lp__     -4.169001\n\n, , chains = chain:5\n\n          stats\nparameter        mean       sd       2.5%         25%        50%        75%\n  mu         7.809227 5.753002  -3.540052   3.9092469   7.997191  11.524356\n  tau        7.741493 5.904724   1.182937   3.6291849   6.311963  10.454191\n  theta[1]  12.027258 9.546470  -3.429993   5.6852704  10.533546  17.041697\n  theta[2]   7.738394 6.784932  -5.469701   3.2753704   7.880612  11.921140\n  theta[3]   5.833821 7.886881 -11.564060   1.3506997   5.779110  10.608191\n  theta[4]   7.449897 6.997839  -6.370111   3.1381277   7.372843  11.542219\n  theta[5]   4.617703 6.773813  -8.477131   0.6694448   4.833342   9.073287\n  theta[6]   5.487537 7.362180 -11.166849   1.2774045   5.828258  10.462749\n  theta[7]  11.082788 7.347846  -1.566953   5.9998747  10.452288  14.912969\n  theta[8]   8.175263 8.591608  -8.094414   3.0607827   8.161392  13.135705\n  lp__     -18.775596 5.353685 -28.725127 -22.6867659 -18.983923 -15.256365\n          stats\nparameter      97.5%\n  mu       18.385645\n  tau      22.728494\n  theta[1] 34.352263\n  theta[2] 20.930151\n  theta[3] 21.262265\n  theta[4] 21.398070\n  theta[5] 17.499642\n  theta[6] 19.395661\n  theta[7] 27.724934\n  theta[8] 25.965795\n  lp__     -8.376766",
    "crumbs": [
      "8 Schools Stan Improved"
    ]
  },
  {
    "objectID": "Bayesian/Schools8Stan-improved.html#summaries-using-the-base-stan-functions",
    "href": "Bayesian/Schools8Stan-improved.html#summaries-using-the-base-stan-functions",
    "title": "8 Schools Stan Improved",
    "section": "",
    "text": "Printing gives summaries of the posterior for the specified parameters. Use the pars argument to select what to print.\n\nprint(school8.fit1,pars=c(\"theta\",\"mu\",\"tau\",\"lp__\"), probs=c(.1,.5,.9))\n\nInference for Stan model: anon_model.\n5 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=5000.\n\n           mean se_mean   sd    10%    50%   90% n_eff Rhat\ntheta[1]  11.64    0.31 8.32   2.83   9.93 22.71   713 1.01\ntheta[2]   7.86    0.15 6.16   0.40   8.03 15.47  1670 1.00\ntheta[3]   6.10    0.19 7.96  -3.49   6.87 15.04  1693 1.00\ntheta[4]   7.55    0.15 6.45  -0.38   7.43 15.58  1765 1.00\ntheta[5]   5.23    0.20 6.36  -2.98   6.01 12.79  1020 1.00\ntheta[6]   5.89    0.20 6.79  -2.73   6.40 14.02  1194 1.00\ntheta[7]  10.77    0.24 6.80   3.12   9.78 19.59   791 1.01\ntheta[8]   8.27    0.18 7.68  -0.79   7.94 17.68  1887 1.00\nmu         7.92    0.16 5.08   1.77   7.83 14.19   988 1.00\ntau        6.78    0.46 5.46   1.38   5.42 13.77   140 1.04\nlp__     -17.29    0.84 6.03 -24.50 -18.13 -8.47    51 1.09\n\nSamples were drawn using NUTS(diag_e) at Sun Jan  4 20:11:41 2026.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\nThe plot function gives 50% and 95% intervals.\n\nplot(school8.fit1,pars=c(\"theta\",\"mu\",\"tau\"))\n\nci_level: 0.8 (80% intervals)\n\n\nouter_level: 0.95 (95% intervals)\n\n\n\n\n\n\n\n\n\nTraceplot shows convergence (note stan and coda have slightly different traceplot functions).\n\nrstan::traceplot(school8.fit1,pars=c(\"mu\",\"tau\",\"lp__\"), inc_warmup=TRUE, nrow=3)\n\n\n\n\n\n\n\n\nWhat we are looking for here is (a) white-noise like, and all chains plotting over top of each other.\nTypically variance and scale parameters will be positively skewed, and log-posterior is often not as well behaved as others.\nPair plot sometimes uncover problems with models.\n\n## This is not working, and I can't figure out why\n## pairs(school8.fit1,pars=c(\"mu\",\"eta[1]\",\"tau\"))",
    "crumbs": [
      "8 Schools Stan Improved"
    ]
  },
  {
    "objectID": "Bayesian/Schools8Stan-improved.html#shinystan",
    "href": "Bayesian/Schools8Stan-improved.html#shinystan",
    "title": "8 Schools Stan Improved",
    "section": "",
    "text": "Shinystan opens a new shiny workspace which allows interactive browsing of the results.\n\nschool8.fit1s &lt;- launch_shinystan(school8.fit1)",
    "crumbs": [
      "8 Schools Stan Improved"
    ]
  },
  {
    "objectID": "Bayesian/Schools8Stan-improved.html#footnotes",
    "href": "Bayesian/Schools8Stan-improved.html#footnotes",
    "title": "8 Schools Stan Improved",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRubin, D. B. (1981). Estimation in Parallel randomized experiments. Journal of Educational Statistics, 6, 377-401.↩︎\nGelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A. and Rubin, D. B. (2014). Bayesian Data Analysis: Third Edition. CRC Press. (ISBN: 978-1-4398-4095-5)↩︎",
    "crumbs": [
      "8 Schools Stan Improved"
    ]
  },
  {
    "objectID": "IntroStats/ConfidenceInterval.html",
    "href": "IntroStats/ConfidenceInterval.html",
    "title": "Confidence Intervals",
    "section": "",
    "text": "The simple heuristic for the confidence interval is that if we take a sample and calculate an estimator and a standard error for that estimator; 95% of the time the estimand will be within 2 standard errors of the estimate. This heuristic works best for sample means, because by the central limit theorem the distribution of the sample mean will be approximately normal. It also works fairly well for other statistics, like regression slopes.\nLet the goal be to produce an interval which \\((1-\\alpha)\\)% of the time captures the estimand. If we assume that the estimate, \\(\\widetilde{f(Y)}\\), is approximately normally distributed with a mean of the estimand, \\(f(Y)\\) and a standard deviation of the standard error of the estimate, \\(\\sigma_{\\widetilde{f(Y)}}\\), (i.e., we are assuming that central limit theorem holds at least approximately for \\(\\widetilde{f(Y)}\\)), then we can produce a confidence interval using the following expression: \\[ \\widetilde{f(Y)} \\pm z_{1-\\alpha/2}\\ \\sigma_{\\widetilde{f(Y)}}\\; .\\]\nUgh. Lets break this apart using an example. Let’s say what we are interested in is the sample mean. Then \\(\\widetilde{f(Y)} = \\bar Y\\) is just the sample mean. The quantile \\(z_{1-\\alpha/2}\\) depends on the desired accuracy. The default choice is \\(1-\\alpha=.95\\), so \\(1-\\alpha/2 = .975\\), and looking this up on the normal table \\(z_{.975}\\approx 1.96 \\approx 2\\). Finally, \\(\\sigma_{\\widetilde{f(Y)}}\\) is the standard error of the mean, so if the population standard deviation of \\(Y\\) is \\(\\sigma_Y\\) and the sample size is \\(N\\), then \\(\\sigma_{\\widetilde{f(Y)}} = \\sigma_Y/\\sqrt{N}\\). This gives us the slightly easier to look at: \\[\\bar Y \\pm 1.96 \\sigma_Y/\\sqrt{N} \\approx \\bar Y \\pm 2 \\sigma_Y/\\sqrt{N}\\ ;\\] Note that we are assuming that \\(\\sigma_Y\\) is known here. If we need to estimate it from the data, we need a slightly different formula given later."
  },
  {
    "objectID": "IntroStats/ConfidenceInterval.html#confidence-intervals",
    "href": "IntroStats/ConfidenceInterval.html#confidence-intervals",
    "title": "Confidence Intervals",
    "section": "",
    "text": "The simple heuristic for the confidence interval is that if we take a sample and calculate an estimator and a standard error for that estimator; 95% of the time the estimand will be within 2 standard errors of the estimate. This heuristic works best for sample means, because by the central limit theorem the distribution of the sample mean will be approximately normal. It also works fairly well for other statistics, like regression slopes.\nLet the goal be to produce an interval which \\((1-\\alpha)\\)% of the time captures the estimand. If we assume that the estimate, \\(\\widetilde{f(Y)}\\), is approximately normally distributed with a mean of the estimand, \\(f(Y)\\) and a standard deviation of the standard error of the estimate, \\(\\sigma_{\\widetilde{f(Y)}}\\), (i.e., we are assuming that central limit theorem holds at least approximately for \\(\\widetilde{f(Y)}\\)), then we can produce a confidence interval using the following expression: \\[ \\widetilde{f(Y)} \\pm z_{1-\\alpha/2}\\ \\sigma_{\\widetilde{f(Y)}}\\; .\\]\nUgh. Lets break this apart using an example. Let’s say what we are interested in is the sample mean. Then \\(\\widetilde{f(Y)} = \\bar Y\\) is just the sample mean. The quantile \\(z_{1-\\alpha/2}\\) depends on the desired accuracy. The default choice is \\(1-\\alpha=.95\\), so \\(1-\\alpha/2 = .975\\), and looking this up on the normal table \\(z_{.975}\\approx 1.96 \\approx 2\\). Finally, \\(\\sigma_{\\widetilde{f(Y)}}\\) is the standard error of the mean, so if the population standard deviation of \\(Y\\) is \\(\\sigma_Y\\) and the sample size is \\(N\\), then \\(\\sigma_{\\widetilde{f(Y)}} = \\sigma_Y/\\sqrt{N}\\). This gives us the slightly easier to look at: \\[\\bar Y \\pm 1.96 \\sigma_Y/\\sqrt{N} \\approx \\bar Y \\pm 2 \\sigma_Y/\\sqrt{N}\\ ;\\] Note that we are assuming that \\(\\sigma_Y\\) is known here. If we need to estimate it from the data, we need a slightly different formula given later."
  },
  {
    "objectID": "IntroStats/ConfidenceInterval.html#catching-fish-in-our-net",
    "href": "IntroStats/ConfidenceInterval.html#catching-fish-in-our-net",
    "title": "Confidence Intervals",
    "section": "Catching Fish in Our Net",
    "text": "Catching Fish in Our Net\nThere are two interpretations of confidence intervals (c.i.s): classical and Bayesian (although the latter are often called credibility intervals to distinguish them). As the Bayesian interpretation requires fewer assumptions, we will explore it first.\nIn the classical interpretation the c.i., the c.i. is like a net that is cast into the sea. It either will or will not catch the fish (the estimand). On average, the c.i. will catch the fish \\((1-\\alpha)\\)% of the time; this probability comes from the sampling. On a given time, we either will or will not have the fish in the net, but if we through it out many times, we will catch the fish \\((1-\\alpha)\\)% of the time."
  },
  {
    "objectID": "IntroStats/ConfidenceInterval.html#random-points",
    "href": "IntroStats/ConfidenceInterval.html#random-points",
    "title": "Confidence Intervals",
    "section": "Random Points",
    "text": "Random Points\nSelect the number of repetitions (how many times we through the net), the sample size (the size of the net) as well as the parameters of the population.\nYou may need to press the regenerate button to get the graph to have the right symbols.\n\n\n\n\n\n\nNumber of Repetitions:\n\n50\n100\n200\n\n\n\n\n\n\nSample Size:\n\n1\n5\n10\n25\n50\n100\n\n\n\n\n\n\nMean of Y:\n\n\n\n\n\nStandard Deviation of Y:\n\n\n\n\n(Re)Generate\n\n\n\n\n\n\n\n\n\n\n\n\nApproximaly 2/3 of the data points should be within 1 SE of the mean (plotted as circles)\nApproximately 95 percent of the data points should be within 1 SE of the mean (circles and triangles).\nApproximately 5 percent of the data points should be 2 SEs or more away from the mean (plotted at diamonds).\n\nNote that changing the mean and sd of the population only changes the scales on the graph, not the structure of the problem."
  },
  {
    "objectID": "IntroStats/ConfidenceInterval.html#random-intervals",
    "href": "IntroStats/ConfidenceInterval.html#random-intervals",
    "title": "Confidence Intervals",
    "section": "Random Intervals",
    "text": "Random Intervals\nTaking the sample mean and going plus or minus two standard errors produces a confidence interval.\nActually, the two standard error rule is based on looking up the .975 (1-.05/2) point on the normal table. We could put other values in there as well (50%, 75%, 90% and 99% are common choices). This will adjust the length of the slider.\n\n\n\n\n\n\nConfidence\n\n\n\n\n\n\n\n\n\n\nThe number of confidence intervals that don’t overlap the target line should be around \\(\\alpha\\) (the number in the slider) of the total number of intervals.\n\nThis graph and the random points above are based on the same data. For the 95% interval; the data points where the intervals don’t cross correspond to the data points outside of the 95% region."
  },
  {
    "objectID": "IntroStats/ConfidenceInterval.html#interpreting-confidence-intervals",
    "href": "IntroStats/ConfidenceInterval.html#interpreting-confidence-intervals",
    "title": "Confidence Intervals",
    "section": "Interpreting Confidence Intervals",
    "text": "Interpreting Confidence Intervals\nThe number \\(\\alpha\\), most often 95%, is known as the level of the confidence interval. The level is interpreted as a probability, but there are two schools of thought for interpreting it.\n\nClassical Approach\nThe classical statistical paradigm regards the population mean as a fixed but unknown quantity. The true value is either in or not in the interval, we don’t know which.\nRandom variability comes from the sampling, therefore the 95% comes from imagining different worlds in which we repeated the same sampling and analysis over and over again. 95% of the time, our net (the interval) will catch the fish.\nIn the classical paradigm, we can’t say that their is a 95% chance that the fish is in the net, as we can’t express the position of the fish as a probability: only the position of the net.\n\n\nBayesian Approach\nThe Bayesian paradigm makes the position of the fish a random variable. To do that, however, it needs an additional assumption: a probability distribution for the initial position of the fish.\nFor simplicity, assume that all positions of the fish are [equally likely]1. Using that assumption and Bayes’s Theorem, calculate the posterior probability of the fish (after observing the data). The interval that is constructed is called a credibility interval. There is a 95% chance that the fish is in the credibility interval (at least if the model, both the prior assumption and the normal distribution of the data is correct).\n\n\nWhich approach is better\nActually, most people want both interpretations to hold. They want a proceedure that catches the fish 95% of the time and they want the fish to be in the net 95% of the time.\nFortunately, when we use the approximation that all positions of the fish are equally likely, the two intervals are the same. The abbreviation c.i. could stand for either confidence or credibility interval.\nThe Bayesian interpretation relies on an additional assumption, but both intervals rely on assumptions about the distribution of the observed data. In particular, in this example, we are using the normal distribution to calculate the intervals. That means that the distribution must be close enough to normal that by the central limit theorem, it is reasonable to think that the mean is approximately normally distributed.\nBoth c.i.s break down if the there is a problem with the sample. If this was a convenience sample and not a random one, the normal distribution around the population mean might not be at all right. The c.i. only talks about random error and not bias."
  },
  {
    "objectID": "IntroStats/ConfidenceInterval.html#footnotes",
    "href": "IntroStats/ConfidenceInterval.html#footnotes",
    "title": "Confidence Intervals",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe equally likely assumption is actually a bit nonsensical as we probably expect the fish to in the middle of the pond and not out past the orbit of Pluto. However, if we have enough data, the assumption will not play a big role in our estimate.↩︎"
  },
  {
    "objectID": "IntroStats/index.html",
    "href": "IntroStats/index.html",
    "title": "Index of R Demonstrations for Intro Stat",
    "section": "",
    "text": "Florida State University\nThese are demonstrations which were written for EDF 5400, which is an introductory statistics class taught at the graduate level.",
    "crumbs": [
      "Home",
      "Intro Stats"
    ]
  },
  {
    "objectID": "IntroStats/index.html#index-of-r-demonstrations",
    "href": "IntroStats/index.html#index-of-r-demonstrations",
    "title": "Index of R Demonstrations for Intro Stat",
    "section": "Index of R Demonstrations",
    "text": "Index of R Demonstrations\n\n\n\n\n\n\n\n\nCC-BY\n\n\nThese are licensed under the creative commons CC BY license. You many distribute, remix, adapt, and build upon the material in any medium or format, so long as attribution is given to the creator.\nFor more information contact Russell Almond.\nThe Source files for these demonstrations can be found at https://pluto.coe.fsu.edu/svn/common/rgroup-shiny/IntroStats",
    "crumbs": [
      "Home",
      "Intro Stats"
    ]
  },
  {
    "objectID": "IntroStats/PoissonParms.html",
    "href": "IntroStats/PoissonParms.html",
    "title": "Poisson Params",
    "section": "",
    "text": "The Poisson distribution is a distribution for counts of events.\nAssume the following things:\n\nEvents happen at a rate \\(\\lambda\\) per unit interval on average.\nCount the number of events in a time interval of \\(T\\) units.\nAssume that the events happen at a uniform rate throughout the interval (e.g., we don’t get more customers in the morning than the afternoon).\n\nThe the number of events, \\(X\\), follows a Poisson distribution.\n\\[P(X=x) = \\frac{(\\lambda T)^x}{x!}e^{-\\lambda T}\\] The distribution looks like:\n\n\n\n\n\n\nExpected number of events per unit time\n\n\n\n\n\nTime Interval:\n\n\n\n\n\n\n\n\n\n\nThe mean and variance of the Poisson distribution are \\(\\lambda T\\) and \\(\\lambda T\\).\nAs the variance grows pretty quickly, statisticians will often take the square root of count data (especially if there is heteroscedasticity) to stabilize the variance."
  },
  {
    "objectID": "IntroStats/SkewnessQQ.html",
    "href": "IntroStats/SkewnessQQ.html",
    "title": "Skewness Practice with Quantile-Quantile Plots",
    "section": "",
    "text": "In this exercise, the computer will generate 3 datasets: A, B and C. These will be randomly assigned to a positively skewed, negatively skewed, and symmetric distribution type. The data (sorted in order) are plotted on the Y axis and the quantiles of a standard normal (qnorm) distribution are plotted on the X axis. A normal distribution should appear as a straight line; positively skewed distributions will yield a ‘U’ shaped curve, and negatively skewed distributions a ‘C’ shaped curve. (An ‘S’ or ‘Z’ shaped curve indicates kurtosis, not skewness). Note: SPSS plots the normal quantiles on the Y axis and the data on the X: Which means that leptokurtic and platykurtic distributions will curve in the opposite direction from these Q-Q plots.\nYou can redraw from the same distributions by changing the sample size. (Bigger sample sizes are easier.)\n\n\n\n\n\n\nSample Size:\n\n50\n100\n500\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentify the skewness of each distribution.\n\n\n\n\n\n\nA\n\nUnknown\nNegatively Skewed\nPositively Skewed\nSymmetric\n\n\n\n\n\n\nB\n\nUnknown\nNegatively Skewed\nPositively Skewed\nSymmetric\n\n\n\n\n\n\nC\n\nUnknown\nNegatively Skewed\nPositively Skewed\nSymmetric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo try again with different distributions, reload the page. If you are having trouble, try increasing the sample size: sometimes a small sample won’t display the characteristics of the distribution strongly.\nHere are the other exercises in this series:\n\nSkewness Practice:\n\nHistograms\nBoxplots\nQ-Q Plots\n\nKurtosis Practice:\n\nHistograms\nBoxplots\nQ-Q Plots"
  },
  {
    "objectID": "IntroStats/SkewnessQQ.html#skewness-determination-exercise.",
    "href": "IntroStats/SkewnessQQ.html#skewness-determination-exercise.",
    "title": "Skewness Practice with Quantile-Quantile Plots",
    "section": "",
    "text": "In this exercise, the computer will generate 3 datasets: A, B and C. These will be randomly assigned to a positively skewed, negatively skewed, and symmetric distribution type. The data (sorted in order) are plotted on the Y axis and the quantiles of a standard normal (qnorm) distribution are plotted on the X axis. A normal distribution should appear as a straight line; positively skewed distributions will yield a ‘U’ shaped curve, and negatively skewed distributions a ‘C’ shaped curve. (An ‘S’ or ‘Z’ shaped curve indicates kurtosis, not skewness). Note: SPSS plots the normal quantiles on the Y axis and the data on the X: Which means that leptokurtic and platykurtic distributions will curve in the opposite direction from these Q-Q plots.\nYou can redraw from the same distributions by changing the sample size. (Bigger sample sizes are easier.)\n\n\n\n\n\n\nSample Size:\n\n50\n100\n500\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentify the skewness of each distribution.\n\n\n\n\n\n\nA\n\nUnknown\nNegatively Skewed\nPositively Skewed\nSymmetric\n\n\n\n\n\n\nB\n\nUnknown\nNegatively Skewed\nPositively Skewed\nSymmetric\n\n\n\n\n\n\nC\n\nUnknown\nNegatively Skewed\nPositively Skewed\nSymmetric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo try again with different distributions, reload the page. If you are having trouble, try increasing the sample size: sometimes a small sample won’t display the characteristics of the distribution strongly.\nHere are the other exercises in this series:\n\nSkewness Practice:\n\nHistograms\nBoxplots\nQ-Q Plots\n\nKurtosis Practice:\n\nHistograms\nBoxplots\nQ-Q Plots"
  },
  {
    "objectID": "IntroStats/Geyser.html",
    "href": "IntroStats/Geyser.html",
    "title": "Histogram Bins",
    "section": "",
    "text": "The data show the duration (in minutes) of eruptions of the Geyser ‘Old Faithful’ in Yellowstone National Park, Wyoming. Härdle (1991)."
  },
  {
    "objectID": "IntroStats/Geyser.html#bin-size-and-bandwidth.",
    "href": "IntroStats/Geyser.html#bin-size-and-bandwidth.",
    "title": "Histogram Bins",
    "section": "Bin size and bandwidth.",
    "text": "Bin size and bandwidth.\nThe number of histograms in a describe how much detail you get. Try adjusting thing number to see the effect. Too few bins might cause the viewer to miss out on important details. Too many bins might cause the viewer to see details that are just an artifact of the sample (which might be different if the data were taken from a different window of time).\nThe smooth line is a kernel smoother (a technique which available in R, but not SPSS). The bandwidth of the smoother is like the bin size of the histogram. Large bandwidths provide less detail and smaller provide more.\nWith both bin sizes and bandwidths: * The analyst sometimes need to try a few values to find the best one for your purposes. * The defaults in the stat packages are usually a good starting point. Try more bins (or smaller bandwidth) and fewer bins (less bandwidth) than the automatically chosen starting point.\n\n\n\n\n\n\nNumber of bins:\n\n10\n20\n35\n50\n\n\n\n\n\n\nBandwidth adjustment:\n\n\n\n\n\n\n\n\n\n\nHärdle, W. (1991). Smoothing Techniques with Implementation in S. New York: Springer."
  },
  {
    "objectID": "IntroStats/Geyser.html#credits",
    "href": "IntroStats/Geyser.html#credits",
    "title": "Histogram Bins",
    "section": "Credits",
    "text": "Credits\nI borrowed this document from one of the Shiny sample documents. Original instructions below. (Russell Almond).\nThis R Markdown document is made interactive using Shiny. Unlike the more traditional workflow of creating static reports, you can now create documents that allow your readers to change the assumptions underlying your analysis and see the results immediately.\nTo learn more, see Interactive Documents."
  },
  {
    "objectID": "IntroStats/CorrelationExamples.html",
    "href": "IntroStats/CorrelationExamples.html",
    "title": "Scatterplot examples",
    "section": "",
    "text": "This demonstration will use some random data. Lets start by generating the random data. So give a [random seed][seed] and pick a sample size for your sample.\nSample Size:\n\n25\n50\n100\n250\n500\n1000\n\n\n\n\n\n\nRandom number Seed (integer)"
  },
  {
    "objectID": "IntroStats/CorrelationExamples.html#mostly-linear",
    "href": "IntroStats/CorrelationExamples.html#mostly-linear",
    "title": "Scatterplot examples",
    "section": "Mostly linear",
    "text": "Mostly linear\nThis happens when we have a moderately high to strong correlation.\n\n\n\n\n\n\nCorrelation Coefficient:\n\n\n\n\n\n\n\n\nNegative Correlation"
  },
  {
    "objectID": "IntroStats/CorrelationExamples.html#blobby-elipse",
    "href": "IntroStats/CorrelationExamples.html#blobby-elipse",
    "title": "Scatterplot examples",
    "section": "Blobby Elipse",
    "text": "Blobby Elipse\nAs the correlation coefficient gets lower, the scatterplot looks more blobby, but you can still tell that there is a slope. This is a weak to moderate correlation.\n\n\n\n\n\n\nCorrelation Coefficient:\n\n\n\n\n\n\n\n\nNegative Correlation"
  },
  {
    "objectID": "IntroStats/CorrelationExamples.html#no-relationship",
    "href": "IntroStats/CorrelationExamples.html#no-relationship",
    "title": "Scatterplot examples",
    "section": "No Relationship",
    "text": "No Relationship\nNot much is going on here. One thing that confuses people is the idea that linear regression doesn’t work here. Actually, it gives a quite accurate picture: it tells you that not much is going on, which is what is actually happening. The prediction from the regression will be that \\(\\bar Y\\) is the best predicted value for \\(Y\\).\n\n\n\n\n\n\nCorrelation Coefficient:\n\n\n\n\n\n\n\n\nNegative Correlation"
  },
  {
    "objectID": "IntroStats/CorrelationExamples.html#curve",
    "href": "IntroStats/CorrelationExamples.html#curve",
    "title": "Scatterplot examples",
    "section": "Curve",
    "text": "Curve\nA curved relationship doesn’t look like a line.\nConsider a quadradic relationship: \\[ Y = b_2 X^2 + b_1 X + b_0 + \\epsilon\\] This is a multiple (or quadradic) regression. You can adjust the coefficients in the plot below.\n\n\n\n\n\n\nQuadradic Term Slope:\n\n\n\n\n\nLinear Term Slope:\n\n\n\n\n\nIntercept:\n\n\n\n\n\nError Standard Deviation:\n\n\n\n\n\n\n\n\n\n\nIf we try to run a linear regression when the relationship is curved, it will only tell us part of the story. The story it will tell is the red line, and not the blue curve."
  },
  {
    "objectID": "IntroStats/CorrelationExamples.html#broken-lines",
    "href": "IntroStats/CorrelationExamples.html#broken-lines",
    "title": "Scatterplot examples",
    "section": "Broken Lines",
    "text": "Broken Lines\nSometimes the reltionship changes somewhere through the range of the data. Often this is a ceiling effect: the effect of \\(X\\) on \\(Y\\) hits a ceiling. For example, in the first couple of years of teaching, the ability of new teachers rises very rapidly as they gain experience. But after 3–5 years, the effect levels out and the teachers grow much more slowly.\nIdeally we would fit two linear regression to these data splitting at a certain value of \\(X\\), \\(x_0\\). So,\n\\[ Y = \\begin{cases}\nb_{11} X + b_{01} + \\epsilon & \\text {when} X \\leq x_0 \\\\\nb_{12} X + b_{02} + \\epsilon & \\text {when} X \\ge x_0\n\\end{cases}\n\\]\n\n\n\n\n\n\nFirst Slope:\n\n\n\n\n\nSecond Slope:\n\n\n\n\n\nCrossover Point (x[0])\n\n\n\n\n\nError Standard Deviation:\n\n\n\n\n\n\n\n\n\n\nCheck out this page to practice identifying these."
  },
  {
    "objectID": "IntroStats/RegressionPrediction.html",
    "href": "IntroStats/RegressionPrediction.html",
    "title": "Regression Prediction Error",
    "section": "",
    "text": "We will work with an example data set from Ezekiel (1930) which provides information about the speed of a number of cars and the stopping distance in feet.\n\nhelp(cars)\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00  \n\n\nLets take a quick look at these data.\n\nplot(dist~speed,data=cars,xlab=\"Speed (mph)\", ylab=\"Stopping Distance (ft)\")\nabline(lm(dist~speed,data=cars))\nlines(lowess(cars),col=2,lty=2)\n\n\n\n\n\n\n\n\nThe solid black line is the least squares regression line, or basic model.\nThe dashed red line is a lowess curve fit to the same date.\n\nThere may be a little bit of a curve here, but it is hard to see.\n\nWe will go ahead and fit a regression using least squares. (This the the lm or linear model function in R.)\n\ncars.fit &lt;- lm (dist~speed,data=cars)\nprint(cars.fit)\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nCoefficients:\n(Intercept)        speed  \n    -17.579        3.932  \n\n\nThe method of least squares or maximum likelihood (which in the case of simple regression are the same) finds the single best fitting line.\n\nLeast Squares means the line has the smallest some of squared residuals.\nMaximum Likelihood means that these are the parameters (slope and intercept) that have the highest probability of generating the target data."
  },
  {
    "objectID": "IntroStats/RegressionPrediction.html#the-cars-data-set",
    "href": "IntroStats/RegressionPrediction.html#the-cars-data-set",
    "title": "Regression Prediction Error",
    "section": "",
    "text": "We will work with an example data set from Ezekiel (1930) which provides information about the speed of a number of cars and the stopping distance in feet.\n\nhelp(cars)\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00  \n\n\nLets take a quick look at these data.\n\nplot(dist~speed,data=cars,xlab=\"Speed (mph)\", ylab=\"Stopping Distance (ft)\")\nabline(lm(dist~speed,data=cars))\nlines(lowess(cars),col=2,lty=2)\n\n\n\n\n\n\n\n\nThe solid black line is the least squares regression line, or basic model.\nThe dashed red line is a lowess curve fit to the same date.\n\nThere may be a little bit of a curve here, but it is hard to see.\n\nWe will go ahead and fit a regression using least squares. (This the the lm or linear model function in R.)\n\ncars.fit &lt;- lm (dist~speed,data=cars)\nprint(cars.fit)\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nCoefficients:\n(Intercept)        speed  \n    -17.579        3.932  \n\n\nThe method of least squares or maximum likelihood (which in the case of simple regression are the same) finds the single best fitting line.\n\nLeast Squares means the line has the smallest some of squared residuals.\nMaximum Likelihood means that these are the parameters (slope and intercept) that have the highest probability of generating the target data."
  },
  {
    "objectID": "IntroStats/RegressionPrediction.html#lots-of-different-regression-lines",
    "href": "IntroStats/RegressionPrediction.html#lots-of-different-regression-lines",
    "title": "Regression Prediction Error",
    "section": "Lots of different regression lines",
    "text": "Lots of different regression lines\nI’ll try the regression using a different method (Markov Chain Monte Carlo, or MCMC). In this method we sample 4000 different plausible sets of parameters that could have given rise to the data. (These are sampled with a probability proportional to how likely they are to have generated the observed data).\nThe printed summary shows the median of the 4000 samples. It should be close to, but not exactly the same as, the least squares/maximum likelihood estimate.\n\n#library(rstanarm)  ## Called above\ncars.mcmc &lt;- stan_glm(dist~speed,data=cars,refresh=0)\ncars.coef &lt;- as.matrix(cars.mcmc$stanfit)\nprint(cars.mcmc)\n\nstan_glm\n family:       gaussian [identity]\n formula:      dist ~ speed\n observations: 50\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) -17.4    6.6 \nspeed         3.9    0.4 \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 15.6    1.6  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg"
  },
  {
    "objectID": "IntroStats/RegressionPrediction.html#mean-confidence-interval",
    "href": "IntroStats/RegressionPrediction.html#mean-confidence-interval",
    "title": "Regression Prediction Error",
    "section": "Mean Confidence Interval",
    "text": "Mean Confidence Interval\nThe MCMC approach is useful because it helps us remember that the estimates that are produced by the [least squares] regression are not the truth, but rather just the most likely set of parameters. There are other possibilities that are nearly as likely.\nThe next graph is designed to show this.\nThe first N (you can adjust using the slider) samples from the MCMC are plotted as gray lines.\nThe least squares line is plotted in black.\n\n\n\n\n\n\nConfidence Level:\n\n50\n68\n90\n95\n99\n\n\n\n\n\n\nNumber of plausible values to plot\n\n\n\n\n\n\n\n\n\n\nNote the dashed curves surrounding the regression line.\nThese are the confidence interval for the regression line.\nThe level of the confidence interval is how many of these plausible regression lines should fit between the dashed curves (expressed as a percentage).\nSPSS calls this the “mean” prediction interval. R calls it the “confidence” interval.\nYou can use the graph (or the R predict function, or the prediction option in SPSS) to get a prediction for the average (over a number of trials) stopping time at a given speed."
  },
  {
    "objectID": "IntroStats/RegressionPrediction.html#individual-prediction-interval",
    "href": "IntroStats/RegressionPrediction.html#individual-prediction-interval",
    "title": "Regression Prediction Error",
    "section": "Individual Prediction Interval",
    "text": "Individual Prediction Interval\nThe mean confidence interval above is for the average over many attempts at stopping the car.\nWe don’t expect a single attempt to fall exactly on the line.\n\n68% of the time we expect to be one standard error above or below the line.\n95% of the time we expect to be two standard errors above or below the line.\nTo get the total error, we need to add\n\nThe error in the regression line (see above)\nThe error around the regression line.\n\n\n(Actually, we add these on the squared variance scale).\nThe picture below shows the individual prediction interval. Once again, you can pick your confidence level.\nSPSS calls this the individual prediction interval. R calls it the prediction interval.\n\n\n\n\n\n\nConfidence Level:\n\n50\n68\n90\n95\n99\n\n\n\n\n\n\n\n\n\n\n\nLook at the area in the graph which is colored cyan.\nThese are predictions that the car will stop in negative distance. Impossible!\nThe model is wrong.\nThat shouldn’t worry us, models are always wrong. The just might be close enough to be right to be useful.\nWe might say that the linear model is useful, but only if the car is going 5 mph or more."
  },
  {
    "objectID": "IntroStats/RegressionPrediction.html#model-checking",
    "href": "IntroStats/RegressionPrediction.html#model-checking",
    "title": "Regression Prediction Error",
    "section": "Model Checking",
    "text": "Model Checking\nNote that there was a slight curve in the lowess line in the scatterplot at the top of this analysis.\nSometimes the curve is easier to see if we take the linear trend out.\nWe can do this by plotting the residuals versus the fitted values.\n In a simple regression, this is the same as plotting against \\(X\\), as the fitted values are just a linear transformation of \\(X\\) (and the graph will just be rescaled to fit). For multiple regression, the fitted values are a mix of all the \\(X\\) values, so this plot is a useful summary.\n\n\n\n\n\n\n\n\n\nLooking a little more closely, we can see the curve.\nIt would be easy to miss without the lowess line, but the lowess line points it out to us.\nThere is a little bit of curvature, curving up at the lower distances, keeping the stopping distances in positive territory.\nSo what to conclude?\n\nIn the range of 5 mph – 25 mph the linear model looks pretty good.\nFor low speeds, we need a better model.\nMaybe we need a better model for higher speeds as well."
  },
  {
    "objectID": "IntroStats/KurtosisBoxplots.html",
    "href": "IntroStats/KurtosisBoxplots.html",
    "title": "Kurtosis Boxplot Practice",
    "section": "",
    "text": "In this exercise, the computer will generate 3 datasets: X, Y and Z. These will be randomly assigned to high (leptokurtic), medium (mesokurtic) and low (platykurtic) distributions.\nYou can redraw from the same distributions by changing the sample size.\n\n\n\n\n\n\nSample Size:\n\n50\n100\n500\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentify the kurtosis of each distribution.\n\n\n\n\n\n\nX\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\nY\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\nZ\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo try again with different distributions, reload the page. If you are having trouble, try increasing the sample size: sometimes a small sample won’t display the characteristics of the distribution strongly."
  },
  {
    "objectID": "IntroStats/KurtosisBoxplots.html#kurtosis-determination-exercise.",
    "href": "IntroStats/KurtosisBoxplots.html#kurtosis-determination-exercise.",
    "title": "Kurtosis Boxplot Practice",
    "section": "",
    "text": "In this exercise, the computer will generate 3 datasets: X, Y and Z. These will be randomly assigned to high (leptokurtic), medium (mesokurtic) and low (platykurtic) distributions.\nYou can redraw from the same distributions by changing the sample size.\n\n\n\n\n\n\nSample Size:\n\n50\n100\n500\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentify the kurtosis of each distribution.\n\n\n\n\n\n\nX\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\nY\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\nZ\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo try again with different distributions, reload the page. If you are having trouble, try increasing the sample size: sometimes a small sample won’t display the characteristics of the distribution strongly."
  },
  {
    "objectID": "IntroStats/KurtosisBoxplots.html#what-to-look-for",
    "href": "IntroStats/KurtosisBoxplots.html#what-to-look-for",
    "title": "Kurtosis Boxplot Practice",
    "section": "What to look for",
    "text": "What to look for\n\nIn a normal distribution the whiskers extend about 1.5 box-lengths (IQR)s from the hinges (sides of the box).\n\nIf the box is long compared to the whiskers, this is a sign the distribuiton is platykurtic.\nIf the box is short compared to the whiskers, this is a sign the distribution is leptokurtic.\n\nThe whiskers only extend to the farthest data point within 1.5 IQRs from the box. So if there is high kurtosis, this will show up as lots of outliers.\n\nWith a normal distribution, there is often 1–2 outliers per 100 data points. Much more than that is a sign of high kurtosis.\n\nIs the length of the box (IQR) long compared to the length of the whiskers?"
  },
  {
    "objectID": "IntroStats/KurtosisBoxplots.html#related-pages",
    "href": "IntroStats/KurtosisBoxplots.html#related-pages",
    "title": "Kurtosis Boxplot Practice",
    "section": "Related Pages:",
    "text": "Related Pages:\nHere are the other exercises in this series:\n\nSkewness Practice:\n\nHistograms\nBoxplots\nQ-Q Plots\n\nKurtosis Practice:\n\nHistograms\nBoxplots\nQ-Q Plots"
  },
  {
    "objectID": "IntroStats/NormalParams.html",
    "href": "IntroStats/NormalParams.html",
    "title": "Normal Parameters",
    "section": "",
    "text": "A parameter is a value that can be changed in a statistical model. For example, the mean and standard deviation are the parameters of the normal distribution, which is a model for a population. Changing the value of a parameter, changes the model. We can see that in the illustration below. Try changing the values of the mean and standard deviation and see what happens to the shape of the curve."
  },
  {
    "objectID": "IntroStats/NormalParams.html#inputs-and-outputs",
    "href": "IntroStats/NormalParams.html#inputs-and-outputs",
    "title": "Normal Parameters",
    "section": "Inputs and Outputs",
    "text": "Inputs and Outputs\n\n\n\n\n\n\nMean:\n\n\n\n\n\nStandard Deviation:"
  },
  {
    "objectID": "IntroStats/NormalParams.html#scale-and-location-parameters",
    "href": "IntroStats/NormalParams.html#scale-and-location-parameters",
    "title": "Normal Parameters",
    "section": "Scale and Location Parameters",
    "text": "Scale and Location Parameters\nThe mean has a special role in the normal distribution; it determines where the center of the curve is. This makes it a location parameter.\nThe standard deviation has a special role in the normal distribution; it stretches and shrinks the curve around the mean. This makes it a scale parameter.\nSometimes, the effects of scale and location parameters can be hard to see. This is because most statistical graphics packages adjust the axis of the graph, so that the curve will always appear centered in the plotting window. In the normal curve above, I fixed the plotting window so that you can see the curve move. In the example below, I let the plotting window adjust with the curve. Notice how the curve stays the same, but the labels on the axis change.\n\n\n\n\n\n\nMean:\n\n\n\n\n\nStandard Deviation:"
  },
  {
    "objectID": "IntroStats/LawOfLargeNumbers.html",
    "href": "IntroStats/LawOfLargeNumbers.html",
    "title": "Law of Large Numbers",
    "section": "",
    "text": "This is pretty close to the frequency definition of probability. Suppose the probability of some event is \\(p\\). Suppose further than we sample \\(N\\) times from the process that generates this event. Let \\(p_N\\) be the proportion of times the event occurs in \\(N\\) trials. As \\(N\\) gets bigger and bigger, \\(p_N\\) gets closer and closer to \\(p\\).\n(Skip this unless you are good with calculus.) This is one of those epsilon-delta theorems. So let \\(\\delta\\) be a difference from \\(p\\) and let \\(\\epsilon\\) be a small probability. For any \\(\\epsilon\\) and \\(\\delta\\), there exists an \\(N\\) such that \\(P(|p_N-p|&gt;\\delta) &lt; \\epsilon\\).\n\n\nIn the picture below, pick a probability \\(p\\) and a sample size \\(N\\). The computer will generate samples up to \\(N\\) and plot \\(p_N\\).\nThe \\(\\delta\\)-line is an error bound plus or minus \\(\\delta\\) units from the target \\(p\\). This is a target so you can judge how close you got.\n\n\n\n\n\n\nMaximum Sample Size:\n\n50\n100\n200\n500\n1000\n\n\n\n\n\n\nProbability of event (p)\n\n\n\n\n\nDistance of reference line from target (delta)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can use the Law of Large Numbers to prove an important theorem. As the sample size gets larger and larger, the sample looks more and more like the population it is drawn from.\n Technically, the Law of Large Numbers refers to the result above. But we can use it so show a very important basis of statistics. Suppose we have some kind of distribution, \\(F(x)\\), that generates numbers, \\(X\\). Recall that the definition of \\(F(x)=\\Pr(X \\leq x)\\).\n Draw a sample of size \\(N\\) from this distribution. Now consider the sampled data points \\(X_1,\\ldots,X_N\\), and consider sampling a new value \\(Y\\) from that distribution. Let \\(F_N(y) = \\Pr(Y \\leq y)\\). This is sometimes called the bootstrap distribution.\n By the law of large numbers, for every \\(y\\), as \\(N\\) gets large \\(F_N(y) \\rightarrow F(y)\\). So the sample distribution \\(F_N()\\) converges to the \\(F()\\).\n\n\n\nPick a distribution: * Normal – standard normal * Exponential – highly skewed * Gamma (shape = 3) – skewed * T (df =3) – high kurtosis\nSlide the sample size up and down, notice how the empirical distribution function and histogram coverge to the theoretical distribution function and density.\n\n\n\n\n\n\nDistribution Type\n\nNormal\nExponential\nGamma\nT\n\n\n\n\n\n\nMaximum Sample Size:\n\n\n\n\n\n\n\n\n\n\nSee also the animated version."
  },
  {
    "objectID": "IntroStats/LawOfLargeNumbers.html#a-demonstration.",
    "href": "IntroStats/LawOfLargeNumbers.html#a-demonstration.",
    "title": "Law of Large Numbers",
    "section": "",
    "text": "In the picture below, pick a probability \\(p\\) and a sample size \\(N\\). The computer will generate samples up to \\(N\\) and plot \\(p_N\\).\nThe \\(\\delta\\)-line is an error bound plus or minus \\(\\delta\\) units from the target \\(p\\). This is a target so you can judge how close you got.\n\n\n\n\n\n\nMaximum Sample Size:\n\n50\n100\n200\n500\n1000\n\n\n\n\n\n\nProbability of event (p)\n\n\n\n\n\nDistance of reference line from target (delta)"
  },
  {
    "objectID": "IntroStats/LawOfLargeNumbers.html#convergence-of-distributions-boot-strap-distribution",
    "href": "IntroStats/LawOfLargeNumbers.html#convergence-of-distributions-boot-strap-distribution",
    "title": "Law of Large Numbers",
    "section": "",
    "text": "We can use the Law of Large Numbers to prove an important theorem. As the sample size gets larger and larger, the sample looks more and more like the population it is drawn from.\n Technically, the Law of Large Numbers refers to the result above. But we can use it so show a very important basis of statistics. Suppose we have some kind of distribution, \\(F(x)\\), that generates numbers, \\(X\\). Recall that the definition of \\(F(x)=\\Pr(X \\leq x)\\).\n Draw a sample of size \\(N\\) from this distribution. Now consider the sampled data points \\(X_1,\\ldots,X_N\\), and consider sampling a new value \\(Y\\) from that distribution. Let \\(F_N(y) = \\Pr(Y \\leq y)\\). This is sometimes called the bootstrap distribution.\n By the law of large numbers, for every \\(y\\), as \\(N\\) gets large \\(F_N(y) \\rightarrow F(y)\\). So the sample distribution \\(F_N()\\) converges to the \\(F()\\)."
  },
  {
    "objectID": "IntroStats/LawOfLargeNumbers.html#demonstration-of-convergence-of-distributions.",
    "href": "IntroStats/LawOfLargeNumbers.html#demonstration-of-convergence-of-distributions.",
    "title": "Law of Large Numbers",
    "section": "",
    "text": "Pick a distribution: * Normal – standard normal * Exponential – highly skewed * Gamma (shape = 3) – skewed * T (df =3) – high kurtosis\nSlide the sample size up and down, notice how the empirical distribution function and histogram coverge to the theoretical distribution function and density.\n\n\n\n\n\n\nDistribution Type\n\nNormal\nExponential\nGamma\nT\n\n\n\n\n\n\nMaximum Sample Size:\n\n\n\n\n\n\n\n\n\n\nSee also the animated version."
  },
  {
    "objectID": "IntroStats/ConditionalProbability.html",
    "href": "IntroStats/ConditionalProbability.html",
    "title": "ConditionalProbability",
    "section": "",
    "text": "On Oct 1, Merck announced exciting results for a new drug, Molnupiravir a pill for treating patients with COVID-19. The results were good enough by 29 days into the study, that they submitted the results to the FDA for emergency approval. (Note that the link above is to a press release and not to a scientific paper that has been peer or FDA review; but if the results hold up to scrutiny, this could be exciting.)\nHere is the relevant bit of the press release: &gt; At the interim analysis, molnupiravir reduced the risk of hospitalization or death by approximately 50%; 7.3% of patients who received molnupiravir were either hospitalized or died through Day 29 following randomization (28/385), compared with 14.1% of placebo-treated patients (53/377); p=0.0012.\nFrom this we can construct the following data table.\n\nn &lt;- c(drug=385,placebo=377)\nhd &lt;- c(drug=28,placebo=53)\nnhd &lt;- n-hd\ntab &lt;- data.frame(hd,nhd,n)\ntab1 &lt;- rbind(tab,Total=colSums(tab))\ntab1\n\n        hd nhd   n\ndrug    28 357 385\nplacebo 53 324 377\nTotal   81 681 762\n\n\nA mosaic plot allows us to look at this table graphically.\n\n## Subset to just the inner part of the table.\nppddat &lt;- as.matrix(tab[,1:2])\nnames(dimnames(ppddat)) &lt;- c(\"Treatment\",\"Outcome\")\nstrucplot(ppddat,labeling=labeling_values(\"observed\"))"
  },
  {
    "objectID": "IntroStats/ConditionalProbability.html#some-example-data.",
    "href": "IntroStats/ConditionalProbability.html#some-example-data.",
    "title": "ConditionalProbability",
    "section": "",
    "text": "On Oct 1, Merck announced exciting results for a new drug, Molnupiravir a pill for treating patients with COVID-19. The results were good enough by 29 days into the study, that they submitted the results to the FDA for emergency approval. (Note that the link above is to a press release and not to a scientific paper that has been peer or FDA review; but if the results hold up to scrutiny, this could be exciting.)\nHere is the relevant bit of the press release: &gt; At the interim analysis, molnupiravir reduced the risk of hospitalization or death by approximately 50%; 7.3% of patients who received molnupiravir were either hospitalized or died through Day 29 following randomization (28/385), compared with 14.1% of placebo-treated patients (53/377); p=0.0012.\nFrom this we can construct the following data table.\n\nn &lt;- c(drug=385,placebo=377)\nhd &lt;- c(drug=28,placebo=53)\nnhd &lt;- n-hd\ntab &lt;- data.frame(hd,nhd,n)\ntab1 &lt;- rbind(tab,Total=colSums(tab))\ntab1\n\n        hd nhd   n\ndrug    28 357 385\nplacebo 53 324 377\nTotal   81 681 762\n\n\nA mosaic plot allows us to look at this table graphically.\n\n## Subset to just the inner part of the table.\nppddat &lt;- as.matrix(tab[,1:2])\nnames(dimnames(ppddat)) &lt;- c(\"Treatment\",\"Outcome\")\nstrucplot(ppddat,labeling=labeling_values(\"observed\"))"
  },
  {
    "objectID": "IntroStats/ConditionalProbability.html#pr-ohd",
    "href": "IntroStats/ConditionalProbability.html#pr-ohd",
    "title": "ConditionalProbability",
    "section": "Pr (O=HD)",
    "text": "Pr (O=HD)\nWith no bar, indicating no conditioning, we are looking at the probability among all people in the sample. This is called the marginal probability because it comes from the margins (sums) of the table.\n\n\n\n\n\n\n\n\n\nThe marginal (no condition) probability of a negative (hospitalization or death) outcome is 81 (red areas)/762 (blue areas) = 0.106."
  },
  {
    "objectID": "IntroStats/ConditionalProbability.html#pr-ohd-t-drug",
    "href": "IntroStats/ConditionalProbability.html#pr-ohd-t-drug",
    "title": "ConditionalProbability",
    "section": "Pr (O=HD | T = Drug)",
    "text": "Pr (O=HD | T = Drug)\nThe bar conditions or restricts the sample to just the people who meet the condition, in this case, those that have taken the active treatment.\n\n\n\n\n\n\n\n\n\nThe condtional probability of a negative (hospitalization or death) outcome given the drug is 28 (red areas)/385 (purple areas) = 0.073."
  },
  {
    "objectID": "IntroStats/ConditionalProbability.html#pr-ohd-t-placebo",
    "href": "IntroStats/ConditionalProbability.html#pr-ohd-t-placebo",
    "title": "ConditionalProbability",
    "section": "Pr (O=HD | T = Placebo)",
    "text": "Pr (O=HD | T = Placebo)\nThe bar conditions or restricts the sample to just the people who meet the condition, in this case, those that have taken the placebo treatment.\n\n\n\n\n\n\n\n\n\nTThe condtional probability of a negative (hospitalization or death) outcome given the placebo is 53 (red areas)/377 (purple areas) = 0.141."
  },
  {
    "objectID": "IntroStats/ConditionalProbability.html#independence",
    "href": "IntroStats/ConditionalProbability.html#independence",
    "title": "ConditionalProbability",
    "section": "Independence",
    "text": "Independence\nIf the drug was independent of hospitalization, then the two probabilities we calculated above should be the same. In fact, when we calculate the ratio, we get 0.517. A huge improvement. No wonder Merck was so excited.\nWe should also be able to look at the column probabilities in the same way. The original sample is close to a 50-50 split between drug and placebo. (It actually 0.505, probably because they were not finished their recruting. This little bit of unbalance doesn’t matter). However, when we look at the fraction of the people who were hospitalized who got the drug \\(\\Pr(Treatment=drug | Outcome=HD)\\) we see it is 0.346. Once again this is very different.\nSuppose we have two variables \\(A\\) with possible outcomes \\(\\{a_1,\\ldots,a_J\\}\\) and \\(B\\) with possible outcomes \\(\\{b_1,\\ldots,b_K\\}\\). If \\(A\\) and \\(B\\) are independent, three different relationships will hold:\n\n\\(\\Pr(A=a_j | B=b_k) = \\Pr(A=a_j | B=b_k') = \\Pr(A=a_j) \\qquad \\forall j,k,k'\\)\n\\(\\Pr(B=b_k | A=a_j) = \\Pr(B=b_k | A=a_j') = \\Pr(B=b_j) \\qquad \\forall j,j',k\\)\n\\(\\Pr(A=a_j \\wedge B=b_k) = \\Pr(A=a_j) \\Pr(B=b_k) \\qquad \\forall j,k\\) (where \\(\\wedge\\) means and).\n\nRecall that the conditional probability is formally defined as \\[ \\Pr(A=a_j|B=b_k) = \\frac{\\Pr(A=a_j \\wedge B=b_k)}{\\Pr(B=b_k)} \\ .\\] So if \\(\\Pr(B=b_k)=0\\) for any \\(k\\), then the first expression doesn’t quite work right because we need to divide by zero. Similarly, the second has problems if \\(\\Pr(A=a_j)=0\\) for any \\(j\\). Therfore, the thrid expression is used as the definition, because it avoids the technical problems (divide by zero in certain cases) of the first two. But I think the first two give better intuition for what independence means."
  },
  {
    "objectID": "IntroStats/ConditionalProbability.html#the-chi-square-test",
    "href": "IntroStats/ConditionalProbability.html#the-chi-square-test",
    "title": "ConditionalProbability",
    "section": "The Chi-square test",
    "text": "The Chi-square test\nNote that even though the experimental design called for equal numbers in the drug and placebo arms of the studies, random events in how the participants were recruited made them slightly unequal. This is likely just luck, and not a serious problem.\nNow look at the different probabilities of a negative outcome for the drug and the placebo. How do we know if that is real, or just luck? One way we could answer that question is to build a statistical model for “just luck” and calculate how like the the observed data are if that model is true.\nIn this case, the model for “just luck” is the independence value up above. Using that we can calculate the expected values."
  },
  {
    "objectID": "IntroStats/KurtosisQQ.html",
    "href": "IntroStats/KurtosisQQ.html",
    "title": "Kurtosis Practice using Quantile-Quantile Plots",
    "section": "",
    "text": "In this exercise, the computer will generate 3 datasets: X, Y and Z. These will be randomly assigned to high (leptokurtic), medium (mesokurtic) and low (platykurtic) distributions. The data (sorted in order) are plotted on the Y axis and the quantiles of a standard normal (qnorm) distribution are plotted on the X axis. A normal distribution should appear as a straight line; leptokurtic and platykurtic distributions as ‘S’ or ‘Z’ curves. (A ‘U’ or ‘C’ shaped curve indicates skewness, not kurtosis.) Note: SPSS plots the normal quantiles on the Y axis and the data on the X: Which means that leptokurtic and platykurtic distributions will curve in the opposite direction from these Q-Q plots.\nYou can redraw from the same distributions by changing the sample size (higher sample sizes are easier to see).\n\n\n\n\n\n\nSample Size:\n\n50\n100\n500\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentify the kurtosis of each distribution.\n\n\n\n\n\n\nX\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\nY\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\nZ\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo try again with different distributions, reload the page. If you are having trouble, try increasing the sample size: sometimes a small sample won’t display the characteristics of the distribution strongly.\nHere are the other exercises in this series:\n\nSkewness Practice:\n\nHistograms\nBoxplots\nQ-Q Plots\n\nKurtosis Practice:\n\nHistograms\nBoxplots\nQ-Q Plots"
  },
  {
    "objectID": "IntroStats/KurtosisQQ.html#kurtosis-determination-exercise.",
    "href": "IntroStats/KurtosisQQ.html#kurtosis-determination-exercise.",
    "title": "Kurtosis Practice using Quantile-Quantile Plots",
    "section": "",
    "text": "In this exercise, the computer will generate 3 datasets: X, Y and Z. These will be randomly assigned to high (leptokurtic), medium (mesokurtic) and low (platykurtic) distributions. The data (sorted in order) are plotted on the Y axis and the quantiles of a standard normal (qnorm) distribution are plotted on the X axis. A normal distribution should appear as a straight line; leptokurtic and platykurtic distributions as ‘S’ or ‘Z’ curves. (A ‘U’ or ‘C’ shaped curve indicates skewness, not kurtosis.) Note: SPSS plots the normal quantiles on the Y axis and the data on the X: Which means that leptokurtic and platykurtic distributions will curve in the opposite direction from these Q-Q plots.\nYou can redraw from the same distributions by changing the sample size (higher sample sizes are easier to see).\n\n\n\n\n\n\nSample Size:\n\n50\n100\n500\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentify the kurtosis of each distribution.\n\n\n\n\n\n\nX\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\nY\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\nZ\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo try again with different distributions, reload the page. If you are having trouble, try increasing the sample size: sometimes a small sample won’t display the characteristics of the distribution strongly.\nHere are the other exercises in this series:\n\nSkewness Practice:\n\nHistograms\nBoxplots\nQ-Q Plots\n\nKurtosis Practice:\n\nHistograms\nBoxplots\nQ-Q Plots"
  },
  {
    "objectID": "IntroStats/EffectSize.html",
    "href": "IntroStats/EffectSize.html",
    "title": "Effect Size Calculator",
    "section": "",
    "text": "If the units of a test are well known, the size of an effect is pretty easy to understand. For example, if a study found that on average, people on this diet lost about 5 lbs (2.5 kg) in a week, most people would know what that means.\nIn many other cases, the size of the statistic depends on the measure used to determine it. For example, if a researcher finds that students on average gain 5 points on a math test after playing a mathematical game, is that a big gain or a small gain? Unlike the weight loss experiment, we don’t have the experience with that test to judge.\nThere are a number of measures that can be used to put the size of the effect into perspective. Jacob Cohen proposed dividing the difference by the population standard deviation: \\[ d = \\frac{\\mu_1 - \\mu_0}{\\sigma} .\\] This has the advantage of putting things on a readily apparent scale. So, suppose in the example of the math game above, the effect size was \\(d=.1\\). If the game was just a short thing that took an hour, that would be a pretty big deal. On the other hand, if the students needed to play all year to get that effect, it might not be so good."
  },
  {
    "objectID": "IntroStats/EffectSize.html#simple-case-one-group",
    "href": "IntroStats/EffectSize.html#simple-case-one-group",
    "title": "Effect Size Calculator",
    "section": "Simple Case – One Group",
    "text": "Simple Case – One Group\nWhen we are looking at a single group, the definition of the effect size is fairly simple. The single variable usually is a difference score; e.g., posttest - pretest. The standard deviation of interest is the standard deviation of the population of difference scores (estimated by the sample).\n\n\n\n\n\n\nMean Difference:\n\n\n\n\n\nStandard Deviation of Difference:"
  },
  {
    "objectID": "IntroStats/EffectSize.html#complex-case-two-groups",
    "href": "IntroStats/EffectSize.html#complex-case-two-groups",
    "title": "Effect Size Calculator",
    "section": "Complex Case – Two Groups",
    "text": "Complex Case – Two Groups\nConceptually, the two group case is just as simple. The numerator is the difference between the group means. The denominator is the standard deviation of the population. There is a problem: we don’t have one population, we have two: Group 1 and Group 2. What to do?\nThe answer is to take an average. Actually, we take the average of the variances, and then take the square root. There is one further complication, the two groups might have different sizes. In this case, we take a weighted average, weighting by the degrees of freedom (sample size -1). Here is the formula: \\[ \\sigma^2_{pooled} = \\frac{(N_1 -1)\\sigma^2_1 +(N_2 -1)\\sigma^2_2}{N_1+N_2-1} ,\\] take the square root of that to get the standard deviation.\nAlthough one could calculate that by hand, the calculator below will do the job for you, and then calculate the effect size at the same time. Note that the pooled SD should always be in between the SDs of group 1 and group 2. For a rough and ready estimate, you could just take the number halfway between the two.\n\n\n\n\n\n\n\nStatistics for Group 1 (experimental/focal)\n\nMean:\n\n\n\nSD:\n\n\n\nN:\n\n\n\n\nStatistics for Group 2 (control/reference)\n\nMean:\n\n\n\nSD:\n\n\n\nN:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you want to do these calculations away from the internet, you can download this Excel spreadsheet which has the formula baked in: https://pluto.coe.fsu.edu/effectSize_d.xls\n\nHow big is big? Cohen’s guide to effect sizes.\nReally, the answer is entirely discipline specific. In Physics, it is quite common to be able to exert large forces and to measure very accurately, and hence be able to get very large effect sizes. In the social sciences and education, it often hard to control all of the variables that might affect the outcome, so the typical effect sizes are quite small.\nJacob Cohen presented a guideline for use in power analyses. This was really just for when you had no idea of what the size of the effect would be. He suggested:\n\n\n\nEffect\nd\n\n\n\n\nSmall\n.2\n\n\nMedium\n.5\n\n\nLarge\n.8\n\n\n\nHowever, these are really not designed for interpreting effects. There instead, you should compare to what other similar interventions are achieving.\nDylan Wiliams suggests that for educational applications, you might try dividing by the effect size of a year’s growth for that grade level. This changes rapidly with 1st graders growing nearly 2 SDs while a years worth of growth for a high school student is closer to .5 SDs. In high school, an effect size of .25 would be half a years growth, which is considerable.\n(Of course to measure small effects, you also need a very sensitive instrument, which in education means a longer test; the cost of testing is often prohibitive.)"
  },
  {
    "objectID": "IntroStats/CovidVaccines.html",
    "href": "IntroStats/CovidVaccines.html",
    "title": "Covid-19 Vaccines",
    "section": "",
    "text": "Good News! In November, both Pfizer and Moderna announce Phase 3 Vaccine Trials with promising results.\nWith over 30,000 participants in each study they reported the following data.\ncovidVaccines &lt;- tibble(\n  Treatment=c(\"Placebo\",\"Vaccine\"),\n  Pfizer=c(90,5),ModernaAll=c(95,5),\n  ModernaSevere=c(11,0),\n  N=c(15000,15000))\ncovidVaccines.N &lt;- 15000\nkable(covidVaccines)\n\n\n\n\nTreatment\nPfizer\nModernaAll\nModernaSevere\nN\n\n\n\n\nPlacebo\n90\n95\n11\n15000\n\n\nVaccine\n5\n5\n0\n15000"
  },
  {
    "objectID": "IntroStats/CovidVaccines.html#measures-of-effectiveness",
    "href": "IntroStats/CovidVaccines.html#measures-of-effectiveness",
    "title": "Covid-19 Vaccines",
    "section": "Measures of Effectiveness",
    "text": "Measures of Effectiveness\nWe start with a cross-tab\n\n\n\nTreatment\nSick\nHealthy\nTotal\n\n\n\n\nPlacebo\nSP\nHP\nNP\n\n\nVaccine\nSV\nHV\nNV\n\n\nTotal\nNS\nNH\nN\n\n\n\nOdds of getting sick\nPlacebo: $ SP/HP $ Vaccine: $ SV/HV $\n\nCross Product (Odds) Ratio\n\\[ OR = \\frac{SP/HP}{SV/HV} \\] How much does your odds of getting sick increase if you get the placebo instead of the vaccine.\n\n\nRisk Ratio\n\\[ RR = \\frac{SP/NP}{SV/NV} \\] How much does your probiliby of getting sick increase if you get the placebo instead of the vaccine.\n\n\nVaccine Effectiveness\n\\[ VE = 100 (1 - \\frac{1}{RR}) \\]\n\n\nChi-square test\nNull hypothesis is that getting the disease is independent of the vaccine. In other words, \\(OR=RR=1\\).\n\\[ SV/NV = SP/NP \\]\nLarge chi-squared value incidates that cross product rate is not 1 (but doesn’t tell if placebo or vaccine is better!\n\n\nZ-score test\nAnother way to work with these data is to calculate probabilities of infection for each group and the standard errors. Then can use the \\(z\\)-test to compare.\n\\[p_V = p(S|V) = SV/NV \\qquad SE(p_V) = \\sqrt{p_V(1-p_V)/NV} \\] \\[p_P = p(S|P) = SP/NP \\qquad SE(p_P) = \\sqrt{p_P(1-p_P)/NP} \\] \\[ z = \\frac{p_P-p_V}{\\sqrt{SE(p_V)^2 + SE(p_P)^2}}\\]"
  },
  {
    "objectID": "IntroStats/CovidVaccines.html#pfizer-vaccine",
    "href": "IntroStats/CovidVaccines.html#pfizer-vaccine",
    "title": "Covid-19 Vaccines",
    "section": "Pfizer Vaccine",
    "text": "Pfizer Vaccine\nThere were around 30,000 volunteers in the Phase 3 trials; 15,000 in each arm.\n\ncovidVaccines %&gt;% \n  mutate(p.Pfizer=Pfizer/N) %&gt;%\n  mutate(s.Pfizer=sqrt(p.Pfizer*(1-p.Pfizer)/N)) -&gt;\n  covidVaccines\n\nselect(covidVaccines,Treatment,contains(\"Pfizer\")) %&gt;% kable(digits=c(4,5))\n\n\n\n\nTreatment\nPfizer\np.Pfizer\ns.Pfizer\n\n\n\n\nPlacebo\n90\n6e-03\n0.00063\n\n\nVaccine\n5\n3e-04\n0.00015\n\n\n\n\n\n\nggplot(covidVaccines,aes(x=Treatment,y=p.Pfizer,ymin=p.Pfizer-2*s.Pfizer,ymax=p.Pfizer+2*s.Pfizer)) + geom_pointrange()\n\n\n\n\n\n\n\n\n\nX^2 – Pfizer\nIn SPSS this is done by producing a cross-tab. We don’t have the number of negative cases in each arm of the study, but up to rounding error it is just the sample size, so we will use that.\n\n\n\nPfizer Cross Tab\n\n\n\n\n\nPhizer X2\n\n\n\nchisq.test(as.matrix(select(covidVaccines,Pfizer,N)))\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  as.matrix(select(covidVaccines, Pfizer, N))\nX-squared = 74.034, df = 1, p-value &lt; 2.2e-16\n\n\n\np &lt;- pull(covidVaccines,p.Pfizer)\ns &lt;- pull(covidVaccines,s.Pfizer)\n\nz &lt;- (p[1]-p[2])/sqrt(sum(s^2))\npz &lt;- 1-pnorm(z)\ncat(\"Z = \",round(z,2), \"p = \",round(pz,3),\"\\n\")\n\nZ =  8.75 p =  0 \n\n\n\n\n\nPfizer Risk Ratio\n\n\n\np &lt;- pull(covidVaccines,p.Pfizer)\nRR &lt;- p[1]/p[2]\nVE &lt;- 100*(1-1/RR)\ncat(\"Risk Ratio: \",round(RR,2),\n    \"Vaccine Effectiveness: \",round(VE,1),\"\\n\")\n\nRisk Ratio:  18 Vaccine Effectiveness:  94.4"
  },
  {
    "objectID": "IntroStats/CovidVaccines.html#moderna-vaccine-all-cases",
    "href": "IntroStats/CovidVaccines.html#moderna-vaccine-all-cases",
    "title": "Covid-19 Vaccines",
    "section": "Moderna Vaccine – All Cases",
    "text": "Moderna Vaccine – All Cases\nThere were around 30,000 volunteers in the Phase 3 trials; 15,000 in each arm.\n\ncovidVaccines %&gt;% \n  mutate(p.ModernaAll=ModernaAll/N) %&gt;%\n  mutate(s.ModernaAll=sqrt(p.ModernaAll*(1-p.ModernaAll)/N)) -&gt;\n  covidVaccines\n\nselect(covidVaccines,Treatment,contains(\"ModernaAll\")) %&gt;% kable(digits=c(4,5))\n\n\n\n\nTreatment\nModernaAll\np.ModernaAll\ns.ModernaAll\n\n\n\n\nPlacebo\n95\n0.0063\n0.00065\n\n\nVaccine\n5\n0.0003\n0.00015\n\n\n\n\n\n\nggplot(covidVaccines,aes(x=Treatment,y=p.ModernaAll,ymin=p.ModernaAll-2*s.ModernaAll,ymax=p.ModernaAll+2*s.ModernaAll)) + geom_pointrange()\n\n\n\n\n\n\n\n\n\nX^2 – Moderna (All Cases)\nIn SPSS this is done by producing a cross-tab. We don’t have the number of negative cases in each arm of the study, but up to rounding error it is just the sample size, so we will use that.\n\n\n\nModerna All Cases Cross Tab\n\n\n\n\n\nModerna All Cases X2\n\n\n\nchisq.test(as.matrix(select(covidVaccines,ModernaAll,N)))\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  as.matrix(select(covidVaccines, ModernaAll, N))\nX-squared = 78.942, df = 1, p-value &lt; 2.2e-16\n\n\n\np &lt;- pull(covidVaccines,p.ModernaAll)\ns &lt;- pull(covidVaccines,s.ModernaAll)\n\nz &lt;- (p[1]-p[2])/sqrt(sum(s^2))\npz &lt;- 1-pnorm(z)\ncat(\"Z = \",round(z,2), \"p = \",round(pz,3),\"\\n\")\n\nZ =  9.03 p =  0 \n\n\n\n\n\nModerna All Cases Risk Ratio\n\n\n\np &lt;- pull(covidVaccines,p.ModernaAll)\nRR &lt;- p[1]/p[2]\nVE &lt;- 100*(1-1/RR)\ncat(\"Risk Ratio: \",round(RR,2),\n    \"Vaccine Effectiveness: \",round(VE,1),\"\\n\")\n\nRisk Ratio:  19 Vaccine Effectiveness:  94.7"
  },
  {
    "objectID": "IntroStats/CovidVaccines.html#moderna-vaccine-severe-cases",
    "href": "IntroStats/CovidVaccines.html#moderna-vaccine-severe-cases",
    "title": "Covid-19 Vaccines",
    "section": "Moderna Vaccine – Severe Cases",
    "text": "Moderna Vaccine – Severe Cases\n\ncovidVaccines %&gt;% \n  mutate(p.ModernaSevere=ModernaSevere/N) %&gt;%\n  mutate(s.ModernaSevere=sqrt(p.ModernaSevere*(1-p.ModernaSevere)/N)) -&gt;\n  covidVaccines\n\nselect(covidVaccines,Treatment,contains(\"ModernaSevere\")) %&gt;% kable(digits=c(4,5))\n\n\n\n\nTreatment\nModernaSevere\np.ModernaSevere\ns.ModernaSevere\n\n\n\n\nPlacebo\n11\n7e-04\n0.00022\n\n\nVaccine\n0\n0e+00\n0.00000\n\n\n\n\n\n\nggplot(covidVaccines,aes(x=Treatment,y=p.ModernaSevere,ymin=p.ModernaSevere-2*s.ModernaSevere,ymax=p.ModernaSevere+2*s.ModernaSevere)) + geom_pointrange()\n\n\n\n\n\n\n\n\n\nX^2 – Moderna (All Cases)\nIn SPSS this is done by producing a cross-tab. We don’t have the number of negative cases in each arm of the study, but up to rounding error it is just the sample size, so we will use that.\n\n\n\nModerna Severe Cases Cross Tab\n\n\n\n\n\nModerna Severe Cases X2\n\n\n\nchisq.test(as.matrix(select(covidVaccines,ModernaSevere,N)))\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  as.matrix(select(covidVaccines, ModernaSevere, N))\nX-squared = 9.0869, df = 1, p-value = 0.002574\n\n\n\np &lt;- pull(covidVaccines,p.ModernaSevere)\ns &lt;- pull(covidVaccines,s.ModernaSevere)\n\nz &lt;- (p[1]-p[2])/sqrt(sum(s^2))\npz &lt;- 1-pnorm(z)\ncat(\"Z = \",round(z,2), \"p = \",round(pz,3),\"\\n\")\n\nZ =  3.32 p =  0 \n\n\n\np &lt;- pull(covidVaccines,p.ModernaSevere)\nRR &lt;- p[1]/p[2]\nVE &lt;- 100*(1-1/RR)\ncat(\"Risk Ratio: \",round(RR,2),\n    \"Vaccine Effectiveness: \",round(VE,1),\"\\n\")\n\nRisk Ratio:  Inf Vaccine Effectiveness:  100 \n\n\nYikes! The estimate for the chances of getting Severe Covid-19 with the virus is 0. Divide by zero error!\nBut probability zero means impossible. That is not right!\n\n\nContinuity Correction\nFix this by adding a conditinuity correction. We add 1/2 to all of the entries in the table.\nIn particular, this makes the estimated rate for getting severe COVID-19 \\(\\frac{1}{2}/(N+1)\\).\n\ncovidVaccines %&gt;% \n  mutate(p.ModernaSevere=(ModernaSevere+.5)/(N+1)) %&gt;%\n  mutate(s.ModernaSevere=sqrt(p.ModernaSevere*(1-p.ModernaSevere)/(N+1))) -&gt;\n  covidVaccines\n\nselect(covidVaccines,Treatment,contains(\"ModernaSevere\")) %&gt;% kable(digits=c(5,5))\n\n\n\n\nTreatment\nModernaSevere\np.ModernaSevere\ns.ModernaSevere\n\n\n\n\nPlacebo\n11\n0.00077\n0.00023\n\n\nVaccine\n0\n0.00003\n0.00005\n\n\n\n\n\n\nggplot(covidVaccines,aes(x=Treatment,y=p.ModernaSevere,ymin=p.ModernaSevere-2*s.ModernaSevere,ymax=p.ModernaSevere+2*s.ModernaSevere)) + geom_pointrange()\n\n\n\n\n\n\n\n\n\np &lt;- pull(covidVaccines,p.ModernaSevere)\nRR &lt;- p[1]/p[2]\nVE &lt;- 100*(1-1/RR)\ncat(\"Risk Ratio: \",round(RR,2),\n    \"Vaccine Effectiveness: \",round(VE,1),\"\\n\")\n\nRisk Ratio:  23 Vaccine Effectiveness:  95.7"
  },
  {
    "objectID": "IntroStats/CovidVaccines.html#references",
    "href": "IntroStats/CovidVaccines.html#references",
    "title": "Covid-19 Vaccines",
    "section": "References:",
    "text": "References:\n\nStatNews article on Pfizer vaccine: https://www.statnews.com/2020/11/09/covid-19-vaccine-from-pfizer-and-biontech-is-strongly-effective-early-data-from-large-trial-indicate/\nOfficial Protocol document from Pfizer: https://www.pfizer.com/science/coronavirus\nPfizer Press Release: https://www.pfizer.com/news/press-release/press-release-detail/pfizer-and-biontech-announce-vaccine-candidate-against\nModerna Press Release: https://investors.modernatx.com/news-releases/news-release-details/modernas-covid-19-vaccine-candidate-meets-its-primary-efficacy\nEntries from Andrew Gelman’s Blog: https://statmodeling.stat.columbia.edu/2020/11/16/estimating-efficacy-of-the-vaccine-from-95-true-infections/\n\nhttps://statmodeling.stat.columbia.edu/2020/11/11/the-pfizer-biontech-vaccine-may-be-a-lot-more-effective-than-you-think/\n\nHow to use SPSS to obtain Odd Ratio and Relative Risk http://brahms.emu.edu.tr/icetin/spss8-RR-OR.pdf"
  },
  {
    "objectID": "IntroStats/Chi2calculator.html",
    "href": "IntroStats/Chi2calculator.html",
    "title": "Chi-squared Calculator",
    "section": "",
    "text": "In this tool, you input a \\(\\chi^2\\) score and the degrees of freedom, and get a corresponding \\(p\\)-value.\n\n\n\n\n\n\nWhich tails\n\nUpper tail: Pr(x^2 &lt; X^2)\nLower tail: Pr(X^2 &lt; x^2)\n\n\n\n\n\n\nchi-squared value:\n\n\n\n\n\nDegrees of Freedom"
  },
  {
    "objectID": "IntroStats/Chi2calculator.html#chi-squared-probabilities.",
    "href": "IntroStats/Chi2calculator.html#chi-squared-probabilities.",
    "title": "Chi-squared Calculator",
    "section": "",
    "text": "In this tool, you input a \\(\\chi^2\\) score and the degrees of freedom, and get a corresponding \\(p\\)-value.\n\n\n\n\n\n\nWhich tails\n\nUpper tail: Pr(x^2 &lt; X^2)\nLower tail: Pr(X^2 &lt; x^2)\n\n\n\n\n\n\nchi-squared value:\n\n\n\n\n\nDegrees of Freedom"
  },
  {
    "objectID": "IntroStats/Chi2calculator.html#chi-square-quantiles-critical-values.",
    "href": "IntroStats/Chi2calculator.html#chi-square-quantiles-critical-values.",
    "title": "Chi-squared Calculator",
    "section": "Chi-square Quantiles (Critical values).",
    "text": "Chi-square Quantiles (Critical values).\nIn this tool, you input a probability and degrees of freedom, and get a corresponding \\(X^2\\) score.\n\n\n\n\n\n\nWhich tails\n\nUpper tail: Pr(x^2 &lt; X^2)\nLower tail: Pr(X^2 &lt; x^2)\n\n\n\n\n\n\nProbability of shaded region:\n\n\n\n\n\nDegrees of Freedom\n\n\n\n\n\n\n\n\n\n\nFor \\(\\chi^2\\) tests, one almost always looks at upper tail, and rarely look at the lower tail (does the data fit better than expected). Two-tailed tests are never (“What never?”, “No Never!”, “What Never?”, “Well hardly ever!”, Gilbert & Sulivan, HMS Pinfore) done."
  },
  {
    "objectID": "IntroStats/Correlation.html",
    "href": "IntroStats/Correlation.html",
    "title": "Correlation Coefficient",
    "section": "",
    "text": "This demonstration will use some random data. Lets start by generating the random data. So give a random seed and pick a sample size for your sample.\nSample Size:\n\n25\n50\n100\n250\n500\n1000\n\n\n\n\n\n\nRandom number Seed (integer)"
  },
  {
    "objectID": "IntroStats/Correlation.html#regression.",
    "href": "IntroStats/Correlation.html#regression.",
    "title": "Correlation Coefficient",
    "section": "Regression.",
    "text": "Regression.\nGalton’s discovery was that if you want to predict \\(Y\\) from \\(X\\), then you want to regress that prediction towards the mean. If \\(X\\) and \\(Y\\) were perfectly correlated, then the \\(z\\)-score for a variable on the \\(X\\) scale would be the same for the variable on the \\(Y\\) scale, so all we need to do is change units. The ratio \\(\\sigma_Y/\\sigma_X\\) changes units from \\(X\\) to \\(Y\\). We also want the mean of \\(X\\) to map to the mean of \\(Y\\). The equation for this change-of-units line, the SD-line, is: \\[ \\widetilde y = \\frac{\\sigma_Y}{\\sigma_X} x + \\left ( \\mu_Y - \\frac{\\sigma_Y}{\\sigma_X} \\mu_X\\right ) .\\] The first term is the change of units, the second term makes sure the line goes through the mean of \\(X\\) and the mean of \\(Y\\).\nThe ideal discount is the correlation coefficient \\(\\rho_{XY}\\). This gives the the following final regression line: \\[\\widehat y = \\rho_{XY}\\frac{\\sigma_Y}{\\sigma_X} x + \\left ( \\mu_Y - \\rho_{XY}\\frac{\\sigma_Y}{\\sigma_X} \\mu_X\\right ) .\\] Because second term has \\(\\rho_{XY}\\) in it as well, this will make the predicted value closer to the mean of \\(Y\\), \\(\\mu_Y\\).\n The notations \\(\\widetilde{y}\\) and \\(\\widehat{y}\\) indicate predicted values for \\(y\\). The y hat (\\(\\widehat y\\)) notation is reserved for what is called maximum likelihood predictions. In the case of a regression with normally distributed errors, the maximum likelihood predictor is also the least squares estimator. Usually, the estimators use the sample values, thus in the regression equation: \\[ \\widehat{b_1} = r_{XY}\\frac{s_X}{s_Y}\\;;\\qquad \\widehat{b_0}=\\bar Y -r_{XY}\\frac{s_X}{s_Y} \\bar X\\; .\\]"
  },
  {
    "objectID": "IntroStats/Correlation.html#lets-try-it.",
    "href": "IntroStats/Correlation.html#lets-try-it.",
    "title": "Correlation Coefficient",
    "section": "Lets Try it.",
    "text": "Lets Try it.\nIn the graph below, you can set the mean and standard deviation of both X and Y as well as the correlation coefficient. The SD line is a dashed blue, and the regression line is a solid red. Play around for a bit.\n\n\n\n\n\n\nMean of X:\n\n\n\n\n\nStandard Deviation of X:\n\n\n\n\n\nMean of Y:\n\n\n\n\n\nStandard Deviation of Y:\n\n\n\n\n\nCorrelation between X and Y:\n\n\n\n\n\n\n\n\n\n\nNotice how changing the means and standard deviations doesn’t change much except the numbers in the equations and the labels on the axis. This is because R is automatically adjusting the scale of the graphs to fit the data. In this view, the SD line, the one that is a change of scale, is a perfect 45 degress, no matter what.\nActually, if you set the correlation coefficient to be the same in the two plots, they should look the same. The two plots show the same data, its just in the lower plot, the data are transformed to fit the statistis. Correlation is a property that is independent from Mean and SD."
  },
  {
    "objectID": "IntroStats/Correlation.html#for-the-more-mathematically-inclined.",
    "href": "IntroStats/Correlation.html#for-the-more-mathematically-inclined.",
    "title": "Correlation Coefficient",
    "section": "For the more mathematically inclined.",
    "text": "For the more mathematically inclined.\nThis is how I generated the data with the given correlations.\nFor the first plot, I first generated two vectors of standard normal numbers, \\({\\bf X}\\) and \\({\\bf e}\\), that is with mean 0 and standard deviation 1. Then I defined \\({\\bf Y}\\) with the following equation: \\[ Y_i = \\rho_{XY}X_i + \\sqrt{(1-\\rho_{XY}^2)}\\ e_i\\] As \\(\\rho_{XY}^2 + (1-\\rho_{XY}^2) =1\\), \\({\\bf Y}\\) also has mean 0 and standard deviation 1.\nFor the second plot, I used the following expressions: \\[ XX_i = \\mu_X + \\sigma_X X_i\\;; \\qquad YY_i = \\mu_X + \\sigma_Y Y_i\\;.\\] The transformation is exactly the same."
  },
  {
    "objectID": "IntroStats/NormalCalculator.html",
    "href": "IntroStats/NormalCalculator.html",
    "title": "Normal Calculator",
    "section": "",
    "text": "In this tool, you input a z score, and get a corresponding normal probability.\n\n\n\n\n\n\nWhich tails\n\nUpper tail: Pr(z &lt; Z)\nLower tail: Pr(Z &lt; z)\nBoth tails: Pr(Z &lt;-z or z&lt; Z)\nMiddle: Pr(-z &lt; Z &lt; z)\n\n\n\n\n\n\nNormal quantile (z):"
  },
  {
    "objectID": "IntroStats/NormalCalculator.html#normal-probabilities.",
    "href": "IntroStats/NormalCalculator.html#normal-probabilities.",
    "title": "Normal Calculator",
    "section": "",
    "text": "In this tool, you input a z score, and get a corresponding normal probability.\n\n\n\n\n\n\nWhich tails\n\nUpper tail: Pr(z &lt; Z)\nLower tail: Pr(Z &lt; z)\nBoth tails: Pr(Z &lt;-z or z&lt; Z)\nMiddle: Pr(-z &lt; Z &lt; z)\n\n\n\n\n\n\nNormal quantile (z):"
  },
  {
    "objectID": "IntroStats/NormalCalculator.html#normal-quantiles.",
    "href": "IntroStats/NormalCalculator.html#normal-quantiles.",
    "title": "Normal Calculator",
    "section": "Normal Quantiles.",
    "text": "Normal Quantiles.\nIn this tool, you input a probability, and get a corresponding z score.\n\n\n\n\n\n\nWhich tails\n\nUpper tail: Pr(z &lt; Z)\nLower tail: Pr(Z &lt; z)\nBoth tails: Pr(Z &lt;-z or z&lt; Z)\nMiddle: Pr(-z &lt; Z &lt; z)\n\n\n\n\n\n\nProbability of shaded region:"
  },
  {
    "objectID": "IntroStats/LawOfLargeNumbersAnimated.html",
    "href": "IntroStats/LawOfLargeNumbersAnimated.html",
    "title": "Law of Large Numbers",
    "section": "",
    "text": "This is pretty close to the frequency definition of probability. Suppose the probability of some event is \\(p\\). Suppose further than we sample \\(N\\) times from the process that generates this event. Let \\(p_N\\) be the proportion of times the event occurs in \\(N\\) trials. As \\(N\\) gets bigger and bigger, \\(p_N\\) gets closer and closer to \\(p\\).\n\n(Skip this unless you are good with calculus.)_ This is one of those epsilon-delta theorems. So let \\(\\delta\\) be a difference from \\(p\\) and let \\(\\epsilon\\) be a small probability. For any \\(\\epsilon\\) and \\(\\delta\\), there exists an \\(N\\) such that \\(P(|p_N-p|&gt;\\delta) &lt; \\epsilon\\).\n\n\n\nIn the picture below, pick a probability \\(p\\) and a sample size \\(N\\). The computer will generate samples up to \\(N\\) and plot \\(p_N\\).\nThe \\(\\delta\\)-line is an error bound plus or minus \\(\\delta\\) units from the target \\(p\\). This is a target so you can judge how close you got.\n\n\n\n\n\n\nMaximum Sample Size:\n\n50\n100\n200\n500\n1000\n\n\n\n\n\n\nProbability of event (p)\n\n\n\n\n\nDistance of reference line from target (delta)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can use the Law of Large Numbers to prove an important theorem. As the sample size gets larger and larger, the sample looks more and more like the population it is drawn from.\n\nTechnically, the Law of Large Numbers refers to the result above. But we can use it so show a very important basis of statistics. Suppose we have some kind of distribution, \\(F(x)\\), that generates numbers, \\(X\\). Recall that the definition of \\(F(x)=\\Pr(X \\leq x)\\).\nDraw a sample of size \\(N\\) from this distribution. Now consider the sampled data points \\(X_1,\\ldots,X_N\\), and consider sampling a new value \\(Y\\) from that distribution. Let \\(F_N(y) = \\Pr(Y \\leq y)\\). This is sometimes called the bootstrap distribution.\nBy the law of large numbers, for every \\(y\\), as \\(N\\) gets large \\(F_N(y) \\rightarrow F(y)\\). So the sample distribution \\(F_N()\\) converges to the \\(F()\\).\n\n\n\n\nPick a distribution: * Normal – standard normal * Exponential – highly skewed * Gamma (shape = 3) – skewed * T (df =3) – high kurtosis\nSlide the sample size up and down, notice how the empirical distribution function and histogram coverge to the theoretical distribution function and density.\n\n\n\n\n\n\nDistribution Type\n\nNormal\nExponential\nGamma\nT\n\n\n\n\n\n\nMaximum Sample Size:\n\n50\n100\n200\n500\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSee also the non-animated version."
  },
  {
    "objectID": "IntroStats/LawOfLargeNumbersAnimated.html#a-demonstration.",
    "href": "IntroStats/LawOfLargeNumbersAnimated.html#a-demonstration.",
    "title": "Law of Large Numbers",
    "section": "",
    "text": "In the picture below, pick a probability \\(p\\) and a sample size \\(N\\). The computer will generate samples up to \\(N\\) and plot \\(p_N\\).\nThe \\(\\delta\\)-line is an error bound plus or minus \\(\\delta\\) units from the target \\(p\\). This is a target so you can judge how close you got.\n\n\n\n\n\n\nMaximum Sample Size:\n\n50\n100\n200\n500\n1000\n\n\n\n\n\n\nProbability of event (p)\n\n\n\n\n\nDistance of reference line from target (delta)"
  },
  {
    "objectID": "IntroStats/LawOfLargeNumbersAnimated.html#convergence-of-distributions-boot-strap-distribution",
    "href": "IntroStats/LawOfLargeNumbersAnimated.html#convergence-of-distributions-boot-strap-distribution",
    "title": "Law of Large Numbers",
    "section": "",
    "text": "We can use the Law of Large Numbers to prove an important theorem. As the sample size gets larger and larger, the sample looks more and more like the population it is drawn from.\n\nTechnically, the Law of Large Numbers refers to the result above. But we can use it so show a very important basis of statistics. Suppose we have some kind of distribution, \\(F(x)\\), that generates numbers, \\(X\\). Recall that the definition of \\(F(x)=\\Pr(X \\leq x)\\).\nDraw a sample of size \\(N\\) from this distribution. Now consider the sampled data points \\(X_1,\\ldots,X_N\\), and consider sampling a new value \\(Y\\) from that distribution. Let \\(F_N(y) = \\Pr(Y \\leq y)\\). This is sometimes called the bootstrap distribution.\nBy the law of large numbers, for every \\(y\\), as \\(N\\) gets large \\(F_N(y) \\rightarrow F(y)\\). So the sample distribution \\(F_N()\\) converges to the \\(F()\\)."
  },
  {
    "objectID": "IntroStats/LawOfLargeNumbersAnimated.html#demonstration-of-convergence-of-distributions.",
    "href": "IntroStats/LawOfLargeNumbersAnimated.html#demonstration-of-convergence-of-distributions.",
    "title": "Law of Large Numbers",
    "section": "",
    "text": "Pick a distribution: * Normal – standard normal * Exponential – highly skewed * Gamma (shape = 3) – skewed * T (df =3) – high kurtosis\nSlide the sample size up and down, notice how the empirical distribution function and histogram coverge to the theoretical distribution function and density.\n\n\n\n\n\n\nDistribution Type\n\nNormal\nExponential\nGamma\nT\n\n\n\n\n\n\nMaximum Sample Size:\n\n50\n100\n200\n500\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSee also the non-animated version."
  },
  {
    "objectID": "IntroStats/SkewnessBoxplot.html",
    "href": "IntroStats/SkewnessBoxplot.html",
    "title": "Skewness Boxplot Practice",
    "section": "",
    "text": "In this exercise, the computer will generate 3 datasets: A, B and C. These will be randomly assigned to a positively skewed, negatively skewed, and symmetric distribution type. Your job is to determine which is which.\nYou can redraw from the same distributions by changing the sample size.\n\nlibrary(shiny)\nui &lt;- fluidPage(\ninputPanel(\n  selectInput(\"nn\", label = \"Sample Size:\",\n              choices = c(50, 100, 500, 1000), selected = 100)),\nmainPanel(\n  plotOutput(\"boxplots\")),\n  h4(\"Which is which?\"),\n  p(\"Identify the skewness of each distribution.\"),\n  do.call(inputPanel,\n         lapply(names(key), function (k)\n                selectInput(k, label=k,\n                  choices=c(Unknown=\"unknown\", \n                           longnames),\n                  selected=\"unknown\"))),\n  h4(\"Answers:\\n\"),\n  tableOutput(\"answers\"))\n\nserver &lt;- function (input,output) {\n  output$boxplots &lt;- renderPlot({\n    ## Draw random data \n    kdat &lt;- lapply(names(key), function (k) {\n    x &lt;-do.call(distlist[[key[k]]][[kdist[k]]],\n                list(input$nn))\n      scale(x,(min(x)+max(x))/2,(max(x)-min(x)))*100+50\n  })\n  names(kdat) &lt;- names(key)\n  kdat &lt;- as.data.frame(kdat)\n  \n  boxplot(kdat,xlab=\"X\")\n\n})\n  output$answers &lt;- renderTable({\n  answer &lt;- sapply(names(key),\n   function (k) {\n      if (input[[k]]==\"unknown\") {\n        \"Make your selection.\\n\"\n      } else {\n        paste(ifelse(input[[k]]==key[k],\n                     \"Correct:\", \"Incorrect:\"),\n              \"Distribution was\",kdist[k],\n               \"(\",\n        names(longnames)[grep(key[k],longnames)],\n               \")\\n\")\n    }})\n  names(answer) &lt;- names(key)\n  as.data.frame(answer)\n}, colnames=FALSE,rownames=TRUE)\n}\nshinyApp(ui=ui,server=server)\n\n\n\n\nTo try again with different distributions, reload the page. If you are having trouble, try increasing the sample size: sometimes a small sample won’t display the characteristics of the distribution strongly."
  },
  {
    "objectID": "IntroStats/SkewnessBoxplot.html#skewness-determination-exercise.",
    "href": "IntroStats/SkewnessBoxplot.html#skewness-determination-exercise.",
    "title": "Skewness Boxplot Practice",
    "section": "",
    "text": "In this exercise, the computer will generate 3 datasets: A, B and C. These will be randomly assigned to a positively skewed, negatively skewed, and symmetric distribution type. Your job is to determine which is which.\nYou can redraw from the same distributions by changing the sample size.\n\nlibrary(shiny)\nui &lt;- fluidPage(\ninputPanel(\n  selectInput(\"nn\", label = \"Sample Size:\",\n              choices = c(50, 100, 500, 1000), selected = 100)),\nmainPanel(\n  plotOutput(\"boxplots\")),\n  h4(\"Which is which?\"),\n  p(\"Identify the skewness of each distribution.\"),\n  do.call(inputPanel,\n         lapply(names(key), function (k)\n                selectInput(k, label=k,\n                  choices=c(Unknown=\"unknown\", \n                           longnames),\n                  selected=\"unknown\"))),\n  h4(\"Answers:\\n\"),\n  tableOutput(\"answers\"))\n\nserver &lt;- function (input,output) {\n  output$boxplots &lt;- renderPlot({\n    ## Draw random data \n    kdat &lt;- lapply(names(key), function (k) {\n    x &lt;-do.call(distlist[[key[k]]][[kdist[k]]],\n                list(input$nn))\n      scale(x,(min(x)+max(x))/2,(max(x)-min(x)))*100+50\n  })\n  names(kdat) &lt;- names(key)\n  kdat &lt;- as.data.frame(kdat)\n  \n  boxplot(kdat,xlab=\"X\")\n\n})\n  output$answers &lt;- renderTable({\n  answer &lt;- sapply(names(key),\n   function (k) {\n      if (input[[k]]==\"unknown\") {\n        \"Make your selection.\\n\"\n      } else {\n        paste(ifelse(input[[k]]==key[k],\n                     \"Correct:\", \"Incorrect:\"),\n              \"Distribution was\",kdist[k],\n               \"(\",\n        names(longnames)[grep(key[k],longnames)],\n               \")\\n\")\n    }})\n  names(answer) &lt;- names(key)\n  as.data.frame(answer)\n}, colnames=FALSE,rownames=TRUE)\n}\nshinyApp(ui=ui,server=server)\n\n\n\n\nTo try again with different distributions, reload the page. If you are having trouble, try increasing the sample size: sometimes a small sample won’t display the characteristics of the distribution strongly."
  },
  {
    "objectID": "IntroStats/SkewnessBoxplot.html#what-to-look-for",
    "href": "IntroStats/SkewnessBoxplot.html#what-to-look-for",
    "title": "Skewness Boxplot Practice",
    "section": "What to look for:",
    "text": "What to look for:\n\nIs the box from median to quartile longer on one side than the other?\nIs the whisker longer on one side than the other?\nAre there outliers on one side and not the other?\n\nAll three of these are signs of skewness in that direction (longer box, whisker, or outliers)."
  },
  {
    "objectID": "IntroStats/SkewnessBoxplot.html#related-pages",
    "href": "IntroStats/SkewnessBoxplot.html#related-pages",
    "title": "Skewness Boxplot Practice",
    "section": "Related Pages",
    "text": "Related Pages\nHere are the other exercises in this series:\n\nSkewness Practice:\n\nHistograms\nBoxplots\nQ-Q Plots\n\nKurtosis Practice:\n\nHistograms\nBoxplots\nQ-Q Plots"
  },
  {
    "objectID": "IntroStats/RegressionPredictionA.html",
    "href": "IntroStats/RegressionPredictionA.html",
    "title": "Regression Prediction Error",
    "section": "",
    "text": "We will work with an example data set from Ezekiel (1930) which provides information about the speed of a number of cars and the stopping distance in feet.\n\nhelp(cars)\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00  \n\n\nLets take a quick look at these data.\n\nplot(dist~speed,data=cars,xlab=\"Speed (mph)\", ylab=\"Stopping Distance (ft)\")\nabline(lm(dist~speed,data=cars))\nlines(lowess(cars),col=2,lty=2)\n\n\n\n\n\n\n\n\nThe solid black line is the least squares regression line, or basic model.\nThe dashed red line is a lowess curve fit to the same date.\n\nThere may be a little bit of a curve here, but it is hard to see.\n\nWe will go ahead and fit a regression using least squares. (This the the lm or linear model function in R.)\n\ncars.fit &lt;- lm (dist~speed,data=cars)\nprint(cars.fit)\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nCoefficients:\n(Intercept)        speed  \n    -17.579        3.932  \n\n\nThe method of least squares or maximum likelihood (which in the case of simple regression are the same) finds the single best fitting line.\n\nLeast Squares means the line has the smallest some of squared residuals.\nMaximum Likelihood means that these are the parameters (slope and intercept) that have the highest probability of generating the target data."
  },
  {
    "objectID": "IntroStats/RegressionPredictionA.html#the-cars-data-set",
    "href": "IntroStats/RegressionPredictionA.html#the-cars-data-set",
    "title": "Regression Prediction Error",
    "section": "",
    "text": "We will work with an example data set from Ezekiel (1930) which provides information about the speed of a number of cars and the stopping distance in feet.\n\nhelp(cars)\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00  \n\n\nLets take a quick look at these data.\n\nplot(dist~speed,data=cars,xlab=\"Speed (mph)\", ylab=\"Stopping Distance (ft)\")\nabline(lm(dist~speed,data=cars))\nlines(lowess(cars),col=2,lty=2)\n\n\n\n\n\n\n\n\nThe solid black line is the least squares regression line, or basic model.\nThe dashed red line is a lowess curve fit to the same date.\n\nThere may be a little bit of a curve here, but it is hard to see.\n\nWe will go ahead and fit a regression using least squares. (This the the lm or linear model function in R.)\n\ncars.fit &lt;- lm (dist~speed,data=cars)\nprint(cars.fit)\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nCoefficients:\n(Intercept)        speed  \n    -17.579        3.932  \n\n\nThe method of least squares or maximum likelihood (which in the case of simple regression are the same) finds the single best fitting line.\n\nLeast Squares means the line has the smallest some of squared residuals.\nMaximum Likelihood means that these are the parameters (slope and intercept) that have the highest probability of generating the target data."
  },
  {
    "objectID": "IntroStats/RegressionPredictionA.html#lots-of-different-regression-lines",
    "href": "IntroStats/RegressionPredictionA.html#lots-of-different-regression-lines",
    "title": "Regression Prediction Error",
    "section": "Lots of different regression lines",
    "text": "Lots of different regression lines\nI’ll try the regression using a different method (Markov Chain Monte Carlo, or MCMC). In this method we sample 4000 different plausible sets of parameters that could have given rise to the data. (These are sampled with a probability proportional to how likely they are to have generated the observed data).\nThe printed summary shows the median of the 4000 samples. It should be close to, but not exactly the same as, the least squares/maximum likelihood estimate.\n\n#library(rstanarm)  ## Called above\ncars.mcmc &lt;- stan_glm(dist~speed,data=cars,refresh=0)\ncars.coef &lt;- as.matrix(cars.mcmc$stanfit)\nprint(cars.mcmc)\n\nstan_glm\n family:       gaussian [identity]\n formula:      dist ~ speed\n observations: 50\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) -17.7    7.0 \nspeed         3.9    0.4 \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 15.6    1.6  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg"
  },
  {
    "objectID": "IntroStats/RegressionPredictionA.html#mean-confidence-interval",
    "href": "IntroStats/RegressionPredictionA.html#mean-confidence-interval",
    "title": "Regression Prediction Error",
    "section": "Mean Confidence Interval",
    "text": "Mean Confidence Interval\nThe MCMC approach is useful because it helps us remember that the estimates that are produced by the [least squares] regression are not the truth, but rather just the most likely set of parameters. There are other possibilities that are nearly as likely.\nThe next graph is designed to show this.\nThe first N (you can adjust using the slider) samples from the MCMC are plotted as gray lines.\nThe least squares line is plotted in black.\n\n\n\n\n\n\nConfidence Level:\n\n50\n68\n90\n95\n99\n\n\n\n\n\n\nNumber of plausible values to plot\n\n\n\n\n\n\n\n\n\n\nNote the dashed curves surrounding the regression line.\nThese are the confidence interval for the regression line.\nThe level of the confidence interval is how many of these plausible regression lines should fit between the dashed curves (expressed as a percentage).\nSPSS calls this the “mean” prediction interval. R calls it the “confidence” interval.\nYou can use the graph (or the R predict function, or the prediction option in SPSS) to get a prediction for the average (over a number of trials) stopping time at a given speed."
  },
  {
    "objectID": "IntroStats/RegressionPredictionA.html#individual-prediction-interval",
    "href": "IntroStats/RegressionPredictionA.html#individual-prediction-interval",
    "title": "Regression Prediction Error",
    "section": "Individual Prediction Interval",
    "text": "Individual Prediction Interval\nThe mean confidence interval above is for the average over many attempts at stopping the car.\nWe don’t expect a single attempt to fall exactly on the line.\n\n68% of the time we expect to be one standard error above or below the line.\n95% of the time we expect to be two standard errors above or below the line.\nTo get the total error, we need to add\n\nThe error in the regression line (see above)\nThe error around the regression line.\n\n\n(Actually, we add these on the squared variance scale).\nThe picture below shows the individual prediction interval. Once again, you can pick your confidence level.\nSPSS calls this the individual prediction interval. R calls it the prediction interval.\n\n\n\n\n\n\nConfidence Level:\n\n50\n68\n90\n95\n99\n\n\n\n\n\n\n\n\n\n\n\nLook at the area in the graph which is colored cyan.\nThese are predictions that the car will stop in negative distance. Impossible!\nThe model is wrong.\nThat shouldn’t worry us, models are always wrong. The just might be close enough to be right to be useful.\nWe might say that the linear model is useful, but only if the car is going 5 mph or more."
  },
  {
    "objectID": "IntroStats/RegressionPredictionA.html#model-checking",
    "href": "IntroStats/RegressionPredictionA.html#model-checking",
    "title": "Regression Prediction Error",
    "section": "Model Checking",
    "text": "Model Checking\nNote that there was a slight curve in the lowess line in the scatterplot at the top of this analysis.\nSometimes the curve is easier to see if we take the linear trend out.\nWe can do this by plotting the residuals versus the fitted values.\n In a simple regression, this is the same as plotting against \\(X\\), as the fitted values are just a linear transformation of \\(X\\) (and the graph will just be rescaled to fit). For multiple regression, the fitted values are a mix of all the \\(X\\) values, so this plot is a useful summary.\n\n\n\n\n\n\n\n\n\nLooking a little more closely, we can see the curve.\nIt would be easy to miss without the lowess line, but the lowess line points it out to us.\nThere is a little bit of curvature, curving up at the lower distances, keeping the stopping distances in positive territory.\nSo what to conclude?\n\nIn the range of 5 mph – 25 mph the linear model looks pretty good.\nFor low speeds, we need a better model.\nMaybe we need a better model for higher speeds as well."
  },
  {
    "objectID": "IntroStats/RegressionPredictionA.html#try-a-transformation",
    "href": "IntroStats/RegressionPredictionA.html#try-a-transformation",
    "title": "Regression Prediction Error",
    "section": "Try a transformation",
    "text": "Try a transformation\n\ncars.sqfit &lt;- lm (sqrt(dist)~speed,data=cars)\nsummary(cars.sqfit)\n\n\nCall:\nlm(formula = sqrt(dist) ~ speed, data = cars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0684 -0.6983 -0.1799  0.5909  3.1534 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.27705    0.48444   2.636   0.0113 *  \nspeed        0.32241    0.02978  10.825 1.77e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.102 on 48 degrees of freedom\nMultiple R-squared:  0.7094,    Adjusted R-squared:  0.7034 \nF-statistic: 117.2 on 1 and 48 DF,  p-value: 1.773e-14\n\nplot(cars.sqfit,ask=FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\nggplot(cars,aes(x=speed,y=dist)) + geom_point() + geom_smooth(method=\"lm\") +\n  scale_y_sqrt()\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "IntroStats/VaccineCI.html",
    "href": "IntroStats/VaccineCI.html",
    "title": "Confidence Interval: COVID Vaccine tests",
    "section": "",
    "text": "Data from Moderna Vaccine Study Control Group\nX &lt;- 95\nN &lt;- 1500\nIn a sample of 1500 volunteers receiving the placebo, there were 95 positive cases; so our estimate for the rate of COVID-19 in this population (and this time period) is 0.063.\nRecall that the formula for the \\(\\alpha\\) confidence interval is\nC.I. \\[ \\bar X \\pm (z_{1-\\alpha/2})\\ \\sigma_{\\bar X}\\] Here \\(z_{1-\\alpha/2}\\) is the \\(1-\\alpha/2\\) quantile of the normal distribution. We can look that up on a Normal Table. For \\(\\alpha=.95\\), \\(z_{1-\\alpha/2}=1.96\\approx 2\\).\nFor binomial distribution \\[ \\bar X = X/N=\\hat p\\] This is the value 0.063 we calculated earlier. The hat over the \\(p\\) is a sign that it is a (maximum likelihood) estimate.\nThe usual formula for the standard error of a mean (from a simple random sample) is \\[ \\sigma_{\\bar X} = \\frac{\\sigma}{\\sqrt{N}} \\] For the binomial distribution, the standard deviation is \\[ \\sigma = \\sqrt{p(1-p)}; \\qquad s = \\sqrt{\\hat p(1-\\hat p)}\\] Plug that into the formula for the standard error and we get:\n\\[ \\sigma_{\\bar X} = \\sqrt{p(1-p)/N} \\] Lets go ahead and calculate those\np.hat &lt;- X/N\nse &lt;- sqrt(p.hat*(1-p.hat)/N)\nThe probability estimate is 0.063 and the standard error is 0.0063.\nI’ll now use an R trick. qnorm() is the R function to calculate the quantiles of the normal distribution. If I give it two probabilities, it will give me both the postive and negative values. So I will pass it \\((\\alpha/2,1-\\alpha/2)\\), this gives the values \\(r round(qnorm(c(.025,.975)),3)\\).\nBecause R does calculations on vectors, it will calculate both sides of the confidence interval with one formula.\nci &lt;- p.hat + qnorm(c(.025,.975))*se\nPrevlance of covid at the time and in the locations the study was run was between (5.1%,7.6%).\nNote that a lot of things have changed between now and then. In particular, the rise of the much more transmissable delta variant. But also changes in how seriously people take masking and other percautions. In particular, there is probably considerable regional variation in the prevalence of COVID-19.\nThe web site https://www.microcovid.org/ tracks this on a county-by-county basis."
  },
  {
    "objectID": "IntroStats/VaccineCI.html#severe-covid",
    "href": "IntroStats/VaccineCI.html#severe-covid",
    "title": "Confidence Interval: COVID Vaccine tests",
    "section": "Severe Covid",
    "text": "Severe Covid\nSame thing with the severe (hospitalizations or death) COVID numbers.\n\nX1 &lt;- 11\np1 &lt;- X1/N\nse1 &lt;- sqrt(p1*(1-p1)/N)\nci1 &lt;- p1 + qnorm(c(.025,.975))*se1\n\nPrevlance of severe covid at the time and in the locations the study was run was between (0.3%,1.2%)."
  },
  {
    "objectID": "IntroStats/LogNormalParams.html",
    "href": "IntroStats/LogNormalParams.html",
    "title": "Log-Normal Parameters",
    "section": "",
    "text": "A parameter is a value that can be changed in a statistical model. For example, the mean and standard deviation are the parameters of the normal distribution, which is a model for a population. Changing the value of a parameter, changes the model. We can see that in the illustration below. Try changing the values of the mean and standard deviation and see what happens to the shape of the curve."
  },
  {
    "objectID": "IntroStats/LogNormalParams.html#inputs-and-outputs",
    "href": "IntroStats/LogNormalParams.html#inputs-and-outputs",
    "title": "Log-Normal Parameters",
    "section": "Inputs and Outputs",
    "text": "Inputs and Outputs\n\n\n\n\n\n\nMean Log:\n\n\n\n\n\nStandard Deviation Log:"
  },
  {
    "objectID": "IntroStats/LogNormalParams.html#scale-and-location-parameters",
    "href": "IntroStats/LogNormalParams.html#scale-and-location-parameters",
    "title": "Log-Normal Parameters",
    "section": "Scale and Location Parameters",
    "text": "Scale and Location Parameters\nThe mean has a special role in the normal distribution; it determines where the center of the curve is. This makes it a location parameter.\nThe standard deviation has a special role in the normal distribution; it streches and shrinks the curve around the mean. This makes it a scale parameter.\nSometimes, the effects of scale and location parameters can be hard to see. This is because most statistical graphcis packages adjust the axis of the graph, so that the curve will always appear centered in the plotting window. In the normal curve above, I fixed the plotting window so that you can see the curve move. In the example below, I let the plotting window adjust with the curve. Notice how the curve stays the same, but the labels on the axis change.\n\n\n\n\n\n\nMean:\n\n\n\n\n\nStandard Deviation:"
  },
  {
    "objectID": "IntroStats/RareDisease.html",
    "href": "IntroStats/RareDisease.html",
    "title": "Rare Disease – COVID-19",
    "section": "",
    "text": "The rare disease problem is one of those “paradoxes” of statistics. The results are surprising because their are two sources of information: the prevalence of the disease in the population and the accuracy of the test. Often the former is stronger evidence than the latter, so people find it surprising.\nThe question of interest is: “What is the probability that a patient has the disease, given that the patient tests positive?” (This is sometimes called the True Positive probability; one minus would be called the False Positive probability.) A related question of interest is “What is the probability that a patient does not have the disease given that the patient tests negative?” (This is the False Negative probability.)\nStart by defining some variables. Let \\(D \\in \\{Y,N\\}\\) be whether or not a given individual has the disease, and let \\(T\\in\\{+,-\\}\\) be whether that individual gets a positive or negative result on the test. The joint probability of \\(D, T\\) is characterized by three numbers:\nLets try an example. A company called BioResponse just (March 19, 2020) launched the CoronaCheck test kit (Press Release). This article reports: “Our manufacturers report a sensitivity of 97.2% and specificity of 92%.”\nNow the hard part: estimating the base rate. This is hard because (a) people can have very mild symptoms for days and not know they have the disease, and (b) there has been a general shortage of test kits. So relying on official numbers is likely to give a big underestimate. As I’m not planning on updating this web site in real time, the numbers I’m putting in here will be out of date by the time you read this.\nAs I live in Florida, I’ll use the official Florida numbers: https://floridahealthcovid19.gov/#latest-stats . As of 2020-03-22 18:000, there were 1007 known cases in Florida, which has a population of 21,992,985. That gives a base rate of 4.5787327^{-5}. For the US, the number is 33,276 known cases, and a population of 330,464,151 for a base rate of 1.0069473^{-4}.\nUpdate: As of 2020-09-02, the state of Florida is reporting 624,116 cases, for a base rate of 0.028378. Note that the number of known cases is smaller than the total number of cases (especially, as we have learned that some people get very mild symptoms and may not know they are sick to seek testing).\n\\(Update^2\\): I have found a web site which gives background rates for SARS-COV-2 by state and county, so you can get local information. Hopefully, they are updating with the latest numbers. microCOVID Project.\nBase Rate (Pr(D=Y)):\n\n\n\n\n\nSensitivity (Pr(T=+|D=Y)):\n\n\n\n\n\nSpecificity (Pr(T=-|D=N)):"
  },
  {
    "objectID": "IntroStats/RareDisease.html#calculating-true-positive-and-false-positive.",
    "href": "IntroStats/RareDisease.html#calculating-true-positive-and-false-positive.",
    "title": "Rare Disease – COVID-19",
    "section": "Calculating true positive and false positive.",
    "text": "Calculating true positive and false positive.\nOne way to calculate this is to use Bayes’ theorem. However, from the table above, it is easy to calculate the true positive and false positive rates. We now just look at the columns of the table.\n\n\n\nFalse Positive Rate\nPr(D=N|T=+)\n\n\n\n\n\nFalse Negative Rate\nPr(D=Y|T=-)\n\n\n\n\nWhat is going on???\nThat false positive rate seems very high. What is really going on? The root cause is that as of this writing (Mar 22, 2020) COVID-19 is still pretty rare. So although getting a false positive is rare, actually having COVID-19 is much rarer. The following picture might help:\n\n\n\n\n\nThe thin bar on the left represents people who have COVID-19. There are still (fortunately) very few of them. The bar on top represents the false positives, fortunately, there are still a lot more of them than the true positives, so true positives are still rare. (May it always be so).\nOn the other hand, the false negative rate is very comforting. It means that if you test negative, you can be pretty sure that it is safe for you to be around other people (especially the old or sick)."
  },
  {
    "objectID": "IntroStats/RareDisease.html#sensitivity-analysis",
    "href": "IntroStats/RareDisease.html#sensitivity-analysis",
    "title": "Rare Disease – COVID-19",
    "section": "Sensitivity Analysis",
    "text": "Sensitivity Analysis\nDon’t forget that these base rates are underestimates. There is currently a shortage of tests, so these are only cases that have actually be able to be tested. Also, symptoms can take up to 3 days to appear, so some people who have it, probably don’t even know that they should ask to be tested. The actual infection rate could be 10 or more times as high as the known infection rate.\nAlso, there are various risk factors which should be added to the base rate. If the person being tested has traveled lately to an area with a higher rate, the base rate should go up. So too if the person has a fever or other symptoms of the virus.\nSo, play around with the base rate. Play with the sensitivity and specificity? How does this change? This will help you get a better feel for how the rare disease problem works.\nFinally, don’t forget that this thing grows exponentially fast (that is why it is a pandemic). This number could be go up very quickly. Here is an explanation.. ( As an aside, this is the kind of thing we would analyze on the log scale.)"
  },
  {
    "objectID": "IntroStats/RareDisease.html#how-would-this-test-be-used.",
    "href": "IntroStats/RareDisease.html#how-would-this-test-be-used.",
    "title": "Rare Disease – COVID-19",
    "section": "How would this test be used.",
    "text": "How would this test be used.\nActually, the most interesting thing about the CoronaCheck kit is that it only takes 15 minutes. This is great considering the older test takes 3 days. So assuming BioResponse can produce these quickly (or that other vendors come online with similar tests), these can be used for screening (say health care workers, or other first responders), as well as people presenting with other symptoms or having recently traveled.\nIf these people test positive on the quick screening test, they should be isolated and possibly a more sensitive (and probably time consuming) test be given. If they test negative, then they can be cleared to go about their normal activity. I’m sure this is how this test will be used.\nAnother factor is that doctors are simply not giving out tests unless there are other risk factors. I was in my doctors office for my daughter’s physical and talking to the nurse. She said that there was a woman who was tired (needs more sleep?) and congested (this is Tallahassee in March, the trees are raining pollen), but no fever. The nurse had to explain that she didn’t have enough test to give out unless there were more symptoms (particularly a fever). This will change as our testing capacity gets better (last I looked, Mar 20, the US was still doing only about 1/2 the number of test per capita as South Korea."
  },
  {
    "objectID": "IntroStats/RareDisease.html#dont-break-lockdownself-isolation",
    "href": "IntroStats/RareDisease.html#dont-break-lockdownself-isolation",
    "title": "Rare Disease – COVID-19",
    "section": "Don’t break lockdown/self-isolation",
    "text": "Don’t break lockdown/self-isolation\nDon’t panic, but do not be complacent either.\nSome of you reading this will be in official lockdown. Others will be under a self-distancing protocol. This is still extremely important as (1) the base rate will rise over time, probably quite quickly and (2) the disease takes up to 3 days to get started and the symptoms might appear like a common cold (Novel Coronavirus 19 is in fact an uncommon cold). You might have it and not know it yet. If you break the self-distancing protocol, you could be another Typhoid Mary spreading sickness and misery all around you.\nOh, and congrats to BioResponse on their breakthrough. I can’t judge the quality of the numbers from just a press release, but if they really can make that number of tests, that would be a big help. I hope lots of other biotech companies are working on this problem, too.\nStay healthy. Keep your distance. Wash your hands, and obey the local health authorities. Lets make sure we keep that base rate (i.e., the infection rate) low."
  },
  {
    "objectID": "IntroStats/CorrelationExercise.html",
    "href": "IntroStats/CorrelationExercise.html",
    "title": "Scatterplot examples",
    "section": "",
    "text": "This demonstration will use some random data. Lets start by generating the random data. So give a [random seed][seed] and pick a sample size for your sample.\n\n\n\n\n\n\nSample Size:\n\n25\n50\n100\n250\n500\n1000\n\n\n\n\n\n\nRandom number Seed (integer)"
  },
  {
    "objectID": "IntroStats/Independence.html",
    "href": "IntroStats/Independence.html",
    "title": "Independence",
    "section": "",
    "text": "Imagine a population which is split into two groups: \\(A\\) and \\(B\\). We select 100 people at random and ask them a question, which has two answers yes and no. Define the following quantities:\nDefine the following values (row and column totals):\nDividing any of those numbers by \\(N_{xx}\\) produces a corresponding proportion \\(P_{xx}\\) (which can be interpreted as a probability or proportion.\nSuppose group membership and the answer to the question are statistically indepedent. In the diagram below, adjust \\(P_{A+}\\) and \\(P_{+y}\\) to make a two-by-two table:\nP(Member of Group A)\n\n\n\n\n\nP(Answered `yes`)\nThere are two things you should notice about the independent data.\nWe could say that the row and column proportions are always the same.\nAnother way to think about this is to say: * If we learned which group a person belongs to, that would not change the probability of their answer. * If we learned how a person answered, that would not change the probablity of their group."
  },
  {
    "objectID": "IntroStats/Independence.html#dependent",
    "href": "IntroStats/Independence.html#dependent",
    "title": "Independence",
    "section": "Dependent",
    "text": "Dependent\nTo make the table dependence, we need to add another parameter to the model to specify the degree of dependence.\nFor a two-by-two table, the odds ratio is as fairly easy to understand choice: \\[ OR = \\frac{P_{Ay}/P_{An}}{P_{By}/P_{Bn}}\\] When group and answer are indpendent the cross product ratio should be 1.\nIf Group \\(A\\) is more likely to answer yes, then the ratio should be bigger than 1.\nIf Group \\(B\\) is more likely to answer yes, then the ratio should be less than one.\n\n\n\n\n\n\nP(Member of Group A)\n\n\n\n\n\nP(Answered `yes`)\n\n\n\n\n\nOdds Ratio\n\n1/4\n1/3\n1/2\n2/3\n1\n3/2\n2\n3\n4"
  },
  {
    "objectID": "IntroStats/KurtosisPractice.html",
    "href": "IntroStats/KurtosisPractice.html",
    "title": "Kurtosis Practice",
    "section": "",
    "text": "In this exercise, the computer will generate 3 datasets: X, Y and Z. These will be randomly assigned to high (leptokurtic), medium (mesokurtic) and low (platykurtic) distributions. A normal curve is drawn over the top for reference.\nYou can redraw from the same distributions by changing the sample size.\n\n\n\n\n\n\nSample Size:\n\n50\n100\n500\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentify the kurtosis of each distribution.\n\n\n\n\n\n\nX\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\nY\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\nZ\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo try again with different distributions, reload the page. If you are having trouble, try increasing the sample size: sometimes a small sample won’t display the characteristics of the distribution strongly.\nHere are the other exercises in this series:\n\nSkewness Practice:\n\nHistograms\nBoxplots\nQ-Q Plots\n\nKurtosis Practice:\n\nHistograms\nBoxplots\nQ-Q Plots"
  },
  {
    "objectID": "IntroStats/KurtosisPractice.html#kurtosis-determination-exercise.",
    "href": "IntroStats/KurtosisPractice.html#kurtosis-determination-exercise.",
    "title": "Kurtosis Practice",
    "section": "",
    "text": "In this exercise, the computer will generate 3 datasets: X, Y and Z. These will be randomly assigned to high (leptokurtic), medium (mesokurtic) and low (platykurtic) distributions. A normal curve is drawn over the top for reference.\nYou can redraw from the same distributions by changing the sample size.\n\n\n\n\n\n\nSample Size:\n\n50\n100\n500\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentify the kurtosis of each distribution.\n\n\n\n\n\n\nX\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\nY\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\nZ\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo try again with different distributions, reload the page. If you are having trouble, try increasing the sample size: sometimes a small sample won’t display the characteristics of the distribution strongly.\nHere are the other exercises in this series:\n\nSkewness Practice:\n\nHistograms\nBoxplots\nQ-Q Plots\n\nKurtosis Practice:\n\nHistograms\nBoxplots\nQ-Q Plots"
  },
  {
    "objectID": "IntroStats/CentralLimitTheroem.html",
    "href": "IntroStats/CentralLimitTheroem.html",
    "title": "Central Limit Theorem",
    "section": "",
    "text": "Pick a distribution: * Uniform – platykurtic * Binomial – symmetric and mesokurtic * Exponential – highly skewed * Gamma (shape = 3) – skewed * T (df =3) – high kurtosis\nSlide the sample size up and down, notice how the empirical distribution function and histogram coverge to the normal distribution function and density.\nDistribution Type\n\nUniform\nBinomial\nExponential\nGamma\nT\n\n\n\n\n\n\nNumber of Samples:\n\n\n\n\n\nSize of each sample:\nThe left column shows the original distribution. (I call that the black hat in my CLT demo.)\nThe right column shows the distribution of means of size \\(M\\) (adjusted with the second slider). (This is the white hat distribuiton, and \\(M\\) is the number of cards averaged to get the white hat value.)\nThe top row shows histograms with a normal curve on top.\nThe bottom row shows a QQ-plot. This shows how much the sample is different from a normal distribution. A normal distribution should be right on top of the diagonal line.\nAs \\(M\\) (the number of cards averages to get to the white hat) gets bigger, the distribution should get closer and closer to the normal distribution."
  },
  {
    "objectID": "IntroStats/CentralLimitTheroem.html#take-home",
    "href": "IntroStats/CentralLimitTheroem.html#take-home",
    "title": "Central Limit Theorem",
    "section": "Take home",
    "text": "Take home\n\nEven if the underlying data aren’t normal, the distribuiton of the means of various groups should be close to normal.\nClose depends on the sample size.\nA bigger sample is needed if the data are highly skewed (expontential and gamma) or leptokurtic (exponential and Student t)."
  },
  {
    "objectID": "IntroStats/StandardDeviations.html",
    "href": "IntroStats/StandardDeviations.html",
    "title": "Standard Deviations",
    "section": "",
    "text": "This R Markdown document is made interactive using Shiny. Unlike the more traditional workflow of creating static reports, you can now create documents that allow your readers to change the assumptions underlying your analysis and see the results immediately.\nTo learn more, see Interactive Documents."
  },
  {
    "objectID": "IntroStats/StandardDeviations.html#inputs-and-outputs",
    "href": "IntroStats/StandardDeviations.html#inputs-and-outputs",
    "title": "Standard Deviations",
    "section": "Inputs and Outputs",
    "text": "Inputs and Outputs\nYou can embed Shiny inputs and outputs in your document. Outputs are automatically updated whenever inputs change. This demonstrates how a standard R plot can be made interactive by wrapping it in the Shiny renderPlot function. The selectInput and sliderInput functions create the input widgets used to drive the plot.\n\ndata &lt;- rnorm(15,mean=0,sd=2)\nplot(data,1:length(data))\nabline(v=0)\nsegments(mean(data),1:length(data),data,1:length(data))\n\n\n\n\n\n\n\nsd(data)\n\n[1] 1.922204\n\nhist(data)\n\n\n\n\n\n\n\n\n\\[ \\sqrt(\\sum (X_i - \\mu)^2/N) \\]\n\ndata1 &lt;- c(11,13,14,15,17,18,19,20)\ndata2 &lt;- c(12,15,15,15,15,15,15,15)\ndata3 &lt;- c(5,13,19,24,33,38,51,70)\ndata4 &lt;- c(11,12,13,16,18,20,22,23)\npar(mfrow=c(2,2))\nplot(data1,1:length(data1),main=\"Data 1\",xlim=c(0,50))\nabline(v=mean(data1))\nsegments(mean(data1),1:length(data1),data1,1:length(data1))\nsd(data1)\n\n[1] 3.136764\n\nplot(data2,1:length(data2),main=\"Data 2\",xlim=c(0,50))\nabline(v=mean(data2))\nsegments(mean(data2),1:length(data2),data2,1:length(data2))\nsd(data2)\n\n[1] 1.06066\n\nplot(data3,1:length(data3),main=\"Data 3\",xlim=c(0,50))\nabline(v=mean(data3))\nsegments(mean(data3),1:length(data3),data3,1:length(data3))\nsd(data3)\n\n[1] 21.25987\n\nplot(data4,1:length(data4),main=\"Data 4\",xlim=c(0,50))\nabline(v=mean(data4))\nsegments(mean(data4),1:length(data4),data4,1:length(data4))\n\n\n\n\n\n\n\nsd(data4)\n\n[1] 4.611709\n\n\n\n\n\n\n\n\nNumber of bins:\n\n10\n20\n35\n50\n\n\n\n\n\n\nBandwidth adjustment:"
  },
  {
    "objectID": "IntroStats/StandardDeviations.html#embedded-application",
    "href": "IntroStats/StandardDeviations.html#embedded-application",
    "title": "Standard Deviations",
    "section": "Embedded Application",
    "text": "Embedded Application\nIt’s also possible to embed an entire Shiny application within an R Markdown document using the shinyAppDir function. This example embeds a Shiny application located in another directory:\n\n\n\n\n\nNote the use of the height parameter to determine how much vertical space the embedded application should occupy.\nYou can also use the shinyApp function to define an application inline rather then in an external directory.\nIn all of R code chunks above the echo = FALSE attribute is used. This is to prevent the R code within the chunk from rendering in the document alongside the Shiny components."
  },
  {
    "objectID": "IntroStats/Studenttcalculator.html",
    "href": "IntroStats/Studenttcalculator.html",
    "title": "Student’s Calculator",
    "section": "",
    "text": "In this tool, you input a \\(t\\) score and the degrees of freedom, and get a corresponding \\(p\\)-value.\n\n\n\n\n\n\nWhich tails\n\nUpper tail: Pr(t &lt; T)\nLower tail: Pr(T &lt; t)\nBoth tails: Pr(T &lt;-t or t&lt; T)\nMiddle: Pr(-t &lt; T &lt; t)\n\n\n\n\n\n\nt-value:\n\n\n\n\n\nDegrees of Freedom (n-1 or n1 + n2 -2)"
  },
  {
    "objectID": "IntroStats/Studenttcalculator.html#students-t-probabilities.",
    "href": "IntroStats/Studenttcalculator.html#students-t-probabilities.",
    "title": "Student’s Calculator",
    "section": "",
    "text": "In this tool, you input a \\(t\\) score and the degrees of freedom, and get a corresponding \\(p\\)-value.\n\n\n\n\n\n\nWhich tails\n\nUpper tail: Pr(t &lt; T)\nLower tail: Pr(T &lt; t)\nBoth tails: Pr(T &lt;-t or t&lt; T)\nMiddle: Pr(-t &lt; T &lt; t)\n\n\n\n\n\n\nt-value:\n\n\n\n\n\nDegrees of Freedom (n-1 or n1 + n2 -2)"
  },
  {
    "objectID": "IntroStats/Studenttcalculator.html#students-t-quantiles-critical-values.",
    "href": "IntroStats/Studenttcalculator.html#students-t-quantiles-critical-values.",
    "title": "Student’s Calculator",
    "section": "Student’s t Quantiles (Critical values).",
    "text": "Student’s t Quantiles (Critical values).\nIn this tool, you input a probability and degrees of freedom, and get a corresponding t score.\n\n\n\n\n\n\nWhich tails\n\nUpper tail: Pr(t &lt; T)\nLower tail: Pr(T &lt; t)\nBoth tails: Pr(T &lt;-t or t&lt; T)\nMiddle: Pr(-t &lt; T &lt; t)\n\n\n\n\n\n\nProbability of shaded region:\n\n\n\n\n\nDegrees of Freedom (n-1 or n1 + n2 -2)"
  },
  {
    "objectID": "IntroStats/BinomialParms.html",
    "href": "IntroStats/BinomialParms.html",
    "title": "Binomial Parameters",
    "section": "",
    "text": "The binomial distribution can be thought of as a number of draws, \\(n\\), from an urn with a proportion \\(p\\), of black balls.\nThe probability of drawing exactly \\(x\\) balls from an this urn is: \\[ p(X|n,p) = \\binom{n}{X} p^X (1-p)^{n-X}\\]\nThe expected value is \\(np\\), and the standard deviation is \\(\\sqrt{np(1-p)}\\).\nSometimes we write this in terms of the proportion of black balls in the sample. That is \\(p\\), with a standard deviation of \\(\\sqrt{p(1-p)/n}\\).\n\n\n\n\n\n\nNumber of draws:\n\n\n\n\n\nProbability of success:\n\n\n\n\n\n\n\n\n\n\nNote that this distribution is positively skewed if \\(p &lt; 0.5\\) and negatively skewed if \\(p &gt; 0.5\\).\nNote how when \\(n\\) gets large, the binomial distribution looks a lot like the normal. This is one of the first central limit theorems that was discovered. (The closer that \\(p\\) is to 0 or 1, the longer convergence to the normal takes.)"
  },
  {
    "objectID": "IntroStats/TestCI.html",
    "href": "IntroStats/TestCI.html",
    "title": "Confidence Intervals and Tests",
    "section": "",
    "text": "Suppose we are trying to find out the mean of a certain population, \\(\\mu\\). For example, suppose we are interested in the game eRebuild (https://mileresearch.coe.fsu.edu/erebuild) which aims to teach middle school students mathematics. Here \\(\\mu\\) would be how much a math a middle school student learns by playing the game. That is our target of inference.\nWe will make three simplifying assumptions:\n\nWe can measure “math”"
  },
  {
    "objectID": "IntroStats/Z-scores.html",
    "href": "IntroStats/Z-scores.html",
    "title": "Standardized Variables",
    "section": "",
    "text": "A (interval or ratio) variable on a raw score can be standardized to have mean 0 and standard deviation 1 by simply subtracting the mean and dividing by the standard deviation. This formula come in two flavors: one using the population mean and standard deviation (mu and sigma) and one using the sample statistics (x-bar and s). The subscripts are to remind you what variable you are using, as there is often both an X and Y wandering around.\n\\[ z = \\frac{x-\\mu_X}{\\sigma_X}; \\qquad Z = \\frac{X-\\bar X}{s_X} \\]\n\n\n\n\n\n\nMean of X:\n\n\n\n\n\nStandard Deviation of X:\n\n\n\n\n\nx:\n\n\n\n\n\n\n\n\n\n\nOften the next step is to look up the Z score on a normal calculator."
  },
  {
    "objectID": "IntroStats/Z-scores.html#standardizing-a-raw-score.",
    "href": "IntroStats/Z-scores.html#standardizing-a-raw-score.",
    "title": "Standardized Variables",
    "section": "",
    "text": "A (interval or ratio) variable on a raw score can be standardized to have mean 0 and standard deviation 1 by simply subtracting the mean and dividing by the standard deviation. This formula come in two flavors: one using the population mean and standard deviation (mu and sigma) and one using the sample statistics (x-bar and s). The subscripts are to remind you what variable you are using, as there is often both an X and Y wandering around.\n\\[ z = \\frac{x-\\mu_X}{\\sigma_X}; \\qquad Z = \\frac{X-\\bar X}{s_X} \\]\n\n\n\n\n\n\nMean of X:\n\n\n\n\n\nStandard Deviation of X:\n\n\n\n\n\nx:\n\n\n\n\n\n\n\n\n\n\nOften the next step is to look up the Z score on a normal calculator."
  },
  {
    "objectID": "IntroStats/Z-scores.html#going-from-a-standard-z-score-to-a-raw-score.",
    "href": "IntroStats/Z-scores.html#going-from-a-standard-z-score-to-a-raw-score.",
    "title": "Standardized Variables",
    "section": "Going from a standard (z) score to a raw score.",
    "text": "Going from a standard (z) score to a raw score.\nSolving the above equations for X allows the z-score to be translated back into a raw score. Often, a new variable is needed, so lets change the variables from X to Y. Once again, there are two variants based on whether sample or population means and standard deviations are used:\n\\[ y = \\sigma_Y z + \\mu_Y\\, ; \\qquad Y = s_Y Z + \\bar{Y}\\ .\\]\n\n\n\n\n\n\nMean of Y:\n\n\n\n\n\nStandard Deviation of Y:\n\n\n\n\n\nz:\n\n\n\n\n\n\n\n\n\n\n\n\nNote that these formulae are well worth memorizing, as they will come up over and over again."
  },
  {
    "objectID": "IntroStats/SkewnessPractice.html",
    "href": "IntroStats/SkewnessPractice.html",
    "title": "Skewness Practice",
    "section": "",
    "text": "In this exercise, the computer will generate 3 datasets: A, B and C. These will be randomly assigned to a positively skewed, negatively skewed, and symmetric distribution type. Each will be plotted with a normal curve on top for reference. Your job is to determine which is which.\nYou can redraw from the same distributions by changing the sample size.\n\n\n\n\n\n\nSample Size:\n\n50\n100\n500\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentify the skewness of each distribution.\n\n\n\n\n\n\nA\n\nUnknown\nNegatively Skewed\nPositively Skewed\nSymmetric\n\n\n\n\n\n\nB\n\nUnknown\nNegatively Skewed\nPositively Skewed\nSymmetric\n\n\n\n\n\n\nC\n\nUnknown\nNegatively Skewed\nPositively Skewed\nSymmetric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo try again with different distributions, reload the page. If you are having trouble, try increasing the sample size: sometimes a small sample won’t display the characteristics of the distribution strongly.\nHere are the other exercises in this series:\n\nSkewness Practice:\n\nHistograms\nBoxplots\nQ-Q Plots\n\nKurtosis Practice:\n\nHistograms\nBoxplots\nQ-Q Plots"
  },
  {
    "objectID": "IntroStats/SkewnessPractice.html#skewness-determination-exercise.",
    "href": "IntroStats/SkewnessPractice.html#skewness-determination-exercise.",
    "title": "Skewness Practice",
    "section": "",
    "text": "In this exercise, the computer will generate 3 datasets: A, B and C. These will be randomly assigned to a positively skewed, negatively skewed, and symmetric distribution type. Each will be plotted with a normal curve on top for reference. Your job is to determine which is which.\nYou can redraw from the same distributions by changing the sample size.\n\n\n\n\n\n\nSample Size:\n\n50\n100\n500\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentify the skewness of each distribution.\n\n\n\n\n\n\nA\n\nUnknown\nNegatively Skewed\nPositively Skewed\nSymmetric\n\n\n\n\n\n\nB\n\nUnknown\nNegatively Skewed\nPositively Skewed\nSymmetric\n\n\n\n\n\n\nC\n\nUnknown\nNegatively Skewed\nPositively Skewed\nSymmetric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo try again with different distributions, reload the page. If you are having trouble, try increasing the sample size: sometimes a small sample won’t display the characteristics of the distribution strongly.\nHere are the other exercises in this series:\n\nSkewness Practice:\n\nHistograms\nBoxplots\nQ-Q Plots\n\nKurtosis Practice:\n\nHistograms\nBoxplots\nQ-Q Plots"
  },
  {
    "objectID": "IntroStats/CorrelationOutliers.html",
    "href": "IntroStats/CorrelationOutliers.html",
    "title": "Correlation Coefficient",
    "section": "",
    "text": "This demonstration will use some random data. Lets start by generating the random data. So give a random seed and pick a sample size for your sample.\nSample Size:\n\n10\n25\n50\n100\n250\n500\n\n\n\n\n\n\nRandom number Seed (integer)"
  },
  {
    "objectID": "IntroStats/CorrelationOutliers.html#the-effect-of-outliers.",
    "href": "IntroStats/CorrelationOutliers.html#the-effect-of-outliers.",
    "title": "Correlation Coefficient",
    "section": "The effect of outliers.",
    "text": "The effect of outliers.\nThe data set below has \\(N\\) data points. The sliders are hooked up to the first one (which is plotted in red). The rest are generated from a normal distribution with mean 0 and standard deviation 1. They are uncorrelated, but there often will be a small correlation because of sampling variability.\n\n\n\n\n\n\nX-coordinate of point 1:\n\n\n\n\n\nY-coordinate of point 1:\n\n\n\n\n\n\n\n\n\n\n\nOutliers in Y\nSet the \\(X\\) value for the red point to zero. Now move the \\(Y\\) value up and down. How sensitive is the correlation to changes in the \\(Y\\) value with \\(X=\\bar X\\)?\n\n\nLeverage Points: Outliers in X\nNow set \\(X\\) to a high value (away from the mean at 0). Again move \\(Y\\) up and down, what happens to the line? Set \\(X\\) to a low value and try again.\nValues which are outliers in the \\(X\\) variable (or in the case of multiple regression, one or more of the \\(X\\) variables) are known as leverage points or influential points.\n\n\nSample Size\nNow try changing the sample size in the dialogues at the top of the page. Note that you will need to tweak one of the sliders for the graph to redraw at the new sample size. Is the correlation more or less sensitive at low sample sizes? At high sample sizes?\nAt the low sample size, set the first data point somewhere close to \\((0,0)\\). It should have little effect on the correlation. Now change the seed (and tweak the point). What happens to the correlation with a new sample? Try that again! Now try it with higher sample sizes.\n\n\nWhat to do about outliers\nThere are four reasons that there might be an outlier in a data set:\n\nSomething went wrong in the data entry. Somebody left our a decimal point, or hit an extra key on the keyboard. Or maybe a number got put in the wrong column, so the subject’s shoe size was entered in place of the subject’s IQ.\nSomething went wrong in the data collection. I had a friend who used to hook research subjects up to the Vitalog monitoring pack. It included a probe for body temperature. Sometimes the probe would read 72 degrees F (room temperature) instead of 98 degrees F (body temperature). They called this “probe slippage.”\nThere is a person in the sample who really doesn’t belong there. For example, the teacher took the test along with the students, and somehow the teacher’s answers were mixed in with th student answers.\nThere is a member of population that is just a little bit different. Maybe they belong to a rare subpopulation.\n\nIf the experimenters took good records, problems of Type 1 can be corrected (also, problems of Type 3 clearly identified). If not, it may still be possible to identify that the value is clearly out of range and needs to be eliminated (for example, an SAT score of 0, when the minimum SAT score is 200). The same thing is true for errors of Type 2. When the value is clearly out of range that is the best solution, although you need to be careful that the missingness is not related to what is being studied (for example, patients dropping out of a drug trial because of the side effects).\nIn the absence of good records Type 3 and Type 4 outliers are hard to distinguish. Often what we want to do is to take the data point out and run the analysis again. Then we can compare the two correlation coefficients. If the difference is small, no problem. If the difference is big, we can report that we have an influential point and let the reader come to his or her own conclusion."
  },
  {
    "objectID": "IntroStats/GammaParams.html",
    "href": "IntroStats/GammaParams.html",
    "title": "Normal Parameters",
    "section": "",
    "text": "The exponential distribution is a distribution often used for waiting times. Suppose the expected time to the next arrival is \\(\\theta\\). Then the probability that person will come at exactly time \\(x\\) is \\(f(x|\\theta) = \\frac{1}{\\theta}e^{-x/\\theta}\\). The exponential distribution has some interesting properties. In particular, if you have already waited for time period \\(z\\), then the conditional expectation is \\(z+\\theta\\).\nSuppose instead of waiting for one event, we wait for \\(k\\) events. Then we get the gamma distribution with shape parameter \\(k\\) and scale parameter \\(\\theta\\). Its probability density function is: \\[ f(x|k,\\theta) = \\frac{1}{\\Gamma(k)\\theta^k}x^{k-1}e^{-x/\\theta}\\]\nThe expected value is \\(k\\theta\\) and the standard deviation is \\(k\\theta^2\\).\n\n\n\n\n\n\nShape parameter\n\n\n\n\n\nScale parameter\n\n\n\n\n\n\n\n\n\n\nBe somewhat careful when using the gamma distribution in R. The gamma distribution is often parameterized using the rate parameter \\(\\beta=1/theta\\). If you are using the scale parameter, you need to name it explicitly and not rely on the position.\nIf the shape parameter is 1, then the gamma distribution is just the exponential distribution. It is extremely positively skewed. As the shape parameter increases, the gamma distribution becomes more and more symmetric, eventually converging to the normal distribution.\nThe chi-squared distribution is also a special case of the gamma distribution, with parameters \\(k=\\nu/2\\) and \\(\\theta=\\nu\\) (where \\(\\nu\\) is the degrees of freedom). Therefore, the gamma distribution is often used to model variances."
  },
  {
    "objectID": "IntroStats/SlopeStandardErrors.html",
    "href": "IntroStats/SlopeStandardErrors.html",
    "title": "Slope Variation",
    "section": "",
    "text": "n &lt;- 1000\nr &lt;- .6\nx &lt;- rnorm(n)\ny &lt;- r*x + sqrt(1-r^2)*rnorm(n)\ndat &lt;- data.frame(x,y)\n\nfit1 &lt;- lm(y~x,data=dat)\n\nplot(x,y,type=\"n\")\ncoef.sim &lt;- coef(sim(fit1))\nfor (i in 1:nrow(coef.sim)) {\n  abline(a=coef.sim[i,1],b=coef.sim[i,2],col=\"gray50\")\n}\npoints(x,y)\nabline(fit1)\n\n\n\n\n\n\n\nplot(coef.sim)\n\n\n\n\n\n\n\ncor(coef.sim)\n\n            (Intercept)         x\n(Intercept)   1.0000000 0.0706974\nx             0.0706974 1.0000000"
  },
  {
    "objectID": "Bayesian/BetaBinomiall.html",
    "href": "Bayesian/BetaBinomiall.html",
    "title": "Normal Normal Bayesian Model",
    "section": "",
    "text": "Prior alpha (psuedo-successes):\n\n\n\n\n\nPrior beta (psuedo-failures):\n\n\n\n\n\nNumber of Successes:\n\n\n\n\n\nNumber of Trials:\n\n\n\n\n\n\n\n\nPosterior Alpha =\n\n  Posterior Beta =",
    "crumbs": [
      "Normal Normal Bayesian Model"
    ]
  },
  {
    "objectID": "Bayesian/BetaBinomiall.html#parameterized-as-alpha-and-beta",
    "href": "Bayesian/BetaBinomiall.html#parameterized-as-alpha-and-beta",
    "title": "Normal Normal Bayesian Model",
    "section": "",
    "text": "Prior alpha (psuedo-successes):\n\n\n\n\n\nPrior beta (psuedo-failures):\n\n\n\n\n\nNumber of Successes:\n\n\n\n\n\nNumber of Trials:\n\n\n\n\n\n\n\n\nPosterior Alpha =\n\n  Posterior Beta =",
    "crumbs": [
      "Normal Normal Bayesian Model"
    ]
  },
  {
    "objectID": "Bayesian/BetaBinomiall.html#parameterized-as-mean-and-peseudo-count",
    "href": "Bayesian/BetaBinomiall.html#parameterized-as-mean-and-peseudo-count",
    "title": "Normal Normal Bayesian Model",
    "section": "Parameterized as mean and peseudo count",
    "text": "Parameterized as mean and peseudo count\n\n\n\n\n\n\nPrior successes rate:\n\n\n\n\n\nPrior weight (psuedo-count):\n\n\n\n\n\nNumber of Successes:\n\n\n\n\n\nNumber of Trials:\n\n\n\n\n\n\n\n\nPosterior Rate =\n\n  Posterior Pseudo-count  =",
    "crumbs": [
      "Normal Normal Bayesian Model"
    ]
  },
  {
    "objectID": "Bayesian/NormalNormal.html",
    "href": "Bayesian/NormalNormal.html",
    "title": "Normal Normal Bayesian Model",
    "section": "",
    "text": "Prior Mean:\n\n\n\n\n\nPrior Standard Deviation:\n\n\n\n\n\nSample mean:\n\n\n\n\n\nStandard error of Mean:\n\n\n\n\n\n\n\n\nPosterior Mean =\n\n  Posterior SD =",
    "crumbs": [
      "Normal Normal Bayesian Model"
    ]
  },
  {
    "objectID": "Bayesian/Schools8.html",
    "href": "Bayesian/Schools8.html",
    "title": "8 Schools EM",
    "section": "",
    "text": "This is the classic eight schools example from Rubin(1981)1. (It is also found in Chapter 5 of Gelman et al., 2014 2.) The story is that 8 different schools experimented with an SAT coaching experiment. The performance gains of the coached students were compared to students on a weight list control. Separate estimates were obtained for each school, but because the size of the schools differed, the standard errors differed as well.\nHere are the data:\n\nSchools &lt;- data.frame(row.names=c(\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\"),\n                      effect = c(28.39,7.94,-2.75,6.82,-.64,.63,18.01,12.16),\n                      see = c(14.9, 10.2, 16.3, 11.0, 9.4, 11.4, 10.4, 17.6))\n\nSchools\n\n  effect  see\nA  28.39 14.9\nB   7.94 10.2\nC  -2.75 16.3\nD   6.82 11.0\nE  -0.64  9.4\nF   0.63 11.4\nG  18.01 10.4\nH  12.16 17.6\n\n\nLets start by calculating a weighted average effect. I’ll weight each case by the precision (one over the square of the see).\n\nSchools$w &lt;- 1/Schools$see^2\nSchools.mean &lt;- sum(Schools$w*Schools$effect)/sum(Schools$w)\nSchools.mean\n\n[1] 7.870546\n\n\nHere is a plot of the data.\n\nord &lt;- order(Schools$effect)\nplot(Schools$effect[ord[c(1,8)]]+c(-2,2)*Schools$see[ord[c(1,8)]],\n     c(nrow(Schools),1),main = \"8 Schools data.\",type=\"n\",yaxt=\"n\",\n     xlab=\"Effect Size\",ylab=\"School\")\npoints(Schools$effect[ord],nrow(Schools):1,pch=rownames(Schools)[ord])\nsegments(Schools$effect[ord]-2*Schools$see[ord],nrow(Schools):1,\n         Schools$effect[ord]+2*Schools$see[ord],nrow(Schools):1)\nabline(v=Schools.mean,col=\"blue\")\n\n\n\n\n\n\n\n\n\n\nThe blue line in the figure above is the grand mean. This is the totally pooled solution. The letters are the unpooled estimates for the schools. Now suppose that the effects for the schools come from a normal population with mean equal to the grand mean above, and a standard deviation of \\(\\tau\\) (Gelman and Hill, 20073, calls this \\(\\sigma_\\alpha\\)). Let \\(\\bar y\\) be the grand mean, and let \\(\\sigma_i\\) be standard error for School \\(i\\). Then we get the following Bayesian posterior: \\[ \\widetilde\\sigma_i^{2} = 1/(\\tau^{-2} + \\sigma_i^{-2})\\;; \\qquad\n\\widetilde y_i = \\frac{\\tau^{-2} \\bar y + \\sigma_i^{-2} y_i}{\\tilde\\sigma_i^{-2}}\\;.\\]\nLets look at this for various values of \\(\\tau\\). For this purpose, the following function will be handy. It will compute the posterior distribution for a given prior over the school means.\n\nSchools.post &lt;- function (data, prior) {\n  V &lt;-  1/data$see^2 + 1/prior[\"sd\"]^2\n  theta &lt;- (data$effect/data$see^2 + prior[\"mean\"]/prior[\"sd\"]^2)/V\n  data.frame(effect=theta,see=1/sqrt(V),row.names=rownames(data))\n}\nSchools.post(Schools,c(mean=Schools.mean,sd=Inf))\n\n  effect  see\nA  28.39 14.9\nB   7.94 10.2\nC  -2.75 16.3\nD   6.82 11.0\nE  -0.64  9.4\nF   0.63 11.4\nG  18.01 10.4\nH  12.16 17.6\n\n\nWith infinite standard deviation in the prior, we recover the unpooled estimate. With zero standard we get NaN, but with a value close to 0 we get the completely pooled estimate.\n\n\n\nLets put a slider on \\(\\tau\\) so we can see going from the pooled to the unpooled estimate.\n\n\n\n\n\n\nSchool-level standard deviation:",
    "crumbs": [
      "8 Schools EM"
    ]
  },
  {
    "objectID": "Bayesian/Schools8.html#unpooled-pooled-and-partially-pooled-estimates.",
    "href": "Bayesian/Schools8.html#unpooled-pooled-and-partially-pooled-estimates.",
    "title": "8 Schools EM",
    "section": "",
    "text": "The blue line in the figure above is the grand mean. This is the totally pooled solution. The letters are the unpooled estimates for the schools. Now suppose that the effects for the schools come from a normal population with mean equal to the grand mean above, and a standard deviation of \\(\\tau\\) (Gelman and Hill, 20073, calls this \\(\\sigma_\\alpha\\)). Let \\(\\bar y\\) be the grand mean, and let \\(\\sigma_i\\) be standard error for School \\(i\\). Then we get the following Bayesian posterior: \\[ \\widetilde\\sigma_i^{2} = 1/(\\tau^{-2} + \\sigma_i^{-2})\\;; \\qquad\n\\widetilde y_i = \\frac{\\tau^{-2} \\bar y + \\sigma_i^{-2} y_i}{\\tilde\\sigma_i^{-2}}\\;.\\]\nLets look at this for various values of \\(\\tau\\). For this purpose, the following function will be handy. It will compute the posterior distribution for a given prior over the school means.\n\nSchools.post &lt;- function (data, prior) {\n  V &lt;-  1/data$see^2 + 1/prior[\"sd\"]^2\n  theta &lt;- (data$effect/data$see^2 + prior[\"mean\"]/prior[\"sd\"]^2)/V\n  data.frame(effect=theta,see=1/sqrt(V),row.names=rownames(data))\n}\nSchools.post(Schools,c(mean=Schools.mean,sd=Inf))\n\n  effect  see\nA  28.39 14.9\nB   7.94 10.2\nC  -2.75 16.3\nD   6.82 11.0\nE  -0.64  9.4\nF   0.63 11.4\nG  18.01 10.4\nH  12.16 17.6\n\n\nWith infinite standard deviation in the prior, we recover the unpooled estimate. With zero standard we get NaN, but with a value close to 0 we get the completely pooled estimate.",
    "crumbs": [
      "8 Schools EM"
    ]
  },
  {
    "objectID": "Bayesian/Schools8.html#senstivity-to-prior-standard-deviation",
    "href": "Bayesian/Schools8.html#senstivity-to-prior-standard-deviation",
    "title": "8 Schools EM",
    "section": "",
    "text": "Lets put a slider on \\(\\tau\\) so we can see going from the pooled to the unpooled estimate.\n\n\n\n\n\n\nSchool-level standard deviation:",
    "crumbs": [
      "8 Schools EM"
    ]
  },
  {
    "objectID": "Bayesian/Schools8.html#em-for-the-8-schools-problem.",
    "href": "Bayesian/Schools8.html#em-for-the-8-schools-problem.",
    "title": "8 Schools EM",
    "section": "EM for the 8 Schools problem.",
    "text": "EM for the 8 Schools problem.\nLet \\(Y_i\\) be the effect size for School \\(i\\), and let \\(\\sigma_i\\) be the standard error.\nLet the latent variables be \\(\\theta = \\{\\mu_1, \\ldots, \\mu_8\\}\\), the means for the 8 schools. The sufficient statistics are the expected value, \\(M_i\\) and the variances \\(V_i\\).\nThe parameters are mean, \\(\\xi\\), and the standard devaition, \\(\\tau\\), of the school mean distribution.\n\nE-Step\nWe will start with an arbitrary set of initial values: \\(\\xi=0\\) and \\(\\tau=\\infty\\).\n\n#Calculates posterior means for each of the schools\nparam0 &lt;- c(mean=0,sd=Inf)\n\nSchools.EStep &lt;- function (data, params) {\n  Pre &lt;-  1/data$see^2 + 1/params[\"sd\"]^2\n  M &lt;- (data$effect/data$see^2 + params[\"mean\"]/params[\"sd\"]^2)/Pre\n  list(M=M,V=1/Pre)\n}\n\nlv0 &lt;- Schools.EStep(Schools,param0)\nlv0\n\n$M\n[1] 28.39  7.94 -2.75  6.82 -0.64  0.63 18.01 12.16\n\n$V\n[1] 222.01 104.04 265.69 121.00  88.36 129.96 108.16 309.76\n\n\nNote that except for some changes of notation, Schools.Estep is the same as Schools.post above.",
    "crumbs": [
      "8 Schools EM"
    ]
  },
  {
    "objectID": "Bayesian/Schools8.html#m-step",
    "href": "Bayesian/Schools8.html#m-step",
    "title": "8 Schools EM",
    "section": "M-Step",
    "text": "M-Step\nWe are rather fortunate in that the maixmization can be done in closed form. In particular, the new value for \\(\\xi\\) is just the mean of \\(M_i\\). The variance is given by \\(\\tau^2 = \\textrm{Var}(M_i) + \\sum V_i\\). This gives us the following code.\n\nSchools.MStep &lt;- function (data, latentvars) {\n  J &lt;- length(latentvars$M)\n  mu &lt;- mean(latentvars$M)\n  sig &lt;- sum((latentvars$M-mu)^2)/(J-1)+sum(latentvars$V)\n  c(mean=mu,sd=sqrt(sig))\n}\n\nSchools.MStep(Schools,lv0)\n\n    mean       sd \n 8.82000 38.20371",
    "crumbs": [
      "8 Schools EM"
    ]
  },
  {
    "objectID": "Bayesian/Schools8.html#the-em-algorithm-1",
    "href": "Bayesian/Schools8.html#the-em-algorithm-1",
    "title": "8 Schools EM",
    "section": "The EM algorithm",
    "text": "The EM algorithm\nOK, Here is a general EM-algorithm function. Note that the E-step and M-step are passed in as functions. It also gives some options to print out the E-step and M-step results as we go.\n\nEM &lt;- function (data,initial.param,EStep,MStep,maxit=10L,tol=.001,\n                printParams=FALSE,printLatent=FALSE) {\n  param.hist &lt;- matrix(NA,maxit,length(initial.param))\n  lv.hist &lt;- vector(\"list\",maxit-1L)\n  dimnames(param.hist) &lt;- list(paste(1:maxit),names(initial.param))\n  param.hist[1,] &lt;- param &lt;- initial.param\n  if (printParams) cat(\"Initial Parameters\",param,\"\\n\")\n  converged &lt;- FALSE\n  \n  for (i in 2:maxit) {\n    if (printParams || printLatent) cat(\"Iteration \",i-1,\"\\n\")\n\n    latent &lt;- do.call(EStep,list(data,param))\n    lv.hist[[i-1]] &lt;- latent\n    if (printLatent) {\n          cat(\"Latent variables:\\n\")\n         print(latent)\n         cat(\"\\n\")\n    }\n    \n    param.hist[i,] &lt;- param &lt;- do.call(MStep,list(data,latent))\n    if (printParams) cat(\"Parameters\",param,\"\\n\")\n\n    if (max(abs(param-param.hist[i-1,])) &lt; tol) {\n      converged &lt;- TRUE\n      cat(\"Converged at iteration \",i,\"\\n\")\n      break\n    }\n  }\n  if (!converged) cat(\"Did not converge in\",i,\"iterations.\\n\")\n  list(param=param,latent=latent,param.hist=param.hist,lv.hist=lv.hist)\n}\n\nHere is an example of the EM algorithm in action:\n\nem.out &lt;- EM(Schools,param0,Schools.EStep,Schools.MStep,printParams=TRUE)\n\nInitial Parameters 0 Inf \nIteration  1 \nParameters 8.82 38.20371 \nIteration  2 \nParameters 8.745069 35.66051 \nIteration  3 \nParameters 8.727737 35.33237 \nIteration  4 \nParameters 8.724355 35.28564 \nIteration  5 \nParameters 8.723759 35.27889 \nIteration  6 \nParameters 8.72366 35.27792 \nConverged at iteration  7 \n\nem.out$param\n\n    mean       sd \n 8.72366 35.27792 \n\nem.out$latent$M\n\n[1] 25.41299752  8.00046259 -0.73153823  6.98868395 -0.01928998  1.39523743\n[7] 17.26751819 11.47521018",
    "crumbs": [
      "8 Schools EM"
    ]
  },
  {
    "objectID": "Bayesian/Schools8.html#visualizing-the-em-algorithm.",
    "href": "Bayesian/Schools8.html#visualizing-the-em-algorithm.",
    "title": "8 Schools EM",
    "section": "Visualizing the EM algorithm.",
    "text": "Visualizing the EM algorithm.\nLets see if we can look at the output of the EM algorithm graphically.\n\ninputPanel(\n  \n  sliderInput(\"mu0\", label=\"Starting School level mean\",\n              min=-25,max=25,value=0,step=.25),\n  sliderInput(\"tau0\", label = \"Starting School-level standard deviation:\",\n              min = 0.02, max = 100, value = 90, step = 0.02)\n)\n\n\n\n\n\nStarting School level mean\n\n\n\n\n\nStarting School-level standard deviation:\n\n\n\n\n\n\nrenderPlot({\n  \n  par0 &lt;- c(mean=input$mu0,sd=input$tau0)\n  emout &lt;- EM(Schools,par0,Schools.EStep,Schools.MStep)\n  NN &lt;- sum(!is.na(emout$param.hist[,1]))\n  Mmat &lt;- do.call(rbind,sapply(emout$lv.hist,function(x) x$M))\n  Mmat &lt;- rbind(Mmat,NA)\n  colnames(Mmat) &lt;- rownames(Schools)\n  layout(matrix(1:3,1,3),c(3,1,1))\n  matplot(Mmat,NN:1,type=\"b\",pch=colnames(Mmat),main=\"School Means\",\n          yaxt=\"n\",xlab=\"Effect Size\")\n  plot(emout$param.hist[1:NN,\"mean\"],NN:1,main=\"Grand Mean Effect\",type=\"b\",\n       yaxt=\"n\",xlab=\"Effect Size\")\n  plot(emout$param.hist[1:NN,\"sd\"],NN:1,main=\"Standard deviation of effect sizes\",type=\"b\",\n       yaxt=\"n\",xlab=\"sd(Effect Size)\")\n \n})",
    "crumbs": [
      "8 Schools EM"
    ]
  },
  {
    "objectID": "Bayesian/Schools8.html#em-in-practice.",
    "href": "Bayesian/Schools8.html#em-in-practice.",
    "title": "8 Schools EM",
    "section": "EM in practice.",
    "text": "EM in practice.\nIn practice, we seldom need to implement the EM algorithm by hand. Many of the easy cases are built into existing R functions, which often use the EM alogirthm to find maximum likelihood estimates under their hood. However, understanding how it works can help diagnose problems with convergence.\nAnother place that it is frequently seen is in the Marginal Maximum Likelihood (MML) algorithm used in Psychometrics. Here the person ability latent variable is like the unknown school means. The MML algorithm alternates between finding the parameters of the ability distribution (in some variants, just the mean and variance of the ability) and the values for the item parameters which maximize the likelihood.\n\nReferences",
    "crumbs": [
      "8 Schools EM"
    ]
  },
  {
    "objectID": "Bayesian/Schools8.html#footnotes",
    "href": "Bayesian/Schools8.html#footnotes",
    "title": "8 Schools EM",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRubin, D. B. (1981). Estimation in Parallel randomized experiments. Journal of Educational Statistics, 6, 377-401.↩︎\nGelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A. and Rubin, D. B. (2014). Bayesian Data Analysis: Third Edition. CRC Press. (ISBN: 978-1-4398-4095-5)↩︎\nGelman, A. and Hill, J. (2007). Data Analysis Using Regression and Hierarchical/Multilevel Models. Cambridge University Press.↩︎\nDempster, A. P. Laird, N. M. and Rubin, D. B. (1977). Maximum likelihood from incomplete data via the EM algorithm (with discussion). Journal of the Royal Statistical Society, 39, 1–38.↩︎",
    "crumbs": [
      "8 Schools EM"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Teaching Demos",
    "section": "",
    "text": "These are a series of small shiny apps I developed for teaching. Please feel free to use them in your courses or for your personal learning.\nAs learning from my R code is part of the learning, the source for these pages can be found on my github account: github:ralmond/TeachingDemos.\nThe following sets of demos are available.\n\nIntro Stats – Demos for an introductory statistics class, includes an online normal distribution calculator.\nBayesian Stats – Some Bayesian demonstrations including Beta-Binomial and Normal-Normal shiny apps and some sample stan code.\nR Intro – Some various talks I’ve given on how to use R (no narration).\n\nCopyright 2026 Russell G. Almond. Permission to reuse and remix granted under the basis of the CC-BY 4.0 license.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "RIntro/RNotebook.html",
    "href": "RIntro/RNotebook.html",
    "title": "R Notebook",
    "section": "",
    "text": "This is an R Markdown Notebook. When you execute code within the notebook, the results appear beneath the code.\nTry executing this chunk by clicking the Run button within the chunk or by placing your cursor inside it and pressing Cmd+Shift+Enter.\n\nplot(cars)\n\n\n\n\n\n\n\n\nAdd a new chunk by clicking the Insert Chunk button on the toolbar or by pressing Cmd+Option+I.\nWhen you save the notebook, an HTML file containing the code and output will be saved alongside it (click the Preview button or press Cmd+Shift+K to preview the HTML file).\nThe preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike Knit, Preview does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.\nLoading packages we need today\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.2.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggplot2)\nlibrary(psych)\n\n\nAttaching package: 'psych'\n\nThe following objects are masked from 'package:ggplot2':\n\n    %+%, alpha\n\n#If you don't already have these packages, please install first\nif (FALSE) {\n  install.packages(\"tidyverse\")\n  install.packages(\"ggplot2\")\n  install.packages(\"psych\")\n}\n\nOpen dataset\n\nd&lt;-as.data.frame(state.x77)\n#What does each variable measure?\n?state.x77\n\nREVIEW for mutate(): Make a new variable by combining two other variables, or transforming a single variable.\nPopulation density = Population/Area\n\nWithDen&lt;-as.data.frame(d%&gt;% mutate(Density = Population/Area))\n#OR\nd$Density&lt;-d$Population/d$Area\n\nAdd Comments\n\n## Covid Cases by State, 2020-04-03.  CDC website.\ncovid.x2020 &lt;- read.csv(\"../data/covidCases.csv\", header=FALSE)\n\nstat.covid &lt;- data.frame(state.x77,Covid=covid.x2020[[2]], region=state.region)\n\nCalculate summary statistics (e.g., Mean, Median, SD, Skewness & Kurtosis statistics) for a variable\n\nmean(d$Income)\n\n[1] 4435.8\n\nmedian(d$Income)\n\n[1] 4519\n\nsd(d$Income)\n\n[1] 614.4699\n\nskew(d$Income)\n\n[1] 0.2046903\n\nkurtosi(d$Income)\n\n[1] 0.2445701\n\ndescribe(d$Income)\n\n   vars  n   mean     sd median trimmed    mad  min  max range skew kurtosis\nX1    1 50 4435.8 614.47   4519 4430.08 581.18 3098 6315  3217  0.2     0.24\n     se\nX1 86.9\n\n\n\ndescribe(stat.covid$Covid)\n\n   vars  n    mean       sd median trimmed     mad min   max range skew\nX1    1 50 4763.66 13090.24   1408  2308.6 1647.91 147 90279 90132 5.64\n   kurtosis      se\nX1    33.56 1851.24\n\nstat.covid %&gt;% mutate(CovidDensity = Covid/Area) -&gt; covid1\npng(\"covidCases.png\")\nhist(covid1$CovidDensity, main=\"Covid Density by State 2020-04-03\",xlab=\"Covid cases per square mile\" )\ndev.off()\n\npng \n  2 \n\n\nCalculate summary statistics separately for groups of cases\n\nWealthy &lt;- d %&gt;% \n  filter(Income &gt; 4435.8)\nNotWealthy &lt;- d %&gt;% \n  filter(Income &lt; 4435.8)\ndescribe(NotWealthy)\n\n           vars  n     mean       sd   median  trimmed      mad     min\nPopulation    1 21  2907.95  2647.22  2341.00  2484.47  2172.01  472.00\nIncome        2 21  3855.19   326.41  3875.00  3873.82   372.13 3098.00\nIlliteracy    3 21     1.46     0.75     1.60     1.43     1.04    0.50\nLife Exp      4 21    70.28     1.35    70.39    70.29     1.53   67.96\nMurder        5 21     8.64     3.99     9.70     8.71     4.89    1.70\nHS Grad       6 21    48.33     8.81    47.40    47.75    10.82   37.80\nFrost         7 21    97.86    50.50    95.00    98.65    51.89   12.00\nArea          8 21 66375.00 55741.94 50708.00 56932.94 29337.69 9027.00\nDensity       9 21     0.06     0.03     0.05     0.06     0.05    0.01\n                 max     range  skew kurtosis       se\nPopulation  12237.00  11765.00  1.98     4.56   577.67\nIncome       4347.00   1249.00 -0.37    -0.69    71.23\nIlliteracy      2.80      2.30  0.07    -1.58     0.16\nLife Exp       72.90      4.94 -0.05    -0.97     0.29\nMurder         15.10     13.40 -0.17    -1.38     0.87\nHS Grad        67.30     29.50  0.42    -1.21     1.92\nFrost         174.00    162.00  0.04    -1.28    11.02\nArea       262134.00 253107.00  2.08     4.63 12163.89\nDensity         0.11      0.11 -0.10    -1.42     0.01\n\n\nMake a histogram, adjust the number of bins, and add a normal curve\n\nggplot(data = d) +\n  geom_histogram(mapping = aes(x = Income), binwidth = 100) \n\n\n\n\n\n\n\n#OR\n\nhist(d$Income,breaks = 30,main = \"Income\", freq=FALSE, col=\"gray\", xlab=\"Income\")\ncurve(dnorm(x, mean=mean(d$Income), sd=sd(d$Income)), add=TRUE, col=\"red\")\n\n\n\n\n\n\n\n\nNote that in the call to hist the option freq = FALSE was set so that the histogram would be a on the same density scale as the normal curve (area under all bars adds to 1, not sample size).\nPanel histograms by rows according to a grouping variable - Income in geographically bigger vs smaller state Creating a group variable based on area\n\nmean(d$Area)\n\n[1] 70735.88\n\nd$BigOrSmall&lt;-ifelse(d$Area&gt;=70735.08,\"Big\",\"Small\")\n  #Histogram of population based on geographically big vs small state\nggplot(data = d, mapping = aes(x = Population)) + \n  geom_freqpoly(mapping = aes(colour = BigOrSmall), binwidth = 100)\n\n\n\n\n\n\n\n\nMake boxplots to compare groups of variables - population in geographically bigger or smaller states\n\nggplot(data = d, mapping = aes(x = BigOrSmall, y = Population)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nRemove outliers or groups\n\nNoOutliers&lt;-d\nNoOutliers$Population[NoOutliers$Population &gt; 10000] &lt;-NA\n\nChecking the boxplots again\n\nggplot(data = NoOutliers, mapping = aes(x = BigOrSmall, y = Population)) +\n  geom_boxplot()\n\nWarning: Removed 6 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nSave your output to a picture file. (jpeg is also possible).\n\n#png file\npng(\"boxplot.png\", width = 350, height = 350) \nggplot(data = NoOutliers, mapping = aes(x = BigOrSmall, y = Population)) +\n  geom_boxplot()\n\nWarning: Removed 6 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\ndev.off()\n\npng \n  2 \n\n\n#OR if you would like a PDF export\n\npdf(\"boxplot.pdf\") \nggplot(data = NoOutliers, mapping = aes(x = BigOrSmall, y = Population)) +\n  geom_boxplot()\n\nWarning: Removed 6 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\ndev.off()\n\npng \n  2 \n\n\nWhere did I save this image to?\n\ngetwd()\n\n[1] \"/home/ralmond/ralmond1/Projects/TeachingDemos/RIntro\"\n\nlist.files()\n\n [1] \"boxplot.pdf\"                      \"boxplot.png\"                     \n [3] \"covidCases.png\"                   \"EDAwithGGPlot.qmd\"               \n [5] \"ErrorHandling.qmd\"                \"foo.html\"                        \n [7] \"foo.png\"                          \"index.qmd\"                       \n [9] \"InstallingR.qmd\"                  \"MatrixesAndDataFrames.qmd\"       \n[11] \"R and R Studio Presentation2.qmd\" \"RDataStructures.qmd\"             \n[13] \"RNotebook_files\"                  \"RNotebook.qmd\"                   \n[15] \"RNotebook.rmarkdown\"              \"TidyStrings.qmd\"                 \n[17] \"WorkingWithRData.qmd\"",
    "crumbs": [
      "R Notebook"
    ]
  },
  {
    "objectID": "RIntro/InstallingR.html",
    "href": "RIntro/InstallingR.html",
    "title": "Installing R",
    "section": "",
    "text": "When you finish this lesson, you will be able to 1) Start and Stop R and R Studio 2) Download, install and run the tidyverse package. 3) Get help on R functions.",
    "crumbs": [
      "Installing R"
    ]
  },
  {
    "objectID": "RIntro/InstallingR.html#objectives",
    "href": "RIntro/InstallingR.html#objectives",
    "title": "Installing R",
    "section": "",
    "text": "When you finish this lesson, you will be able to 1) Start and Stop R and R Studio 2) Download, install and run the tidyverse package. 3) Get help on R functions.",
    "crumbs": [
      "Installing R"
    ]
  },
  {
    "objectID": "RIntro/InstallingR.html#what-you-need-to-download",
    "href": "RIntro/InstallingR.html#what-you-need-to-download",
    "title": "Installing R",
    "section": "What you need to Download",
    "text": "What you need to Download\nR is a programming language for statistics. Generally, the way that you will work with R code is you will write scripts—small programs—that do the analysis you want to do. You will also need a development environment which will allow you to edit and run the scripts. I recommend RStudio, which is pretty easy to learn.\nIn general, you will need three things for an analysis job:\n\nR itself. R can be downloaded from https://cloud.r-project.org.\n\n\nIf you use a package manager on your computer, R is likely available there. The most common package managers are homebrew on Mac OS, apt-get on Debian Linux, yum on Red hat Linux, or chocolatey on Windows. You may need to search for ‘cran’ to find the name of the right package. For Debian Linux (including Ubuntu and Pop_OS), it is called r-base.\n\nR Studio development environment. R Studio https://rstudio.com/products/rstudio/download/. The free version is fine for what we are doing.\n\n\nThere are other choices for development environments. I use Emacs and ESS, Emacs Speaks Statistics, but that is mostly because I’ve been using Emacs for 20 years.\n\n\nA number of R packages for specific analyses. These can be downloaded from the Comprehensive R Archive Network, or CRAN. Go to https://cloud.r-project.org and click on the ‘Packages’ tab. We will cover package installation later.\n\nYou may want to bookmark the R-project.org web site, as it has lots of useful information, including links to documentation and of course the CRAN library of packages.\nGo ahead and download and install R and R Studio using the instructions on those pages.\n\nA brief Tour of R Studio\n\n\n\nRStudio with four regions labeled.\n\n\nWhen you open R Studio, the screen is split into four regions. (You can adjust the size of these regions if you like.)\n\nRegion 1 in the script editor. This is where you will do the bulk of your work.\nRegion 2 is the R command line. This is where R Studio communicates with the underlying R program.\nRegion 3 has a number of different purposes. Probably the most useful one early on is the “Environment” tab which has all of the R objects you have created.\nRegion 4 again as a number of the tabs. The most useful early on will be “Help” and “Plots”.\n\n\n\nThe Command Line\nAlthough Region 1 is where we will do most of our work, I’m going to start with Region 2. This the R console. R is an interactive programming language. It prompts you that it is waiting for a command with a &gt;. You can type a command at that prompt and hit return. R will then print the result of the expression. You can try this. Try typing 2+2 and then hit return. R should respond [1] 4.\n\nWhy the [1]. This is because R always works with vectors. This indicates that the answer is a vector and the first element is 4.\n\n\nR is a separate program from R Studio. The console window communicates between the two programs. In fact, if you open a terminal window (or command window on Windows) in your operating system, and type R, you will get a similar command prompt and can interact directly with R without R Studio.\n\n\n\nThe Script Editor\nRegion 1 contains an editor for R scripts. When I’m doing data analysis, I want to keep a record of all of the steps I took in doing the analysis. That is the script. An R script file is just a series of R commands, one R command per line. These are put in a text file (can be edited by many different programs) with an extension of .R (note the capital; important for case-sensitive file systems, like Linux).\nTo generate a new script file in R Studio, go to the file menu and select “New File … &gt; R Script”. This will open a new window in Region 1. I generally save it right away, so that I can give it a name that reflects my purpose.\nGenerally how I work in R is I build up a script for my analysis. In R Studio, I can put my cursor on the line I want run and press the Run button at the top of the script window. This will copy the line to the console and run it. If it didn’t work quite right (which often happens) I edit the line and try again. This way I don’t keep the mistakes around in my script, just the stuff that worked.\nSometimes I type things directly in the console. These are usually things I just want to try at the moment to see how they work, or maybe to get more information. For example, I might type names(cars) to get information about the variables in the data set cars or maybe help(var) to remind myself of how the command var works.\n\nI said that there was one command per line, but there are a couple of exceptions. First, you can put two commands per line if you separate them with a semicolon (;). Second, if R doesn’t think the command is complete on one line, it will look for the rest on the next line. I seldom use the semicolon to put two lines together, but I often need to split long lines when writing complex code.\n\n\nThe key to successfully splitting a line is letting R know that there is more to come. Consider the following example.\n\n1 +\n  2\n\n[1] 3\n\n\nPutting the plus sign at the end of the line tells R that there is more to come. So R interprets this as one expression 1+2. If I put the plus sign on the second line instead, R would interpret this as two expressions: 1 and +2.\n\n\nIf R thinks there is still more to come, it will prompt with a + instead of a &gt;. Try this. Type (1+2 and then return at the R command prompt. R will prompt you with + because the expression is not complete. Type ) to finish the expression. This is a fairly common mistake to make; if R is unexpectedly prompting you with its continuation prompt, it usually means you forgot a closing quotation mark or quote.\n\nYou can add comments to you R code by using the pound sign (or hash tag), #. When it sees the pound sign, R ignores everything up to the end of the line (unless the pound sign is in a string.)\n\nI use the following convention, which comes from Lisp programming. I use a single pound sign for a comment which comes after the code. This is usually one tab away from the end of the line. I use two pound signs for comments that are in the code. The are aligned with the start of the code line. I use three pound signs for big comments that describe a whole block of code. These are aligned flush left. Finally, I use a whole line of pound signs to separate different parts of a long script file.\n\n\n\nR Markdown\nR Studio introduces a new kind of script file that I find much more useful than the plain R script. An R Markdown (.Rmd) document can be created by selecting `New File … &gt; R Notebook ” from the “File” menu in R Studio.\nAn R Markdown document has three parts. The first part, separated by --- and --- is the YAML header (YAML=Yet Another Markup Language). This contains meta-data about the document, like title, author and date. It also contains instructions to Markdown about how you want to compile the document.\nThe rest of the document alternates between text chunks in the markdown language and code chunks in R. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For the most part, it looks like plain text, but there are some characters that have special meanings. For example, a line that starts with two pound signs starts a new section. If you select “Help &gt; Markdown Quick Reference” you will get a summary of all of the commands. One of the things I like about Markdown is that if you don’t know the markdown syntax, it pretty much looks like plain text, so just about everybody can read it.\nYou can embed an R code chunk like this:\n::: {.cell}\n\n```{.r .cell-code}\nsummary(cars)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00  \n```\n\n\n:::\n:::\nThe code chunk starts and ends with three backquotes. When editing this in R, R puts a little green triangle up in the top right corner of the chunk. Pressing that will run the chunk:\n\n\n\nGreen Triangle location\n\n\nPressing Control-Alt-I will insert a chunk into the document you are currently working on. This is a handy command to use.\n\nAfter the three backquotes to open the code chunk, there is a bit in curly braces. The first thing after the brace is the language used in the chunk. At first, you will almost always use r for R, but there are other languages that R Studio supports as well. Next is an optional name for the chunk; in the example above cars. Then follows a series of instructions to markdown separated by commas. For example, echo=FALSE will suppress printing the code. fig.cap=\"Figure caption.\" will add a caption to a figure.\n\nAnother advantage of using markdown is that it can compile the document into high quality papers or slide shows. When you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. For more details on using R Markdown see http://rmarkdown.rstudio.com.\n\nRmarkdown has been mostly superceded by a new system called Quarto, which uses .qmd files. Quarto is very similar to Rmarkdown, but has extra support for building more complex collections of documents like books and websites. The website https://quarto.org has documentation for Quarto.\n\n\n\nR Environment\nR wouldn’t be very useful if we had to type out data in every time. So R allows us to store data (and functions, and other things) in an environment. An environment maps names to values. To assign a value to a name, use the assignment, &lt;-, operator.\n\nsqrts &lt;- sqrt(1:10)\nGreeting &lt;- \"Holla!\"\nlong_silly_name &lt;- NA\nmean.1.10 &lt;- mean(1:10)\n\nNote that when you run these commands, the names appear in the “Environments” tab (by default in Region 3 on the R Studio window). You can click on the names to see the values. You can also use the names in later expressions instead of the values. The easiest way to see the value referred to by the name is to type the name in the console.\n\nsqrts\n\n [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427\n [9] 3.000000 3.162278\n\nGreeting\n\n[1] \"Holla!\"\n\nlong_silly_name\n\n[1] NA\n\nmean.1.10\n\n[1] 5.5\n\n\nNames in R need to follow certain rules. They need to start with a letter, and then they can’t contain any characters except letters, numbers, underscores _ and periods .. Note that capital and lower case letters are different in R, so Greeting and greeting are different variables.\nAn alternative to the &lt;- assignment operator is the equal sign, =. A single equals, = means assignment and a double equals == is a test if two values are equal. Be careful as these are easy to confuse. You need to use the single equals for assignment when you are calling functions. For example, the expression mean(x,na.rm=TRUE) asks R to calculate the mean of x, while setting the optional argument na.rm to TRUE.\n\nThe inside of a function is a different environment than the global environment. Variables inside a function can have local definitions that aren’t saved in the global environment. If R doesn’t find a variable in the local environment of a function, it will look in the global environment. The global environment is called .GlobalEnv.\n\nThe R function ls() will list all of the names in the current (usually the global) environment. It can be handy if you forgot how you abbreviated some variable name.\nWhen you quit R, it saves the global environment in a file called .RData, usually in your home directory. (Because the file name starts with a dot, it is usually invisible.) Actually, you don’t need to save the environment; R will ask if you want to save at the end of your session. I often don’t. I save all the commands I needed to create all of my variables in my script, and I’d often rather rerun the script to get a clean start.\n\n\nInstalling and Loading Packages\nThe base R distribution comes with somewhere around 2000 commands for analyzing data. You might think that this is enough, but actually one of the best parts of R is that you can easily extend it by writing new functions. These new functions can be bundled up into a package and shared with others. The most common place to share packages is in the “CRAN” archive on https://cloud.r-project.org. There were over 16,000 packages there the last time I visited.\nA lot of the packages are for doing very specialized analyses (e.g., working with spatial data, or sequencing DNA), but some are improvements to make R easier to use. I’m going to recommend one such bundle called tidyverse. Tidyverse is actually not a single package, but rather a meta-package which will load a number of useful packages.\nThe command install.packages() installs packages, that is, it downloads them from the CRAN library to your local computer. The command library() tells R that you want to use that package in this session. You need to run library() every time, but you only need to run install.packages() once.\n\nif (!(\"tidyverse\" %in% row.names(installed.packages()))) {\n  install.packages(\"tidyverse\",repos=\"https://cloud.r-project.org\",dependencies=TRUE)\n}\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.2.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nThere is also a menu function in R Studio for installing packages. Note that sometimes it will prompt you to choose a “mirror”. This is a local machine which has a copy of the archive. All mirrors work, but mirrors in your country (or at least continent) are likely to be faster.\n\nThe command above is a bit more complex than you strictly need. The first line of the script, starting with if checks to see if the package is already installed. (Installing takes a long time, don’t want to do it if we already have it.) The second line asks to install the tidyverse package. The bit saying repos=\"https://cloud.r-project.org\" specifies that R should use the special cloud.r-project.org mirror, which tries to find a local machine that is not too busy. The bit saying dependencies=TRUE indicates that R should install the other packages that tidyverse says it needs.\n\n\nEach package contains an environment which has all of its functions. When you issue the library() command, you add the package environments into your search path. You can see the search path by using the search() command.\n\nsearch()\n\n [1] \".GlobalEnv\"        \"package:lubridate\" \"package:forcats\"  \n [4] \"package:stringr\"   \"package:dplyr\"     \"package:purrr\"    \n [7] \"package:readr\"     \"package:tidyr\"     \"package:tibble\"   \n[10] \"package:ggplot2\"   \"package:tidyverse\" \"package:stats\"    \n[13] \"package:graphics\"  \"package:grDevices\" \"package:utils\"    \n[16] \"package:datasets\"  \"package:methods\"   \"Autoloads\"        \n[19] \"package:base\"     \n\n\nThe list always starts with .GlobalEnv. Most of the things up to package:tidyverse were added by loading tidyverse. The package tools:rstudio is a special package for communicating between R Studio and R. The packages stats, graphics, utils and base are part of the default R distribution.\n\n\n\nGetting Help\nWith several thousand R functions, and over 16,000 packages packages adding more functions, there is no way anybody can remember them all. Add to this that most R functions have optional arguments which change the way the function behaves (e.g., telling mean() to ignore the missing values with the na.rm=TRUE option). There is no way that even the members of the R Core Team can remember them all.\nFortunately, R provides really good help. Every function in base R and almost all of the functions in the packages have a manual page. You can access the manual page in one of two ways. You can use the function help() or the shortcut ?. Try this:\n\n?var\n\nThe manual page will pop up in the 4th region of R Studio. The top of the manual page gives all of the arguments and explanations of what they do. The bottom gives examples of how the function works. You can copy and paste these examples into the console to see how the function works.\nAnother way to access help in R Studio is to search (by the magnifying glass) in the Help tab. Sometimes this will give you a list of commands you can choose from.\nYet another way to search for help is to use the help.search() function. Its argument is a string that you might find in the short description.\n\nhelp.search(\"variance\")\n\nThis will give you a long list. The function names will look like package::function, so you can tell which package the function comes from.\nYet another good trick is to ask Dr. Google (or you favorite search engine). Often you can find bits of other people’s R code which do more or less what you want.\nFinally, if you want to find out what functions are in a package, try library(help=package)\n\nlibrary(help=tibble)\n\nThis pops up a window telling you about help for the package. Note that all R packages have manuals that can be downloaded from the CRAN library, and many have vignettes—examples of how they can be used.\n\n\nMaking Plots\nIf you are using script commands, or you are executing the command in the console window, the commands appear in the Plots tab in Region 4 of R Studio. Try typing hist(faithful$eruptions) in the console window.\nIt might look something like this:\n\n\n\nRStudio with Plot\n\n\nThere are a number of useful buttons up at the top. If you have made a bunch of plots, the left and right arrows allow you to cycle through them. The Zoom button pops up a window with the plot. The Export menu allows you to save the file as a picture or copy it to the pasteboard to put in another document.\nWhen you are using R markdown (i.e., an R Notebook) the plots get also embedded in the document. For example, try pressing the green triangle after this chunk.\n\n\n\n\n\n\n\n\n\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot.\nNote that here you can use the fig.cap option to add a caption to the figure.\n\n\n\n\n\nVapor Pressure of Mercury as a Function of Temperature\n\n\n\n\nThe caption doesn’t show in the notebook mode, but it will if you use knit to make a document.\n\n\nCiting R\nIf you find R useful, you should give credit to the developers. To find out how to properly cite R use the citation command:\n\ncitation()\n\nTo cite R in publications use:\n\n  R Core Team (2025). _R: A Language and Environment for Statistical\n  Computing_. R Foundation for Statistical Computing, Vienna, Austria.\n  &lt;https://www.R-project.org/&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2025},\n    url = {https://www.R-project.org/},\n  }\n\nWe have invested a lot of time and effort in creating R, please cite it\nwhen using it for data analysis. See also 'citation(\"pkgname\")' for\nciting R packages.\n\n\nTo cite a package, check citation(\"packagename\").\n\ncitation(\"tidyverse\")\n\nTo cite package 'tidyverse' in publications use:\n\n  Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R,\n  Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller\n  E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V,\n  Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). \"Welcome to\n  the tidyverse.\" _Journal of Open Source Software_, *4*(43), 1686.\n  doi:10.21105/joss.01686 &lt;https://doi.org/10.21105/joss.01686&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Article{,\n    title = {Welcome to the {tidyverse}},\n    author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},\n    year = {2019},\n    journal = {Journal of Open Source Software},\n    volume = {4},\n    number = {43},\n    pages = {1686},\n    doi = {10.21105/joss.01686},\n  }\n\n\nFinally, R and R Studio are two separate programs, so deserve a separate citations. Use the help menu (“Help &gt; RStudio Docs”) for more information.\n\n\nNext Lesson\nCongrats. You should now know how to install and launch R and RStudio. The next step is learn to work with R data.\nNext Lesson",
    "crumbs": [
      "Installing R"
    ]
  },
  {
    "objectID": "RIntro/InstallingR.html#a-brief-tour-of-r-studio",
    "href": "RIntro/InstallingR.html#a-brief-tour-of-r-studio",
    "title": "Installing R",
    "section": "A brief Tour of R Studio",
    "text": "A brief Tour of R Studio\n\n\n\nRStudio with four regions labeled.\n\n\nWhen you open R Studio, the screen is split into four regions. (You can adjust the size of these regions if you like.)\n\nRegion 1 in the script editor. This is where you will do the bulk of your work.\nRegion 2 is the R command line. This is where R Studio communicates with the underlying R program.\nRegion 3 has a number of different purposes. Probably the most useful one early on is the “Environment” tab which has all of the R objects you have created.\nRegion 4 again as a number of the tabs. The most useful early on will be “Help” and “Plots”.",
    "crumbs": [
      "Installing R"
    ]
  },
  {
    "objectID": "RIntro/InstallingR.html#the-command-line",
    "href": "RIntro/InstallingR.html#the-command-line",
    "title": "Installing R",
    "section": "The Command Line",
    "text": "The Command Line\nAlthough Region 1 is where we will do most of our work, I’m going to start with Region 2. This the R console. R is an interactive programming language. It prompts you that it is waiting for a command with a &gt;. You can type a command at that prompt and hit return. R will then print the result of the expression. You can try this. Try typing 2+2 and then hit return. R should respond [1] 4.\n\nWhy the [1]. This is because R always works with vectors. This indicates that the answer is a vector and the first element is 4.\n\n\nR is a separate program from R Studio. The console window communicates between the two programs. In fact, if you open a terminal window (or command window on Windows) in your operating system, and type R, you will get a similar command prompt and can interact directly with R without R Studio.",
    "crumbs": [
      "Installing R"
    ]
  },
  {
    "objectID": "RIntro/InstallingR.html#the-script-editor",
    "href": "RIntro/InstallingR.html#the-script-editor",
    "title": "Installing R",
    "section": "The Script Editor",
    "text": "The Script Editor\nRegion 1 contains an editor for R scripts. When I’m doing data analysis, I want to keep a record of all of the steps I took in doing the analysis. That is the script. An R script file is just a series of R commands, one R command per line. These are put in a text file (can be edited by many different programs) with an extension of .R (note the capital; important for case-sensitive file systems, like Linux).\nTo generate a new script file in R Studio, go to the file menu and select “New File … &gt; R Script”. This will open a new window in Region 1. I generally save it right away, so that I can give it a name that reflects my purpose.\nGenerally how I work in R is I build up a script for my analysis. In R Studio, I can put my cursor on the line I want run and press the Run button at the top of the script window. This will copy the line to the console and run it. If it didn’t work quite right (which often happens) I edit the line and try again. This way I don’t keep the mistakes around in my script, just the stuff that worked.\nSometimes I type things directly in the console. These are usually things I just want to try at the moment to see how they work, or maybe to get more information. For example, I might type names(cars) to get information about the variables in the data set cars or maybe help(var) to remind myself of how the command var works.\n\nI said that there was one command per line, but there are a couple of exceptions. First, you can put two commands per line if you separate them with a semicolon (;). Second, if R doesn’t think the command is complete on one line, it will look for the rest on the next line. I seldom use the semicolon to put two lines together, but I often need to split long lines when writing complex code.\n\n\nThe key to successfully splitting a line is letting R know that there is more to come. Consider the following example.\n\n1 +\n  2\n\n[1] 3\n\n\nPutting the plus sign at the end of the line tells R that there is more to come. So R interprets this as one expression 1+2. If I put the plus sign on the second line instead, R would interpret this as two expressions: 1 and +2.\n\n\nIf R thinks there is still more to come, it will prompt with a + instead of a &gt;. Try this. Type (1+2 and then return at the R command prompt. R will prompt you with + because the expression is not complete. Type ) to finish the expression. This is a fairly common mistake to make; if R is unexpectedly prompting you with its continuation prompt, it usually means you forgot a closing quotation mark or quote.\n\nYou can add comments to you R code by using the pound sign (or hash tag), #. When it sees the pound sign, R ignores everything up to the end of the line (unless the pound sign is in a string.)\n\nI use the following convention, which comes from Lisp programming. I use a single pound sign for a comment which comes after the code. This is usually one tab away from the end of the line. I use two pound signs for comments that are in the code. The are aligned with the start of the code line. I use three pound signs for big comments that describe a whole block of code. These are aligned flush left. Finally, I use a whole line of pound signs to separate different parts of a long script file.",
    "crumbs": [
      "Installing R"
    ]
  },
  {
    "objectID": "RIntro/InstallingR.html#r-markdown",
    "href": "RIntro/InstallingR.html#r-markdown",
    "title": "Installing R",
    "section": "R Markdown",
    "text": "R Markdown\nR Studio introduces a new kind of script file that I find much more useful than the plain R script. An R Markdown (.Rmd) document can be created by selecting `New File … &gt; R Notebook ” from the “File” menu in R Studio.\nAn R Markdown document has three parts. The first part, separated by --- and --- is the YAML header (YAML=Yet Another Markup Language). This contains meta-data about the document, like title, author and date. It also contains instructions to Markdown about how you want to compile the document.\nThe rest of the document alternates between text chunks in the markdown language and code chunks in R. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For the most part, it looks like plain text, but there are some characters that have special meanings. For example, a line that starts with two pound signs starts a new section. If you select “Help &gt; Markdown Quick Reference” you will get a summary of all of the commands. One of the things I like about Markdown is that if you don’t know the markdown syntax, it pretty much looks like plain text, so just about everybody can read it.\nYou can embed an R code chunk like this:\n::: {.cell}\n\n```{.r .cell-code}\nsummary(cars)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00  \n```\n\n\n:::\n:::\nThe code chunk starts and ends with three backquotes. When editing this in R, R puts a little green triangle up in the top right corner of the chunk. Pressing that will run the chunk:\n\n\n\nGreen Triangle location\n\n\nPressing Control-Alt-I will insert a chunk into the document you are currently working on. This is a handy command to use.\n\nAfter the three backquotes to open the code chunk, there is a bit in curly braces. The first thing after the brace is the language used in the chunk. At first, you will almost always use r for R, but there are other languages that R Studio supports as well. Next is an optional name for the chunk; in the example above cars. Then follows a series of instructions to markdown separated by commas. For example, echo=FALSE will suppress printing the code. fig.cap=\"Figure caption.\" will add a caption to a figure.\n\nAnother advantage of using markdown is that it can compile the document into high quality papers or slide shows. When you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. For more details on using R Markdown see http://rmarkdown.rstudio.com.\n\nRmarkdown has been mostly superceded by a new system called Quarto, which uses .qmd files. Quarto is very similar to Rmarkdown, but has extra support for building more complex collections of documents like books and websites. The website https://quarto.org has documentation for Quarto.",
    "crumbs": [
      "Installing R"
    ]
  },
  {
    "objectID": "RIntro/InstallingR.html#r-environment",
    "href": "RIntro/InstallingR.html#r-environment",
    "title": "Installing R",
    "section": "R Environment",
    "text": "R Environment\nR wouldn’t be very useful if we had to type out data in every time. So R allows us to store data (and functions, and other things) in an environment. An environment maps names to values. To assign a value to a name, use the assignment, &lt;-, operator.\n\nsqrts &lt;- sqrt(1:10)\nGreeting &lt;- \"Holla!\"\nlong_silly_name &lt;- NA\nmean.1.10 &lt;- mean(1:10)\n\nNote that when you run these commands, the names appear in the “Environments” tab (by default in Region 3 on the R Studio window). You can click on the names to see the values. You can also use the names in later expressions instead of the values. The easiest way to see the value referred to by the name is to type the name in the console.\n\nsqrts\n\n [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427\n [9] 3.000000 3.162278\n\nGreeting\n\n[1] \"Holla!\"\n\nlong_silly_name\n\n[1] NA\n\nmean.1.10\n\n[1] 5.5\n\n\nNames in R need to follow certain rules. They need to start with a letter, and then they can’t contain any characters except letters, numbers, underscores _ and periods .. Note that capital and lower case letters are different in R, so Greeting and greeting are different variables.\nAn alternative to the &lt;- assignment operator is the equal sign, =. A single equals, = means assignment and a double equals == is a test if two values are equal. Be careful as these are easy to confuse. You need to use the single equals for assignment when you are calling functions. For example, the expression mean(x,na.rm=TRUE) asks R to calculate the mean of x, while setting the optional argument na.rm to TRUE.\n\nThe inside of a function is a different environment than the global environment. Variables inside a function can have local definitions that aren’t saved in the global environment. If R doesn’t find a variable in the local environment of a function, it will look in the global environment. The global environment is called .GlobalEnv.\n\nThe R function ls() will list all of the names in the current (usually the global) environment. It can be handy if you forgot how you abbreviated some variable name.\nWhen you quit R, it saves the global environment in a file called .RData, usually in your home directory. (Because the file name starts with a dot, it is usually invisible.) Actually, you don’t need to save the environment; R will ask if you want to save at the end of your session. I often don’t. I save all the commands I needed to create all of my variables in my script, and I’d often rather rerun the script to get a clean start.",
    "crumbs": [
      "Installing R"
    ]
  },
  {
    "objectID": "RIntro/InstallingR.html#installing-and-loading-packages",
    "href": "RIntro/InstallingR.html#installing-and-loading-packages",
    "title": "Installing R",
    "section": "Installing and Loading Packages",
    "text": "Installing and Loading Packages\nThe base R distribution comes with somewhere around 2000 commands for analyzing data. You might think that this is enough, but actually one of the best parts of R is that you can easily extend it by writing new functions. These new functions can be bundled up into a package and shared with others. The most common place to share packages is in the “CRAN” archive on https://cloud.r-project.org. There were over 16,000 packages there the last time I visited.\nA lot of the packages are for doing very specialized analyses (e.g., working with spatial data, or sequencing DNA), but some are improvements to make R easier to use. I’m going to recommend one such bundle called tidyverse. Tidyverse is actually not a single package, but rather a meta-package which will load a number of useful packages.\nThe command install.packages() installs packages, that is, it downloads them from the CRAN library to your local computer. The command library() tells R that you want to use that package in this session. You need to run library() every time, but you only need to run install.packages() once.\n\nif (!(\"tidyverse\" %in% row.names(installed.packages()))) {\n  install.packages(\"tidyverse\",repos=\"https://cloud.r-project.org\",dependencies=TRUE)\n}\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.2.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nThere is also a menu function in R Studio for installing packages. Note that sometimes it will prompt you to choose a “mirror”. This is a local machine which has a copy of the archive. All mirrors work, but mirrors in your country (or at least continent) are likely to be faster.\n\nThe command above is a bit more complex than you strictly need. The first line of the script, starting with if checks to see if the package is already installed. (Installing takes a long time, don’t want to do it if we already have it.) The second line asks to install the tidyverse package. The bit saying repos=\"https://cloud.r-project.org\" specifies that R should use the special cloud.r-project.org mirror, which tries to find a local machine that is not too busy. The bit saying dependencies=TRUE indicates that R should install the other packages that tidyverse says it needs.\n\n\nEach package contains an environment which has all of its functions. When you issue the library() command, you add the package environments into your search path. You can see the search path by using the search() command.\n\nsearch()\n\n [1] \".GlobalEnv\"        \"package:lubridate\" \"package:forcats\"  \n [4] \"package:stringr\"   \"package:dplyr\"     \"package:purrr\"    \n [7] \"package:readr\"     \"package:tidyr\"     \"package:tibble\"   \n[10] \"package:ggplot2\"   \"package:tidyverse\" \"package:stats\"    \n[13] \"package:graphics\"  \"package:grDevices\" \"package:utils\"    \n[16] \"package:datasets\"  \"package:methods\"   \"Autoloads\"        \n[19] \"package:base\"     \n\n\nThe list always starts with .GlobalEnv. Most of the things up to package:tidyverse were added by loading tidyverse. The package tools:rstudio is a special package for communicating between R Studio and R. The packages stats, graphics, utils and base are part of the default R distribution.",
    "crumbs": [
      "Installing R"
    ]
  },
  {
    "objectID": "RIntro/InstallingR.html#getting-help",
    "href": "RIntro/InstallingR.html#getting-help",
    "title": "Installing R",
    "section": "Getting Help",
    "text": "Getting Help\nWith several thousand R functions, and over 16,000 packages packages adding more functions, there is no way anybody can remember them all. Add to this that most R functions have optional arguments which change the way the function behaves (e.g., telling mean() to ignore the missing values with the na.rm=TRUE option). There is no way that even the members of the R Core Team can remember them all.\nFortunately, R provides really good help. Every function in base R and almost all of the functions in the packages have a manual page. You can access the manual page in one of two ways. You can use the function help() or the shortcut ?. Try this:\n\n?var\n\nThe manual page will pop up in the 4th region of R Studio. The top of the manual page gives all of the arguments and explanations of what they do. The bottom gives examples of how the function works. You can copy and paste these examples into the console to see how the function works.\nAnother way to access help in R Studio is to search (by the magnifying glass) in the Help tab. Sometimes this will give you a list of commands you can choose from.\nYet another way to search for help is to use the help.search() function. Its argument is a string that you might find in the short description.\n\nhelp.search(\"variance\")\n\nThis will give you a long list. The function names will look like package::function, so you can tell which package the function comes from.\nYet another good trick is to ask Dr. Google (or you favorite search engine). Often you can find bits of other people’s R code which do more or less what you want.\nFinally, if you want to find out what functions are in a package, try library(help=package)\n\nlibrary(help=tibble)\n\nThis pops up a window telling you about help for the package. Note that all R packages have manuals that can be downloaded from the CRAN library, and many have vignettes—examples of how they can be used.",
    "crumbs": [
      "Installing R"
    ]
  },
  {
    "objectID": "RIntro/InstallingR.html#making-plots",
    "href": "RIntro/InstallingR.html#making-plots",
    "title": "Installing R",
    "section": "Making Plots",
    "text": "Making Plots\nIf you are using script commands, or you are executing the command in the console window, the commands appear in the Plots tab in Region 4 of R Studio. Try typing hist(faithful$eruptions) in the console window.\nIt might look something like this:\n\n\n\nRStudio with Plot\n\n\nThere are a number of useful buttons up at the top. If you have made a bunch of plots, the left and right arrows allow you to cycle through them. The Zoom button pops up a window with the plot. The Export menu allows you to save the file as a picture or copy it to the pasteboard to put in another document.\nWhen you are using R markdown (i.e., an R Notebook) the plots get also embedded in the document. For example, try pressing the green triangle after this chunk.\n\n\n\n\n\n\n\n\n\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot.\nNote that here you can use the fig.cap option to add a caption to the figure.\n\n\n\n\n\nVapor Pressure of Mercury as a Function of Temperature\n\n\n\n\nThe caption doesn’t show in the notebook mode, but it will if you use knit to make a document.",
    "crumbs": [
      "Installing R"
    ]
  },
  {
    "objectID": "RIntro/InstallingR.html#citing-r",
    "href": "RIntro/InstallingR.html#citing-r",
    "title": "Installing R",
    "section": "Citing R",
    "text": "Citing R\nIf you find R useful, you should give credit to the developers. To find out how to properly cite R use the citation command:\n\ncitation()\n\nTo cite R in publications use:\n\n  R Core Team (2025). _R: A Language and Environment for Statistical\n  Computing_. R Foundation for Statistical Computing, Vienna, Austria.\n  &lt;https://www.R-project.org/&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2025},\n    url = {https://www.R-project.org/},\n  }\n\nWe have invested a lot of time and effort in creating R, please cite it\nwhen using it for data analysis. See also 'citation(\"pkgname\")' for\nciting R packages.\n\n\nTo cite a package, check citation(\"packagename\").\n\ncitation(\"tidyverse\")\n\nTo cite package 'tidyverse' in publications use:\n\n  Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R,\n  Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller\n  E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V,\n  Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). \"Welcome to\n  the tidyverse.\" _Journal of Open Source Software_, *4*(43), 1686.\n  doi:10.21105/joss.01686 &lt;https://doi.org/10.21105/joss.01686&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Article{,\n    title = {Welcome to the {tidyverse}},\n    author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},\n    year = {2019},\n    journal = {Journal of Open Source Software},\n    volume = {4},\n    number = {43},\n    pages = {1686},\n    doi = {10.21105/joss.01686},\n  }\n\n\nFinally, R and R Studio are two separate programs, so deserve a separate citations. Use the help menu (“Help &gt; RStudio Docs”) for more information.",
    "crumbs": [
      "Installing R"
    ]
  },
  {
    "objectID": "RIntro/InstallingR.html#next-lesson",
    "href": "RIntro/InstallingR.html#next-lesson",
    "title": "Installing R",
    "section": "Next Lesson",
    "text": "Next Lesson\nCongrats. You should now know how to install and launch R and RStudio. The next step is learn to work with R data.\nNext Lesson",
    "crumbs": [
      "Installing R"
    ]
  },
  {
    "objectID": "RIntro/ErrorHandling.html",
    "href": "RIntro/ErrorHandling.html",
    "title": "Error Handling in R",
    "section": "",
    "text": "Let’s start by making an error.\n\nplot(c00rs)\n\n\n\nError in eval(expr, envir) : object 'c00rs' not found\n\n\n\nplotit &lt;- function (x)\n  plot(x)\n\n\nplotit(c00rs)\n\n\n\nError in eval(expr, envir) : object 'c00rs' not found\n\n\nNote that this function doesn’t make sense if the input is negative.\n\nscaleme &lt;- function (x, sf) {\n  x/sqrt(sf)\n}\nscaleme(1:10,-3)\n\nWarning in sqrt(sf): NaNs produced\n\n\n [1] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n\n\nThis produces a NaN. That might be a problem as we might not notice that there is a problem until several calculations down the line.\nCan check for negative scale factor and provide a more informative error message.\n\nscaleme &lt;- function (x, sf) {\n  if (!is.numeric(sf) || sf &lt;= 0) \n    stop(\"Expected sf to be a positive number, got \", sf)\n  x/sqrt(sf)\n}\n\n\nscaleme(1:10,-3)\n\n\n\nError in scaleme(1:10, -3) : Expected sf to be a positive number, got -3\n\n\nOr maybe, just issue a warning and let the NaN go through\n\nscaleme &lt;- function (x, sf) {\n  if (!is.numeric(sf) || sf &lt;= 0) \n    warning(\"Expected sf to be a positive number, got \", sf,\"\\n\")\n  x/sqrt(sf)\n}\nscaleme(1:10,-3)\n\nWarning in scaleme(1:10, -3): Expected sf to be a positive number, got -3\n\n\nWarning in sqrt(sf): NaNs produced\n\n\n [1] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN",
    "crumbs": [
      "Error Handling in R"
    ]
  },
  {
    "objectID": "RIntro/ErrorHandling.html#some-simple-examples",
    "href": "RIntro/ErrorHandling.html#some-simple-examples",
    "title": "Error Handling in R",
    "section": "",
    "text": "Let’s start by making an error.\n\nplot(c00rs)\n\n\n\nError in eval(expr, envir) : object 'c00rs' not found\n\n\n\nplotit &lt;- function (x)\n  plot(x)\n\n\nplotit(c00rs)\n\n\n\nError in eval(expr, envir) : object 'c00rs' not found\n\n\nNote that this function doesn’t make sense if the input is negative.\n\nscaleme &lt;- function (x, sf) {\n  x/sqrt(sf)\n}\nscaleme(1:10,-3)\n\nWarning in sqrt(sf): NaNs produced\n\n\n [1] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n\n\nThis produces a NaN. That might be a problem as we might not notice that there is a problem until several calculations down the line.\nCan check for negative scale factor and provide a more informative error message.\n\nscaleme &lt;- function (x, sf) {\n  if (!is.numeric(sf) || sf &lt;= 0) \n    stop(\"Expected sf to be a positive number, got \", sf)\n  x/sqrt(sf)\n}\n\n\nscaleme(1:10,-3)\n\n\n\nError in scaleme(1:10, -3) : Expected sf to be a positive number, got -3\n\n\nOr maybe, just issue a warning and let the NaN go through\n\nscaleme &lt;- function (x, sf) {\n  if (!is.numeric(sf) || sf &lt;= 0) \n    warning(\"Expected sf to be a positive number, got \", sf,\"\\n\")\n  x/sqrt(sf)\n}\nscaleme(1:10,-3)\n\nWarning in scaleme(1:10, -3): Expected sf to be a positive number, got -3\n\n\nWarning in sqrt(sf): NaNs produced\n\n\n [1] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN",
    "crumbs": [
      "Error Handling in R"
    ]
  },
  {
    "objectID": "RIntro/ErrorHandling.html#r-options-for-handling-warnings.",
    "href": "RIntro/ErrorHandling.html#r-options-for-handling-warnings.",
    "title": "Error Handling in R",
    "section": "R Options for handling warnings.",
    "text": "R Options for handling warnings.\nSetting - options(warn=0) – Print warnings at end of function. - options(warn=1) – Print warnings as they happen - options(warn=2) – Stop on warnings\n\noptions(warn=1)\nscaleme(1:10,-3)\n\nWarning in scaleme(1:10, -3): Expected sf to be a positive number, got -3\n\n\nWarning in sqrt(sf): NaNs produced\n\n\n [1] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n\n\n\noptions(warn=2)\nscaleme(1:10,-3)\n\nAdding a call to recover() where the error occurs will allow the programmer to look around at what is going on when the error occurs.\n\nscaleme &lt;- function (x, sf) {\n  if (!is.numeric(sf) || sf &lt;= 0) {\n    warning(\"Expected sf to be a positive number, got \", sf,\"\\n\")\n    recover()\n  }\n  x/sqrt(sf)\n}\nscaleme(1:10,-3)",
    "crumbs": [
      "Error Handling in R"
    ]
  },
  {
    "objectID": "RIntro/R and R Studio Presentation2.html",
    "href": "RIntro/R and R Studio Presentation2.html",
    "title": "R and RStudio",
    "section": "",
    "text": "R is a language and environment for statistical computing and graphics. {data-background=Rlogo.png data-background-size=250px data-background-position=right}\n\n\n\ndata manipulation and calculations\ngraphical displays\nstatistical techniques\n\n\n\n\n\nfor UNIX platforms, Windows and MacOS\nfor download from (https://cloud.r-project.org/)\nCurrent version: R 3.5.2",
    "crumbs": [
      "R and RStudio"
    ]
  },
  {
    "objectID": "RIntro/R and R Studio Presentation2.html#rstudio-desktop-can-be-downloaded-from",
    "href": "RIntro/R and R Studio Presentation2.html#rstudio-desktop-can-be-downloaded-from",
    "title": "R and RStudio",
    "section": "RStudio Desktop can be downloaded from",
    "text": "RStudio Desktop can be downloaded from\n\n(https://www.rstudio.com/products/rstudio/download/)\nYou can use R with any editor, including RStudio, to write scripts. However, RStudio itself is not very useful without R.\n\n\nAdditional IDE features of RStudio can be located here\n\n(https://www.rstudio.com/products/rstudio/features/)\n\nRStudio Resources\n\n\nWebinars\n\n(https://resources.rstudio.com/webinars)\n\n\n\nCheat Sheets\n\n(https://www.rstudio.com/resources/cheatsheets/#ide)",
    "crumbs": [
      "R and RStudio"
    ]
  },
  {
    "objectID": "RIntro/index.html",
    "href": "RIntro/index.html",
    "title": "Index of R Demonstrations for R Group",
    "section": "",
    "text": "Florida State University\nThese are demonstrations which were written for the FSU R user group. Many are related to introductory topics in R.",
    "crumbs": [
      "Home",
      "Intro to R"
    ]
  },
  {
    "objectID": "RIntro/index.html#index-of-r-demonstrations",
    "href": "RIntro/index.html#index-of-r-demonstrations",
    "title": "Index of R Demonstrations for R Group",
    "section": "Index of R Demonstrations",
    "text": "Index of R Demonstrations\n\n\n\n\nEDAwithGGPlot.qmd\n\n\nErrorHandling.qmd\n\n\nindex.qmd\n\n\nInstallingR.qmd\n\n\nMatrixesAndDataFrames.qmd\n\n\nR and R Studio Presentation2.qmd\n\n\nRDataStructures.qmd\n\n\nRNotebook.qmd\n\n\nTidyStrings.qmd\n\n\nWorkingWithRData.qmd\n\n\n\n\n These are licensed under the creative commons CC BY license. You many distribute, remix, adapt, and build upon the material in any medium or format, so long as attribution is given to the creator.\nThe Source files for these demonstrations can be found at https://github.com/ralmond/TeachingDemos/RIntro.",
    "crumbs": [
      "Home",
      "Intro to R"
    ]
  }
]